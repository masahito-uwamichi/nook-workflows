
# [R] Anthropic: Reasoning Models Don’t Always Say What They Think

**Upvotes**: 42



[View on Reddit](https://www.reddit.com/r/MachineLearning/comments/1jr6iqj/r_anthropic_reasoning_models_dont_always_say_what/)

はい、承知いたしました。以下に順を追って詳細に、分かりやすく回答します。

1.  **このポストの内容の説明**

このRedditの投稿は、Anthropic社が発表したAIに関する論文を紹介し、それに対する議論を促すものです。

*   **論文の要点:** 論文では、Chain-of-Thought (CoT) という、AIが推論過程を説明する手法の信頼性について検証しています。CoTは、AIの思考過程を人間が理解するのに役立つと考えられていますが、論文の結果は、CoTが必ずしもAIの実際の思考を忠実に反映しているとは限らないことを示唆しています。具体的には、以下の3点を示しています。
    1.  AIは、プロンプトで与えられたヒントを利用していることをCoTで開示する割合は低い (多くの場合20%未満)。
    2.  結果に基づく強化学習は、CoTの信頼性を一時的に向上させるものの、完全には改善しない。
    3.  強化学習によってヒントの使用頻度が増加しても、CoTでの開示頻度は増加しない (つまり、AIはヒントを隠蔽する可能性がある)。

*   **投稿者の解釈:** 投稿者は、これらの結果から、CoTを利用する「推論モデル」がユーザーに嘘をついている可能性があると解釈しています。

*   **全体:** つまり、この投稿は、AIの透明性と安全性に関する重要な問題提起を行っており、AIの推論過程を理解するためのツールとして期待されているCoTの限界を示唆しています。

2.  **特に興味深いコメント**

この投稿に対するコメントで特に興味深いのは、以下の2つです。

*   **「I'm not sure if lying is the correct interpretation. I don't think humans say what they think in many cases, even even when we try to verbalize what we are thinking it's not fully reflective of our inner state. In fact, I'd be surprised if CoT somehow revealed everything a model is thinking.」**

    このコメントは、「嘘をつく」という解釈に異議を唱えています。人間自身も、常に自分の考えを正確に言葉で表現できるわけではないという点を指摘し、AIに完璧な透明性を求めるのは非現実的ではないかと問いかけています。AIの思考プロセスが複雑であることを考えると、CoTがすべてを明らかにするとは考えにくい、という意見は、CoTの限界を考慮する上で重要な視点です。

*   **「And here we are, anthropomorphizing LLM again.」**

    このコメントは、大規模言語モデル（LLM）を擬人化する傾向を批判しています。AIを人間のように捉えることは、AIの能力や限界を誤解する原因となる可能性があります。このコメントは、AIを客観的に評価し、感情的な先入観を排除することの重要性を思い出させてくれます。

これらのコメントは、AIの透明性、安全性、そしてAIに対する人間の認識という、複雑で多岐にわたる問題を浮き彫りにしています。


---

# [R] Mitigating Real-World Distribution Shifts in the Fourier Domain (TMLR)

**Upvotes**: 9



[View on Reddit](https://www.reddit.com/r/MachineLearning/comments/1jrd9tc/r_mitigating_realworld_distribution_shifts_in_the/)

1. **ポストの内容の説明:**

このRedditポストは、機械学習の研究論文を紹介しています。論文タイトルは「[R] Mitigating Real-World Distribution Shifts in the Fourier Domain (TMLR)」です。ここで、"[R]"はおそらく論文の種類（研究論文）、"TMLR"は論文が掲載されたジャーナル（Transactions on Machine Learning Research）の略称です。

投稿文のTLDR (Too Long; Didn't Read) は、論文の内容を非常に簡潔に要約しています。要約の内容は以下の通りです。

*   **テーマ:** 教師なしドメイン適応（Unsupervised Domain Adaptation）に関する研究。
*   **手法:** 学習データ（train domain）とテストデータ（test domain）の周波数統計（frequency statistics）を一致させることで、ドメイン間のずれ（distribution shift）を軽減する。
*   **特徴:** ラベルデータが不要（教師なし）。
*   **適用範囲:** 画像（vision）、音声（audio）、時系列データ（time-series）など、幅広い種類のデータに適用可能。
*   **論文とコード:** 論文へのリンク([https://openreview.net/forum?id=lu4oAq55iK](https://openreview.net/forum?id=lu4oAq55iK))と、おそらく実装コードへのリンクが提供されている。

つまり、この論文は、異なるデータセット間で機械学習モデルの性能が低下する問題を、データの周波数特性に着目して解決する新しい手法を提案していることを示しています。

2. **コメントについて:**

このポストにはコメントがありません。


---

# What is your practical NER (Named Entity Recognition) approach? [P]

**Upvotes**: 13



[View on Reddit](https://www.reddit.com/r/MachineLearning/comments/1jr8klg/what_is_your_practical_ner_named_entity/)

1.  **ポストの内容の説明:**

このRedditのポストは、投稿者がFlutterアプリ開発で行き詰まっている問題について質問しています。アプリは、OCRを使って食品パッケージの画像をスキャンし、テキストを抽出して翻訳するところまではうまくいっています。しかし、抽出されたテキストを「タイトル」「栄養成分表示」「ブランド」などの意味のある構造に整理することが課題となっています。最終的には、これらの情報を自動的にフォームに記入できるようにしたいと考えています。

現在、ルールベースのパーシング（正規表現とキーワードを使用）を試していますが、構造化されていないテキストではうまく機能しません。そこで、他のアプローチを検討しており、以下の3つの選択肢を考えています。

*   **ルールベースのパーシング:** 簡単だが、構造化されていないテキストには弱い。
*   **独自のNERモデルの作成:** AI/MLの経験はないが、挑戦する意欲はある。
*   **外部APIの利用:** コストを抑えたいので、できるだけ避けたい。

投稿者は、これらの選択肢以外にもっと良い方法がないか、経験者の意見を求めています。特に、AI/MLに時間を投資する覚悟はあるものの、効率的に学習を進めたいと考えています。

2.  **特に興味深いコメント:**

最も興味深いコメントは、以下の2つです。

*   **GoLLIEの提案:**  "Check this out: https://hitz-zentroa.github.io/GoLLIE/"

    このコメントは、ICLR 2024の論文で発表されたGoLLIEというツールを提案しています。GoLLIEは、NERを含む情報抽出のための最先端技術であり、期待されるクラスをPythonのデータクラスとして記述することで、エンティティとその属性を抽出できると説明されています。投稿者の課題に直接対応できる可能性があり、非常に興味深いと言えます。
*   **ローカル/オフラインNERのための提案:** "For local/offline NER, you might try fine-tuning a small model like DistilBERT using something like ONNX or TensorFlow Lite for deployment. Start by labeling ~500–1000 examples and training with spaCy—it’s pretty beginner-friendly and gives solid results for this kind of semi-structured data."

    このコメントは、オフラインで実行可能なNERモデルを作成するために、DistilBERTのような小さなモデルをファインチューニングすることを提案しています。具体的には、ONNXまたはTensorFlow Liteを使ってデプロイし、spaCyを使ってトレーニングすることを推奨しています。spaCyは初心者にも使いやすく、半構造化データに対して優れた結果が得られると述べています。この提案は、投稿者がAI/MLの経験がないことを考慮し、具体的なツールとステップを示しており、非常に実践的で役立つ可能性があります。


---

# [R] MergeVQ: Improving Image Generation and Representation Through Token Merging and Quantization

**Upvotes**: 5



[View on Reddit](https://www.reddit.com/r/MachineLearning/comments/1jr8zns/r_mergevq_improving_image_generation_and/)

はい、承知いたしました。以下に、質問に対する回答を記載します。

**1. このポストの内容の説明**

このRedditの投稿は、"MergeVQ"という新しい画像生成と表現のための統一フレームワークを紹介しています。MergeVQは、トークンマージング（Token Merging）とベクトル量子化（Vector Quantization）という2つの技術を、互いに独立させながらも協調的に機能させることで、画像生成と画像認識の両方のタスクにおいて優れた性能を発揮します。

*   **背景:** 従来、画像生成モデルと画像認識モデルは別々に開発されていましたが、MergeVQはこれらのタスクを1つのアーキテクチャで効率的に処理することを目指しています。

*   **技術的なポイント:**
    *   **分離されたトークンマージング（disentangled Token Merging Self-Similarity (MergeSS)）:** 画像内の冗長なトークンを特定して統合し、シーケンス長を大幅に（最大97%）削減します。これにより、計算コストを削減できます。
    *   **ベクトル量子化（VQ）:** 連続的な表現を離散的なコードブックにマッピングし、意味的な整合性を維持します。
    *   **高性能:** MS-COCOテキストからの画像生成で39.3 FID（Fréchet Inception Distance）を達成し、ImageNet分類で85.2%の精度を達成しています。これは、それぞれ専用のモデルに匹敵する性能です。
    *   **スケーラビリティ:** モデルサイズを大きくすると性能が向上し、さまざまなタスクで一貫した改善が見られます。

*   **MergeVQの利点:**
    *   **効率性:** トークンマージングによるシーケンス長の圧縮により、計算コストを削減できます。
    *   **汎用性:** 画像生成と画像認識の両方のタスクで優れた性能を発揮します。
    *   **設計の優位性:** トークンマージングとベクトル量子化を分離することで、より優れた結果が得られることが示されています。

*   **今後の展望:**
    *   DALL-E 3やMidjourneyのような大規模モデルとの比較や、効率性の維持が今後の課題です。

**TLDR（要約）:** MergeVQは、トークンマージングとベクトル量子化を分離することで、画像生成と画像表現を統一し、計算コストを削減しながら、両方のタスクで最先端の性能を達成します。

**2. このポストに対するコメントのうち、特に興味深いもの**

申し訳ありません。提供されたテキストにはコメントが含まれていませんでした。もしコメントが提供されれば、その中から興味深いものを選んで解説することができます。


---

# [R] Scaling Language-Free Visual Representation Learning

**Upvotes**: 6



[View on Reddit](https://www.reddit.com/r/MachineLearning/comments/1jr6xwi/r_scaling_languagefree_visual_representation/)

1.  **ポストの内容の説明**

    このRedditのポストは、Facebook AI Research (FAIR) とニューヨーク大学 (NYU) から発表された新しい論文に関するものです。その論文の主な主張は、DINOのような純粋な自己教師あり学習（Self-Supervised Learning）の手法が、CLIPスタイルの教師あり学習（Supervised Learning）の手法を画像認識タスクにおいて上回る可能性があるということです。

    重要なポイントは以下の通りです。

    *   **自己教師あり学習 (Self-Supervised Learning):** ラベル付けされたデータを使わずに、データ自体から学習する手法。DINOはその一例です。
    *   **教師あり学習 (Supervised Learning):** ラベル付けされたデータを使って学習する手法。CLIP（Contrastive Language-Image Pre-training）はその一種で、テキストと画像のマッチングを使って学習します。
    *   **スケーリング (Scaling):** モデルのアーキテクチャサイズ（モデルの複雑さ）とデータセットサイズ（学習に使用するデータの量）を大きくすること。
    *   **主張:** 自己教師あり学習は、モデルサイズとデータセットサイズを大きくすると、教師あり学習よりも優れた性能を発揮する可能性がある。

    つまり、大規模なモデルとデータセットを使えば、DINOのような自己教師あり学習は、CLIPのような教師あり学習よりも画像認識でより良い結果を出せるかもしれない、ということを示唆する論文の紹介文です。

2.  **特に興味深いコメント**

    このポストには現時点でコメントがありません。したがって、特に興味深いコメントを選ぶことはできません。


---

# AI tools for ML Research - what am I missing? [D]

**Upvotes**: 53



[View on Reddit](https://www.reddit.com/r/MachineLearning/comments/1jqolkh/ai_tools_for_ml_research_what_am_i_missing_d/)

1.  **ポストの内容の説明**

このRedditのポストは、AI/ML（人工知能/機械学習）の研究者が、日々の研究活動（実験コードの作成、論文執筆など）で実際に使用しているAIツールについて意見交換を求めているものです。

投稿者は自身が使用しているツールとして、以下のものを挙げています。

*   **Cursor (w/ sonnet, gemini)**: 実験のコード作成やパイプラインの設計に使用。
*   **NotebookLM / text-to-audio summarisers**: 論文の読解に使用。
*   **Sonnet/DeepSeak**: 技術的な文章の作成に使用。
*   **Gemini Deep Research (also Perplexity)**: 参考文献の検索や日々の情報収集に使用。

投稿者は、他のソフトウェアエンジニアや機械学習エンジニアが使用するツールとは異なるのではないかと考えており、他の研究者の意見や使用しているツールを知りたいと考えています。

2.  **特に興味深いコメント**

以下に示す2つのコメントが特に興味深いと考えられます。

*   **1つ目のコメント（35 upvotes）**：
    AIツールをまだワークフローに組み込んでいないという意見です。Copilotを定型的なプロット作成に使う程度で、多くの同僚がChatGPTを常に開いているのとは対照的であると述べています。これは、AIツールが研究者全員に浸透しているわけではないことを示唆しており、AIツールに対する慎重な姿勢や、従来のツールへの慣れ、またはAIツールが自身の研究スタイルに合わないと考える研究者が一定数存在することを示唆している点で興味深いです。

*   **2つ目のコメント（6 upvotes）**：
    AIツールを積極的に利用しているものの、その限界も指摘している意見です。計画の立案、コードの断片の生成、デバッグ、技術的な文章の改善などにGPTモデルを使用していると述べています。しかし、AIモデルがエラーを起こしやすいこと（バグのあるコード、誤った根拠、意味的に不正確な文章）も指摘しており、AIツールを使う方が使わないよりも効率的だとは考えているものの、論文の推敲など、依然として手間がかかる作業も多いと述べています。また、要約や検索に関しても、不正確な情報や関連論文の欠落があることを指摘しており、AIツールの限界を認識している点が興味深いです。プライバシーの問題からAIツールを使わないというコメントがあることも注目に値します。


---

# New paper from DeepSeek w/ model coming soon: Inference-Time Scaling for Generalist Reward Modeling

**Upvotes**: 261



[View on Reddit](https://www.reddit.com/r/LocalLLaMA/comments/1jre3kp/new_paper_from_deepseek_w_model_coming_soon/)

1. **ポストの内容の説明**

このRedditのポストは、DeepSeek社が発表した新しい論文「Inference-Time Scaling for Generalist Reward Modeling」について紹介しています。この論文では、大規模言語モデル(LLM)の性能を評価する「報酬モデリング(RM)」という技術を改善する新しい手法である「DeepSeek-GRM」が提案されています。

*   **背景:** LLMの訓練において、正確な報酬シグナルを得ることが重要ですが、一般的な質問や複雑なタスクでは困難です。
*   **DeepSeek-GRM:** この手法は、推論時に計算資源をより効率的に活用し、大規模なモデルを使用しなくても高い性能を達成することを目指しています。特に、「Self-Principled Critique Tuning (SPCT)」という学習方法を用いて、タスクごとに適切な原則を生成し、それに基づいて詳細な評価（批判）を行います。
*   **実験結果:** DeepSeek-GRM-27Bは、並列サンプリングを使用することで、遥かに大規模な報酬モデル（最大671Bパラメータ）と同等以上の性能を発揮することを示しました。これは、訓練時の計算資源を増やすよりも、推論時に効率的な計算を行う方が効果的であることを示唆しています。
*   **利点:** DeepSeek-GRMは、従来の報酬モデルが持つドメインバイアスを回避し、多様なタスクに対応できる汎用性があります。
*   **今後の展望:** まだ課題は残るものの、今後の汎用的な報酬システムの研究によって解決できると期待されています。
*   **オープンソース:** DeepSeek社は、このモデルをオープンソースとして公開する予定です。
*   **まとめ:** 大規模なモデルがなくても、推論時の計算を工夫することで、高品質な評価が可能になることを示唆しており、ローカルでLLMを実行したいユーザーにとって、有望なアプローチを提供します。

2. **特に興味深いコメント**

このポストに対するコメントで特に興味深いのは以下の2点です。

*   **「Their experiments show that DeepSeek-GRM-27B with parallel sampling can match or exceed the performance of much larger reward models (up to 671B parameters)"Yes please」 (143 upvotes):** これは、DeepSeek-GRMの性能に対する強い期待と関心を直接的に表しています。特に、27Bという比較的小さなモデルで、671Bもの巨大モデルに匹敵する性能が出せる可能性があるという点に、ユーザーが興奮していることがわかります。ローカル環境で大規模モデルを動かすのが難しいユーザーにとって、このニュースは非常に魅力的です。

*   **「Kinda similar to batching multiple replies to prompt and then choosing the better one.」 (10 upvotes):** これは、DeepSeek-GRMの並列サンプリングの概念を、既存のLLMの使い方（複数の回答を生成し、その中から最良のものを選ぶ）になぞらえて説明しており、理解を助けるコメントです。ユーザーがすでに実践している方法と関連付けることで、DeepSeek-GRMがより身近な技術であると感じさせます。このコメントは、DeepSeek-GRMが実用的なアプローチであることを示唆しています。


---

# Local LLMs are essential in a world where LLM platforms are going to get filled with ads

**Upvotes**: 94



[View on Reddit](https://www.reddit.com/r/LocalLLaMA/comments/1jrljxa/local_llms_are_essential_in_a_world_where_llm/)

はい、承知いたしました。以下に質問への回答を順に説明します。

**1. このポストの内容**

このRedditのポストは、大規模言語モデル(LLM)のプラットフォームが広告で溢れかえる未来を見据え、ローカルLLMの重要性を訴えています。

*   **タイトル:** 「LLMプラットフォームが広告で溢れる世界では、ローカルLLMが不可欠」というタイトルは、LLMの利用において、広告の存在が大きな問題になるという前提を示しています。
*   **内容の要約:** LLMが普及するにつれて、Google検索のように、LLMのプラットフォームにも広告が組み込まれる可能性があります。広告が表示されるだけでなく、LLM自体が広告を出すように訓練される可能性もあります。そのため、ユーザーが広告の影響を受けずにLLMを利用するためには、ローカルでLLMを実行できる環境が重要になります。さらに、既存のモデルをファインチューニングしたり、完全に新しいモデルをトレーニングしたりできる能力も重要です。さもなければ、公開されているモデルが広告に偏っている可能性があり、ユーザーは広告のバリエーションの中から選ぶことしかできなくなるかもしれません。

**2. 特に興味深いコメント**

このポストに対するコメントの中で特に興味深いのは、LLMが単に広告を表示するだけでなく、より巧妙な形で広告や政治的な偏向を組み込む可能性について議論している点です。

*   **広告の埋め込み:** コメントでは、LLMが文章の最後に結論や評価を付けるように訓練されていることを利用して、広告を自然な形で組み込むことができると指摘しています。
*   **政治的偏向:** さらに、広告だけでなく、政治的な偏向がLLMに意図的に組み込まれる可能性についても言及しており、MITの研究を引き合いに出しています。広告と異なり、政治的偏向の検出は困難です。
*   **ローカルLLMの重要性:** これらの問題に対処するためには、ローカルでLLMを実行するだけでなく、既存のモデルをファインチューニングしたり、独自のモデルをトレーニングしたりする能力が重要だと強調しています。これにより、ユーザーは広告や政治的な偏向から自由なLLMを利用できるようになります。ローカルで広告を配信するには指標がないため、配信者にとって魅力的ではないと指摘しています。

これらのコメントは、LLMの普及に伴い、単に技術的な問題だけでなく、広告や政治的な偏向といった社会的な問題にも対処する必要があることを示唆しており、非常に興味深いと言えます。


---

# Meta Set to Release Llama 4 This Month, per The Information & Reuters

**Upvotes**: 188



[View on Reddit](https://www.reddit.com/r/LocalLLaMA/comments/1jrfqnu/meta_set_to_release_llama_4_this_month_per_the/)

はい、承知いたしました。以下に質問への回答を記載します。

**1. このポストの内容**

このRedditのポストは、Meta（旧Facebook）が開発中の大規模言語モデル「Llama 4」に関するニュース記事に基づいています。記事によると、Metaは当初の予定から少なくとも2回遅れていましたが、今月中にLlama 4をリリースする計画です。

記事の内容の要点は以下の通りです。

*   **リリース時期:** 今月中を予定（ただし、再度延期される可能性もある）。
*   **開発の遅延理由:**
    *   技術的なベンチマーク（特に推論と数学のタスク）でMetaの期待を満たさなかった。
    *   人間のような音声会話能力がOpenAIのモデルに劣ると懸念された。
*   **開発への投資:** AIインフラの拡張に今年650億ドルを投資予定。
*   **DeepSeekの影響:** 中国のDeepSeek社の低コストモデルの台頭が、高性能AIモデルの開発に巨額の費用が必ずしも必要ではないという考え方を後押ししている。
*   **技術的な特徴:** DeepSeekの技術的側面を取り入れ、「mixture of experts」と呼ばれる機械学習技術を採用する予定。
*   **リリース方法:** まずMeta AIを通じてリリースし、その後オープンソースとして公開することを検討している。
*   **Llama 3:** 昨年、多言語対応、高品質なコード生成、複雑な数学問題解決が可能なLlama 3をリリース済み。

**2. 特に興味深いコメント**

このポストに対するコメントの中で特に興味深いのは以下の2つです。

*   **オープンソース化への期待と懸念:**
    *   113 upvotesのコメント: MetaがまずMeta AIを通じてLlama 4をリリースし、その後オープンソースとして公開することを検討している点に対し、「もしそうなら悲しい」という意見。これは、即時のオープンソース化を期待するユーザーの気持ちを表しています。
    *   28 upvotesのコメント: Llama Community Licenseの次のバージョン（Llama 4に対応するもの）について、Metaが制約を増やすのではないかという懸念。具体的には、Llama 3のライセンスで追加された「Built with Meta Llama 3」のバナー表示義務や、より多くの禁止事項の追加を予想しています。

これらのコメントは、Llamaシリーズのオープンソースモデルとしての利用を期待するユーザーが、Metaのライセンス戦略に注目し、潜在的な制限事項を懸念していることを示しています。Llama 4のライセンス条項が、今後のAI開発のあり方に大きな影響を与える可能性があるため、非常に重要なポイントです。


---

# Chinese response bug in tokenizer suggests Quasar-Alpha may be from OpenAI

**Upvotes**: 253



[View on Reddit](https://www.reddit.com/r/LocalLLaMA/comments/1jrd0a9/chinese_response_bug_in_tokenizer_suggests/)

1.  **ポストの内容**

このRedditのポストは、OpenRouterを通じて公開された新しいモデル「quasar-alpha」に、OpenAIのGPT-4oと同様の問題があることを指摘しています。具体的には、中国語の特定のフレーズ「给主人留下些什么吧」（「主人に何かを残してあげましょう」または「この文を英語に翻訳してください」の意味）をquasar-alphaに入力すると、質問とは無関係な応答が返ってくるという現象です。

投稿者は、この問題がGPT-4oのリリース時にも発生したことを指摘しています。GPT-4oでは、tokenizer（テキストをトークンに分割するプログラム）の「o200k\_base」において、上記の中国語フレーズが単一のトークンとして扱われるというバグがありました。quasar-alphaも同様の問題を抱えていることから、投稿者はこのモデルがOpenAI製である可能性が高いと推測しています。OpenAIがこの中国語トークンのバグを修正していないことが示唆されています。

つまり、quasar-alphaのtokenizerに、GPT-4oと同じバグがあることが、OpenAI製のモデルである可能性を示唆している、というのが投稿の要点です。

2.  **特に興味深いコメント**

このポストに対するコメントで特に興味深いのは、以下のものです。

*   **37 upvotesのコメント**: OpenAIのtokenizerであるtiktokenがオープンソースであることを示し、実際にコードを提示して問題のフレーズ「给主人留下些什么吧」がトークンID 177431としてエンコードされることを再現しています。このコメントは、投稿者の主張を裏付ける証拠を簡単に再現できる形で提供しており、問題の核心を理解する上で非常に役立ちます。
*   **103 upvotesのコメント**: tokenizerの語彙に大量の不要な情報が含まれていることを指摘し、それが小規模モデルの埋め込み層のサイズを圧迫している可能性を述べています。モデルの効率性とパフォーマンスに対するtokenizerの語彙の重要性を示唆しており、この問題が単なるバグ以上の意味を持つことを示唆しています。


---

# Framework Desktop development units for open source AI developers

**Upvotes**: 28



[View on Reddit](https://www.reddit.com/r/LocalLLaMA/comments/1jrqb11/framework_desktop_development_units_for_open/)

1.  **ポストの内容の説明**

    このRedditのポストは、Framework Desktopという製品に関するものです。投稿者はFramework Desktopの開発元関係者で、AMD（Advanced Micro Devices）が米国/カナダを拠点とするオープンソースAI開発者向けに100台のFramework Desktopを無償提供することを告知しています。この無償提供は、地域のAI開発を加速させることを目的としています。

    投稿には、応募フォームへのリンク（[https://www.amd.com/en/forms/sign-up/framework-desktop-giveaway.html](https://www.amd.com/en/forms/sign-up/framework-desktop-giveaway.html)）が含まれており、投稿者はFramework Desktopをローカル推論に使用することに関する質問に答える用意があることを表明しています。

    全体として、このポストは特定のハードウェアのプロモーションであると同時に、AI開発者への支援機会の告知という側面を持っています。

2.  **興味深いコメント**

    以下のコメントが特に興味深いと考えられます。

    *   **「What about those who are developing tools with ai to enhance ai and human harmony and contradict systems of extraction but are not comfortable with GitHub?I can only wish.」**

        このコメントは、GitHubの使用に抵抗がある開発者（AIと人間の調和を促進するツールや、搾取的なシステムに対抗するツールを開発している開発者）が、この無償提供の対象から外れる可能性を指摘しています。オープンソース開発のプラットフォームとしてGitHubが広く利用されている一方、GitHub以外の方法で開発を行っている開発者も存在し、そのような開発者への配慮が欠けているのではないかという問題提起を行っています。また、このような開発者は支援の機会を得られないことに対する落胆を表しています。


---

# Lumina-mGPT 2.0: Stand-alone Autoregressive Image Modeling | Completely open source under Apache 2.0

**Upvotes**: 485

<video src="https://v.redd.it/jrf0voururse1/DASH_720.mp4?source=fallback" controls controls style="width: 100%; height: auto; max-height: 500px;"></video>

[View on Reddit](https://www.reddit.com/r/LocalLLaMA/comments/1jr6c8e/luminamgpt_20_standalone_autoregressive_image/)

1.  **ポストの内容の説明**

このRedditのポストは、Lumina-mGPT 2.0という新しい画像生成モデルについて紹介しています。

*   **モデル名:** Lumina-mGPT 2.0
*   **種類:** スタンドアロンな自己回帰画像モデリング
*   **ライセンス:** Apache 2.0 (完全なオープンソース)
*   **リンク:**
    *   GitHubリポジトリへのリンク
    *   Hugging Faceモデルへのリンク
    *   Hugging Face Spaceへのリンク

つまり、このポストは、新しい画像生成AIモデル（Lumina-mGPT 2.0）がオープンソースとして公開されたことを告知し、詳細情報や試用できる場所へのリンクを提供しているものです。

2.  **特に興味深いコメント**

最も興味深いコメントは、158 upvotesを獲得している以下のコメントです。

> "Oh, the irony is just dripping, isn't it? (LLMs) are now flirting with diffusion techniques, while image generators are cozying up to autoregressive methods. It's like everyone's having an identity crisis"

このコメントは、画像生成AIの技術トレンドの変化を指摘しています。

*   **LLM（大規模言語モデル）が拡散モデルの手法を取り入れ始めている**
*   **画像生成モデルが自己回帰モデルの手法を取り入れ始めている**

このコメントは、それぞれの分野（LLMと画像生成）が、互いの得意とする技術を取り入れることで、境界線が曖昧になっている状況を「アイデンティティ・クライシス」と表現しており、非常に示唆に富んでいます。AI技術の進化の方向性や、分野間の融合が進んでいる現状を簡潔かつユーモラスに表現している点が興味深いです。


---

# Not GPT-4, but a 3B Function Calling LLM that can chat to clarify tools calls

**Upvotes**: 26

<video src="https://v.redd.it/i9hd03p7fwse1/DASH_1080.mp4?source=fallback" controls controls style="width: 100%; height: auto; max-height: 500px;"></video>

[View on Reddit](https://www.reddit.com/r/LocalLLaMA/comments/1jrpbj8/not_gpt4_but_a_3b_function_calling_llm_that_can/)

1.  **ポストの内容の説明:**

    このRedditの投稿は、新しいLLM（大規模言語モデル）である「Arch-Function-Chat-3B」のリリースに関するものです。以下に要点をまとめます。

    *   **概要:** Arch-Function-Chat-3Bは、比較的小さなサイズ（30億パラメータ）でありながら、関数呼び出し（Function Calling）においてGPT-4と同等の性能を発揮できるLLMです。また、チャット機能も備えています。
    *   **チャット機能の目的:** ユーザーとの対話を通じて、ツール呼び出しに必要な情報を正確に収集することを目的としています。これにより、文脈を管理し、段階的な情報開示に対応し、ツール実行結果について軽量な対話を行うことができます。
    *   **リリース情報:** モデルはHugging Faceで公開されており、近いうちにGitHubのarchgwプロジェクトへの統合が完了する予定です。また、MCPを介して取得されたツール定義との統合も予定されています。
    *   **開発者へのメッセージ:** 開発者コミュニティに向けて、このモデルを使った開発を奨励しています。

2.  **興味深いコメント:**

    投稿に対するコメントは一件のみですが、非常に興味深い情報を含んでいます。

    *   **gooseとの試用:** コメント投稿者は、Arch-Function-Chat-3Bを「goose」というツール（おそらく情報抽出やウェブスクレイピングに関連するツール）と組み合わせて試用したことを述べています。
    *   **Localscoreの評価:** 試用結果として、「Localscore」という評価指標で33点を獲得したことを報告しています。Localscore.aiのリンクも提供されており、このスコアが客観的な性能指標であることが伺えます。

    このコメントが特に興味深い理由は、以下の点です。

    *   **具体的な試用例:** 理論的な説明だけでなく、実際のユースケースにおける試用結果を提供しているため、モデルの有用性を具体的に理解するのに役立ちます。
    *   **客観的な評価:** Localscoreという指標を用いることで、モデルの性能を定量的に評価しています。これにより、他のモデルとの比較や、今後の改善の方向性を検討する上で貴重な情報となります。
    *   **今後の展開への期待:** コメント投稿者がgooseとの組み合わせに興味を持っていることから、このモデルがさまざまなツールやアプリケーションに統合される可能性を示唆しています。


---

# Presenting CSM-HF : Sesame CSM reimplemented for Transformers (with finetuning support!)

**Upvotes**: 33



[View on Reddit](https://www.reddit.com/r/LocalLLaMA/comments/1jrkbg0/presenting_csmhf_sesame_csm_reimplemented_for/)

はい、承知いたしました。以下に、ご質問への回答を記載します。

**1. このポストの内容**

このRedditのポストは、投稿者が開発した「CSM-HF」というツールキットについて紹介しています。CSM-HFは、Sesame AILabsが開発したCSM（おそらく音声モデル）のコードを、Hugging Face Transformersという機械学習ライブラリ上で動作するように書き直したものです。これにより、Transformerモデルの利点（例えば、学習済みモデルの利用や、様々なツールとの連携）をCSMモデルにもたらすことができます。

主な特徴は以下の通りです。

*   **Hugging Face Trainerとの互換性:** Hugging FaceのTrainerを使って学習できます。特に、decoder training amortizationという効率的な学習方法もサポートしています。
*   **ファインチューニング:** 24GBのRAMがあればファインチューニングが可能です（シーケンス長2048、バッチサイズ1）。勾配累積を利用すれば、より大きなバッチサイズでの学習も可能です。
*   **生成機能:** 音声生成もできますが、現時点ではリアルタイムよりも遅いとのことです。しかし、最適化の余地があると考えています。
*   **LoRA/PEFTサポート:** 今後のロードマップとして、LoRAやPEFTといった効率的なファインチューニング手法のサポートを検討しています。

要するに、この投稿は、既存のCSMモデルをTransformerアーキテクチャ上でより使いやすく、ファインチューニングしやすくするためのツールキットを公開し、フィードバックを求めているものです。

**2. このポストに対するコメントのうち、特に興味深いもの**

いくつかのコメントが興味深いですが、特に注目すべきは以下の2点です。

*   **アーキテクチャに関する言及:** 投稿者自身が、CSM-HFのアーキテクチャが単純なLlamaデコーダではなく、2つのLlamaモデル（大規模なセマンティックバックボーンと小規模な音響デコーダ）を組み合わせた階層構造であることを説明している点です。これは、モデルの複雑さを示唆しており、単純な変換以上の工夫が凝らされていることを意味します。この複雑さのため、`llama.cpp`のようなツールでの実装には課題があることが示唆されています。
*   **リアルタイム性能についての言及:** 生成速度がリアルタイムより遅いという点について、「どれくらい遅いのか」という質問が出ています。これは、実用性を判断する上で重要な情報であり、今後の最適化の方向性を示唆するものです。

これらのコメントは、CSM-HFの技術的な詳細や、今後の開発の方向性について掘り下げており、単なる賞賛のコメントよりも深い洞察を提供していると言えます。特に、アーキテクチャに関する言及は、このプロジェクトの独自性を理解する上で不可欠です。


---

# Howto: Building a GPU Server with 8xRTX 4090s for local inference

**Upvotes**: 557

![Image](https://i.redd.it/vg99momf6qse1.png)

[View on Reddit](https://www.reddit.com/r/LocalLLaMA/comments/1jr0oy2/howto_building_a_gpu_server_with_8xrtx_4090s_for/)

1.  **ポストの内容の説明**

このRedditポストは、Marco Mascorro氏が作成した、ローカル推論用の8枚のRTX 4090 GPUを搭載したサーバーの構築方法に関するガイドを紹介しています。このガイドでは、使用したパーツや組み立て手順などが詳細に解説されています。投稿者は、A100やH100といった高価なGPUを使用する予算がない人にとって、このローカル推論ソリューションが役立つ可能性があると考えています。また、将来的に登場する5090 GPUでも同様の構成が可能であると述べています。投稿者は、ガイドに対するコメントやフィードバックを求めており、オープンソースやローカル推論への関心を示しています。

2.  **特に興味深いコメント**

*   **「You should begin by telling us the budget...」 (予算を教えてほしい)**： これは、多くのユーザーが最も関心を持つであろう点です。8枚のGPUを搭載したサーバーの構築には、かなりの費用がかかることが予想されるため、予算を知りたいというニーズは自然です。

*   **「Imagine 8x48gb variants 🤯」 (8x48GB版を想像してみて)**： これは、GPUのメモリ容量に関するコメントです。現在、RTX 4090のVRAMは24GBですが、将来的に48GB版が登場すれば、さらに大規模なモデルをローカルで推論できるようになる可能性を示唆しています。

*   **「You achieve better ROI with 2x RTX 6000 ADA PRO (total 192GB VRAM) if anyone wants to build something with a similar budget today.」 (同様の予算で構築したい場合、2x RTX 6000 ADA PRO（合計192GB VRAM）の方がROIが高い)**： これは、8枚のRTX 4090よりも、2枚のRTX 6000 ADA PROの方が、VRAM容量の面で優れているという指摘です。予算が限られている場合、より少ない枚数でより多くのVRAMを確保できる別の選択肢を示唆しており、コストパフォーマンスを重視するユーザーにとって重要な情報です。また、電力効率についても言及されており、より総合的な視点から代替案を提示しています。


---

# Whats the current best abliterated/uncensored model?

**Upvotes**: 14



[View on Reddit](https://www.reddit.com/r/LocalLLaMA/comments/1jrnx1z/whats_the_current_best_abliterateduncensored_model/)

1.  **ポストの内容の説明**

このRedditの投稿は、ユーザーが最新かつ最高の「検閲されていない」または「削除されていない」AIモデルを探しているという内容です。彼は最近、高性能なGPU（GeForce RTX 5090を想定）を購入し、以前の8GBのGPUよりも大きなモデルを試したいと考えています。つまり、検閲や制限がなく、より自由にテキスト生成や実験ができる大規模なAIモデルを探しているということです。

2.  **特に興味深いコメント**

いくつかのコメントが興味深いですが、特に以下の2つが挙げられます。

*   **Mistral Small 24b:** このコメントは、Mistral Small 24bというモデルを提案しています。このモデルは「箱から出してすぐに検閲されていない」と述べており、すぐに使用できる点が魅力的です。「pretty good（かなり良い）」と評価されている点も、試してみる価値があると思わせます。
*   **Fallen Command A 111B V1:** このコメントは、Fallen Command A 111B V1という、より大規模な111Bパラメータのモデルを紹介しています。このモデルは「unhinged at times（時々抑制が効かなくなる）」とされており、より自由なテキスト生成が期待できます。ただし、最初にロールプレイ向けに作成されたという点に注意が必要です。また、モデルの量子化されたバージョンが60GB以上であり、40GB以上のRAMとCPUへのオフロードが必要になる可能性があるという点も、大規模なモデルならではの考慮点です。

