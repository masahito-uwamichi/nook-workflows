
# [D] Why is table extraction still not solved by modern multimodal models?

**Upvotes**: 11



[View on Reddit](https://www.reddit.com/r/MachineLearning/comments/1jnjfaq/d_why_is_table_extraction_still_not_solved_by/)

1.  **ポストの内容の説明**

    このRedditのポストは、現代のマルチモーダルモデル（画像とテキストを同時に扱えるモデル）が、人間には容易なテーブル抽出タスクをなぜ未だにうまくこなせないのかという疑問を提起しています。

    *   **問題提起:** 投稿者は、Qwen 2.5 VL、Omni、GOT、SmolDoclingといった最新のマルチモーダルモデルが、宣伝されているほどの性能を発揮せず、特にテーブル抽出において苦戦していることに不満を感じています。
    *   **具体例:** 添付された画像（テーブルの画像）を例に挙げ、この単純なテーブルを正確にCSV形式で再構築できるオープンソースモデルがあるのかを尋ねています。ポイントは、空のセルも正しく保持できることです。
    *   **質問の意図:** マルチモーダルモデルの限界と、テーブル抽出における課題について、他のユーザーの経験や意見を求めています。

2.  **特に興味深いコメント**

    このポストに対するコメントで特に興味深いのは、以下の2つです。

    *   **4 upvotesのコメント（詳細な技術的分析）**

        このコメントは、なぜマルチモーダルモデルがテーブル抽出で苦戦するのかを技術的な視点から分析しています。

        *   **情報の密度:** 数値データは文字データよりも情報密度が高いため、モデルがわずかな誤りを犯した場合の影響が大きいことを指摘しています。
        *   **幾何学的な問題:** LLMが幾何学的な問題に弱いという研究論文を紹介しています。
        *   **一貫性の重要性:** OCR（光学文字認識）においては一貫性が重要であり、従来の多段階OCRプロセスの方がエラーの原因特定や修正が容易であると述べています。
        *   **ワークフローの提案:** 特定のタスクのために個別のモデルを微調整し、ワークフローを構築することを提案しています。

        **なぜ興味深いか:** このコメントは、単に「モデルがうまくいかない」というだけでなく、その背後にある理由を深く掘り下げています。情報の密度、幾何学的な問題、一貫性の重要性といった具体的な要因を挙げることで、問題の本質を理解するのに役立ちます。また、解決策としてワークフローの構築を提案しており、実践的なアドバイスとしても有用です。

    *   **8 upvotesのコメント（単純な解決策の提案）**

        このコメントは、単純なケースであれば、合成データを使って独自のモデルを訓練することを提案しています。また、VLMs（Vision Language Models）は画像の正確な位置特定が苦手であることが、テーブル抽出を難しくしている原因ではないかと推測しています。YOLOでテーブルのバウンディングボックスを作成し、別のモデルでテキストを抽出するという提案もしています。

        **なぜ興味深いか:** 一見単純に見えるテーブル抽出も、VLMsにとっては難しいタスクになり得るという点が興味深いです。また、YOLOと別のモデルを組み合わせるという提案は、既存のツールを活用して問題を解決する現実的なアプローチを示唆しています。


---

# [R] [D] My (Mostly Failed) Attempt to Improve Transformers by Enriching Embeddings with the Last Hidden State – Why It Didn't Scale

**Upvotes**: 138



[View on Reddit](https://www.reddit.com/r/MachineLearning/comments/1jn0ha9/r_d_my_mostly_failed_attempt_to_improve/)

## このポストの内容説明

この投稿は、投稿者がDecoder Transformerの改善を試みたものの、うまくいかなかった実験について述べています。具体的な内容は以下の通りです。

1. **問題意識:**
   - 投稿者は、Decoder Transformerの構造上の問題点として、最終的な隠れ状態（最後の層の出力）が、単一のトークンに集約されることに着目しました。
   - 最終的な隠れ状態は、多くの情報（32ビット * 埋め込み次元）を持っている可能性がありますが、生成されるトークンは、語彙サイズに応じた情報量（log2(vocab_size)ビット）しか持てないと考えました。

2. **実験内容:**
   - 最終的な隠れ状態を使って、生成されたトークンの埋め込みを強化する新しいアーキテクチャを試しました。
   - 小規模なTransformer（10万パラメータ）では、ある程度の効果が見られましたが、大規模なTransformer（100万パラメータ）では、計算コストに見合う改善が得られませんでした。

3. **失敗の理由（仮説）:**
   - 投稿者は、失敗の理由として、Transformerの自己注意機構（アテンション）の性質を挙げました。
   - 次のトークンを予測する際、最後から2番目のトークンまでのすべての隠れ状態が利用可能であるため、最後の層の隠れ状態の情報がなくても、ある程度の情報が伝達されるのではないかと考察しました。
   - つまり、最後の層の隠れ状態を注入しても、最後の数層で行われた処理の情報を保持する程度の影響しかなく、decoder層が多い場合には、その影響は小さくなると結論付けました。

4. **その他:**
   - 投稿者は、実験の詳細や数学的な説明、牛の写真などをまとめたブログ記事へのリンクを共有しています。
   - 失敗談として、興味があれば読んでほしいと述べています。

要するに、投稿者はTransformerの最終層の情報を活用することで性能向上を目指しましたが、大規模モデルでは期待した効果が得られなかったという内容です。

## 特に興味深いコメント

この投稿に対するコメントで特に興味深いものは、以下の2つです。

1. **モデル容量とオートエンコーダとの類似性についてのコメント:**
   - このコメントは、投稿者の実験の失敗理由として、モデル容量の問題を示唆しています。
   - 潜在表現におけるノイズにモデルが過剰に反応し、有用な信号が埋もれてしまう可能性を指摘しています。
   - LCM（Latent Consistency Model）という類似のアイデアを紹介し、凍結されたオートエンコーダの潜在空間を利用することで、この問題を回避できると述べています。
   - また、EMA（Exponential Moving Average）蒸留を用いることで、再帰的な構造を解消し、更新を安定化させる可能性も示唆しています。
   - さらに、初期の動機におけるビット数の誤りや、モデルが潜在表現を完全に活用できない可能性についても指摘しており、モデルの規模と語彙サイズの関係性について考察を深めるきっかけとなるコメントです。

2. **アテンション機構の理解に関するコメント:**
   - このコメントは、投稿者のTransformerのアテンション機構に対する理解の一部に誤りがあることを指摘しています。
   - 標準的なTransformerでは、トークンNのレイヤーLは、トークンN-1（およびそれ以前のトークン）のレイヤーL-1の隠れ状態にアクセスできるだけであり、投稿者の提案手法のように、トークンN-1の最後から2番目のレイヤーの隠れ状態にアクセスできるわけではないと述べています。
   - この指摘を踏まえ、投稿者の手法は、標準的なTransformerよりもはるかに多くの情報を利用できるため、失敗の理由はもっと複雑である可能性を示唆しています。
   - そして、代替仮説として、(1)Transformerの後期レイヤーの隠れ状態は、初期・中期レイヤーよりも一般的な情報が少なく、予測する特定のトークンに関する情報が多い可能性、(2)各レイヤーのMLPが特定の潜在空間と相互作用しており、安定した学習にはボトムアップのフローが必要である可能性、という2つの可能性を提示しています。
   - メカニスティックな解釈可能性の観点から、Transformerの内部構造とその情報伝達の仕組みについて、さらに深く考察するためのヒントを与えてくれるコメントです。

これらのコメントは、投稿者の実験の失敗をより深く理解するための多角的な視点を提供しており、非常に興味深いと言えます。


---

# [P] Agent - A Local Computer-Use Operator for macOS

**Upvotes**: 5



[View on Reddit](https://www.reddit.com/r/MachineLearning/comments/1jngh6e/p_agent_a_local_computeruse_operator_for_macos/)

1. **ポストの内容の説明**

このRedditのポストは、macOS/Linux環境で複数のアプリケーションを連携させたワークフローを自動化するためのフレームワーク「Agent」をオープンソースとして公開したという告知です。このAgentは、以前に公開された「Computer」という別のツールを基盤として構築されており、ローカルのOllamaモデル（プライバシー重視の場合）またはOpenAI、Anthropicなどのクラウドプロバイダーと連携できます。

**主なポイント:**

*   **問題意識:** 複数のアプリケーションを連携させるAIエージェントが、予測不能な形で動作停止したり、環境によって動作が不安定になったり、複雑なワークフローで失敗したりする問題を解決するために開発されました。
*   **Agentの機能:**
    *   複数のアプリを連携させた複雑なワークフローを安定して実行
    *   ローカルまたはクラウドの任意のモデルを使用可能
    *   さまざまなエージェントループの実装を切り替え可能
    *   他のツールと連携しやすい構造化されたレスポンスを提供
*   **コード例:** Agentの使用例として、GitHubのリポジトリを検索し、Issueを確認し、リポジトリをクローンするタスクを実行するコードが示されています。
*   **応用例:**
    *   タスクごとに異なるエージェントループを使い分ける（OpenAI、Claude、OmniParserなど）
    *   様々なモデルを使用する（OpenAIのcomputer\_use\_preview、Claudeなど）
    *   エージェントの思考や動作の詳細なログを取得する（デバッグに役立つ）
    *   Computerのサンドボックス機能により、メインシステムを保護
*   **インストール方法:** `pip install "cua-agent[all]"`などのコマンドでインストールできます。
*   **利用状況:** 開発チームが内部で数週間使用しており、ワークフローの自動化に役立っているとのことです。
*   **目的:** ユーザーからのフィードバックを求めています。

2. **特に興味深いコメント**

申し訳ありませんが、投稿に対するコメントが提供されていません。そのため、興味深いコメントを特定することができません。


---

# [R] Text based backprop: Optimizing generative AI by backpropagating language model feedback

**Upvotes**: 16



[View on Reddit](https://www.reddit.com/r/MachineLearning/comments/1jn7jvg/r_text_based_backprop_optimizing_generative_ai_by/)

1.  **ポストの内容の説明**

このRedditのポストは、Natureに掲載された論文「TextGrad: Optimizing generative AI by backpropagating language model feedback」について紹介しています。

*   **概要:** 論文は、複数の大規模言語モデル（LLM）や他の専門ツールを組み合わせたAIシステムを、ドメイン専門家が手作業で調整するのではなく、自動的に最適化するための新しいフレームワーク「TextGrad」を提案しています。TextGradは、LLMが生成した自然言語フィードバックを逆伝播させることで最適化を行います。

*   **TextGradの仕組み:** TextGradは、システムのプロンプトから、分子や治療計画などの出力まで、あらゆる部分を自然言語のフィードバックによって評価し、改善を提案します。これにより、さまざまなタスク（博士レベルの科学問題の解決、放射線治療計画の最適化、特定の性質を持つ分子の設計、コーディング、エージェントシステムの最適化など）において、生成AIシステムを自動的に最適化できます。

*   **利点:** TextGradは、科学者やエンジニアが、影響力のある生成AIシステムを容易に開発できるようにします。

*   **投稿者の意見:** 投稿者は、TextGradが潜在力を持つものの、まだ完璧な最適化技術ではないと述べています。

*   **補足:** 投稿者は、論文へのリンクをコメント欄に追加しています。

2.  **興味深いコメント**

以下の2つのコメントが特に興味深いです。

*   **論文へのリンクを求めるコメント:**

    *   「Do you have a link to the paper?」というコメントは、投稿内容への関心の高さを示しています。このコメントに応えて、投稿者が論文へのリンクを共有したことで、他のユーザーも論文の内容を詳しく知ることができるようになりました。

*   **バックプロパゲーションに関するコメント:**

    *   「The development of artifcial neural networks faced a similar challenge until backpropagation and automatic diferentiation transformed the feld by making optimization turnkey. What a strange thing to say. Neutral network implementations always had back propagation.」というコメントは、論文の一節に対する批判的な意見を述べています。コメント主は、ニューラルネットワークの実装には常にバックプロパゲーションがあったため、論文の記述は不適切だと主張しています。このコメントは、論文の内容に対する疑問を提起し、議論を深める可能性があります。

これらのコメントは、TextGradという新しい手法に対する関心と疑問の両方を示しており、AI分野における最適化技術の進化に対する議論を活性化させる可能性があります。


---

# [Discussion] Linear Regression performs better than LGBM or XGBoost on Time Series

**Upvotes**: 6



[View on Reddit](https://www.reddit.com/r/MachineLearning/comments/1jneuix/discussion_linear_regression_performs_better_than/)

はい、承知いたしました。以下に、ご質問に対する回答を記載します。

1.  **このポストの内容の説明**

このRedditの投稿は、投稿者が時間単位の天気予報モデル（気温予測）を開発している際に遭遇した結果に関するものです。具体的には、10万件以上の気温データポイントを用いて、Linear Regression（線形回帰）、XGBoost、LGBMという3つの異なる機械学習モデルを試したところ、Linear Regressionが最も良い結果（MAEが0.30-0.31）を示し、XGBoost (MAEが0.32-0.34) やLGBM (MAEが0.334) よりも優れていた、という状況を説明しています。

投稿者は、特徴量エンジニアリングとして、シフト、ローリング、指数加重移動平均（EWM）といった時系列データ特有の手法を適用し、パラメータ調整やChatGPTへの相談も試みたものの、Linear Regressionが最も良かったため、自分のアプローチに問題があるのか、あるいはLinear Regressionが優れているのが普通なのか疑問に思っています。

まとめると、この投稿は、時系列予測において、複雑な機械学習モデル（XGBoost、LGBM）よりも単純なLinear Regressionが優れる場合があるのか、という疑問を提起し、コミュニティに意見を求めているものです。

2.  **特に興味深いコメント**

以下の2つのコメントが特に興味深いと考えられます。

*   **「totally normal. time series forecasting is really hard. ML options have only become competitive with statistical methods in the last few years, and only in certain scenarios. you can look into the recent developments in ml weather forecasting, but with only one variable you're probably better off sticking with standard statistical methods」**

    このコメントは、時系列予測の難しさを指摘し、機械学習が統計的な手法に匹敵するようになったのはごく最近であり、特定のシナリオに限られる、という点を述べています。また、変数が一つしかない場合は、古典的な統計的手法に固執する方が良いかもしれない、というアドバイスをしています。このコメントは、投稿者の疑問に対する直接的な回答であり、Linear Regressionが優れていることが必ずしも異常ではない、ということを示唆しています。また、より高度な機械学習手法に固執するのではなく、問題設定によっては単純な手法が有効であることを示唆する点で重要です。

*   **「Could simply be the case. It's important to check if your model does not simply predict values near the latest values, i.e. basically predicts a delta of 0. This might get decent performance but would not be usable.」**

    このコメントは、モデルが単に直近の値に近い値を予測しているだけでないか、つまり変化量をほとんど0と予測していないかを確認することの重要性を指摘しています。このようなモデルは一見すると良いパフォーマンスを示すかもしれませんが、実際には使い物にならない可能性があると述べています。このコメントは、モデルの評価方法に対する重要な注意点を示唆しており、MAEなどの評価指標だけでなく、モデルが実際に意味のある予測を行っているかどうかの検証が必要であることを強調しています。特に時系列予測においては、安易に直近の値を予測するようなモデルに陥りやすいため、注意が必要です。


---

# [R] Lumina-Image 2.0: Efficient Text-to-Image Generation via Unified Architecture and Progressive Training

**Upvotes**: 11



[View on Reddit](https://www.reddit.com/r/MachineLearning/comments/1jn6ttj/r_luminaimage_20_efficient_texttoimage_generation/)

1. **ポストの内容の説明**

このRedditの投稿は、Lumina-Image 2.0という新しい画像生成モデルに関するものです。このモデルは、従来の画像生成モデルとは異なり、以下の点で画期的です。

*   **統一アーキテクチャ**: 複数の画像生成タスク（テキストから画像生成、画像編集、インペインティング、アウトペインティング）を、個別のモデルではなく、単一のTransformerベースのモデルで処理します。これは、LLM（大規模言語モデル）がテキストを処理する方法と同様に、画像をトークンのシーケンスとして扱うことによって実現されています。
*   **MSIRサンプリング**: Multiple Sampling with Iterative Refinement (MSIR) という新しいサンプリング技術を使用しています。これは、複数の候補画像を同時に生成し、その中で最も有望なものを選択的に改良することで、計算コストを増やすことなく画質を向上させる技術です。
*   **並列デコードとディープフュージョン**: 複数のトークンを並行して処理し、その結果を融合することで、推論速度を大幅に向上させています。
*   **高い性能と効率**: COCOデータセットで4.11 FIDという高いスコアを達成し、以前の最先端技術を上回りながら、トレーニングに必要な計算量を38%削減しています。また、モデルのスケールアップ（パラメータ数の増加）による性能向上が効率的に行われています。

投稿者は、このモデルが画像生成アーキテクチャにおける重要な転換点となると考えています。特に、MSIR技術は、計算コストを抑えながら画質を向上させる賢い方法であると評価しています。また、トレーニングに必要な計算量が削減されていることも、AIの環境への影響を考慮する上で重要であると指摘しています。

投稿のまとめとして、Lumina-Image 2.0は、複数のタスクにおいて最先端の画像生成を、専用のアーキテクチャではなく単一のTransformerベースのモデルで実現し、MSIRという新しいサンプリングアプローチによって、品質を向上させながら計算コストを削減していると述べています。

2. **興味深いコメント**

この投稿にはまだコメントがありません。

---

# [D] Minimising focal loss but log loss exceeds base rate

**Upvotes**: 2



[View on Reddit](https://www.reddit.com/r/MachineLearning/comments/1jnc4vq/d_minimising_focal_loss_but_log_loss_exceeds_base/)

1.  **ポストの内容の説明**

このRedditのポストは、投稿者が抱える機械学習モデルの課題について相談するものです。具体的には、以下の内容が含まれています。

*   **課題の概要:** 投稿者は、顧客のチャーン（解約）を予測するモデルを開発しようとしています。しかし、顧客の実際のチャーンの定義が曖昧であるため、過去30日間の行動に基づいて、翌日のトランザクション（取引）の可能性を予測し、その予測の変化からチャーンのリスクを判断するアプローチをとっています。

*   **データの問題点:** データセットは、トランザクションが発生しない日が多い（20:1の割合）という不均衡な分布を持っています。そのため、単純なモデル（ベースラインモデル）でも高い精度（95%）を達成できますが、実際には役に立ちません。

*   **モデルと損失関数:** LSTM（Long Short-Term Memory）モデルを使用し、二値交差エントロピー損失（binary log loss）とFocal Lossを試しています。二値交差エントロピー損失を最小化すると、モデルはすぐに0に収束してしまいます。Focal Lossを使うと精度は向上するものの、確率の校正がうまくいかず、ログ損失がベースラインの値を下回ることができません。

*   **困難な点:** 予測モデルが顧客エンゲージメントの潜在的な特徴をうまく捉えているかどうかを判断する方法がありません。また、ネガティブサンプリング（トランザクションが発生しない日のデータを減らす）はデータパイプラインの複雑さからまだ試せていません。

*   **検討中の解決策:** ユーザーが取引を行った日のみをサンプルに含め、最後に取引を行った日からの経過日数を特徴量として追加することを検討しています。

**要約すると、投稿者はデータの不均衡、損失関数の選択、モデルの評価方法など、複数の課題に直面しており、コミュニティからのアドバイスを求めている状況です。**

2.  **興味深いコメント**

現在、この投稿にはコメントがありません。


---

# [N] [P] Transformer model made with PHP

**Upvotes**: 10



[View on Reddit](https://www.reddit.com/r/MachineLearning/comments/1jn11wq/n_p_transformer_model_made_with_php/)

1.  **ポストの内容の説明:**

    このRedditのポストは、PHPで機械学習を行うためのライブラリ「Rindow Neural Networks」の新しいバージョン2.2のリリース告知です。このバージョンにはTransformerモデルのサンプルが含まれており、Transformerモデルの作成チュートリアルも公開されています。

    投稿文では、Rindow Neural NetworksがPHPの高レベルなニューラルネットワークライブラリであり、PHPで強力な機械学習を可能にすることが強調されています。DNN、CNN、RNN、(multi-head) attentionなどの機械学習モデルを構築でき、PythonやKerasの知識を活用できること、コンピュータビジョンや自然言語処理のサンプルが利用可能であること、高速な計算ライブラリによってTensorFlowのCPU版と同等の速度でデータ処理が可能であること、専用の機械学習環境が不要で安価なラップトップでも動作すること、NVIDIA GPUが必須ではないことなどが利点として挙げられています。

    また、Rindow Neural Networksが「推論専用ライブラリではない」「他の機械学習フレームワークのPHPバインディングではない」「AI Webサービスを呼び出すためのライブラリではない」ことが明記されています。

2.  **特に興味深いコメント:**

    このポストに対するコメントで特に興味深いのは以下の3つです。

    *   **「Why. (なぜ？)」**: 最も多くの賛同を得ているこのコメントは、PHPで機械学習を行うことに対する疑問や驚きを率直に表しています。これは、機械学習分野でPHPが一般的な選択肢ではないことを反映しています。
    *   **「Why would anyone use this and not Python? Lmao (なぜPythonではなくこれを使う人がいるんだ？笑)」**: このコメントも同様に、機械学習で広く使われているPythonと比較して、PHPを使うメリットが見当たらないという疑問を投げかけています。ユーモラスな表現で、PHPでの機械学習に対する一般的な見方を表しています。
    *   **「I'm not the biggest fan of PHP myself, but thanks for this. PHP remains a widely used language, and your work democratizing machine learning is a real win for the community. Kudos. (私もPHPの熱狂的なファンではありませんが、これには感謝します。PHPは依然として広く使用されている言語であり、あなたの機械学習を民主化する取り組みは、コミュニティにとって真の勝利です。称賛します。)」**: このコメントは、PHPに対する個人的な感情はさておき、Rindow Neural NetworksがPHPコミュニティに機械学習の機会を提供することの価値を認めています。機械学習の民主化という視点から、このプロジェクトの意義を評価している点が興味深いです。


---

# It’s been 1000 releases and 5000 commits in llama.cpp

**Upvotes**: 472



[View on Reddit](https://www.reddit.com/r/LocalLLaMA/comments/1jnfpnr/its_been_1000_releases_and_5000_commits_in/)

1.  **このポストの内容:**
    このRedditポストは、llama.cppプロジェクトが1000回目のリリースを迎え、約5000回のコミット（実際には4998回）を達成したことを祝うものです。投稿者は、プロジェクトがLlama 1のリークをきっかけに始まったことを振り返り、チームに感謝の意を表しています。

2.  **特に興味深いコメント:**

    *   **168 upvotesのコメント:** llama.cppの初期の目標とその開発の経緯を示す引用が興味深いです。
        *   「*The main goal is to run the model using 4-bit quantization on a MacBook.*（主な目標は、MacBookで4ビット量子化を使用してモデルを実行することです。）」という初期目標。
        *   「*This was hacked in an evening - I have no idea if it works correctly.*（これは一夜漬けでハックされたもので、正しく動作するかどうかはわかりません。）」という、開発初期のラフな様子。
        *   「*So far, I've tested just the 7B model and the generated text starts coherently, but typically degrades significanlty after ~30-40 tokens.*（これまでのところ、7Bモデルのみをテストしましたが、生成されたテキストは最初は一貫性がありますが、通常は約30〜40トークン後に大幅に劣化します。）」という、初期バージョンの性能に関する記述。
    *   **51 upvotesのコメント:** 「Where did you get the "1000th release" from?（「1000回目のリリース」はどこから来ましたか？）」という質問は、投稿の正確性に対する疑問を提起しており、興味深いです。添付された画像は、投稿者が「1000」という数字の根拠を示していることを示唆しています。


---

# LLMs over torrent

**Upvotes**: 189

![Image](https://i.redd.it/8z6t2vvu3ure1.jpeg)

[View on Reddit](https://www.reddit.com/r/LocalLLaMA/comments/1jnd6px/llms_over_torrent/)

1.  **ポストの内容の説明:**

このRedditのポストは、大規模言語モデル（LLM）をBitTorrentを使って共有するという実験的な試みについて説明しています。投稿者は、Qwen2.5-VL-3B-InstructというLLMモデルをBitTorrentで共有するために、オランダのデータセンターにあるシードボックスにアップロードしました。そして、そのTorrentファイルを共有し、他のユーザーがダウンロードして試せるようにしました。

投稿者は、この実験の動機として、AIの重要性が高まる中で、モデルの配布が一箇所に集中することのリスクを指摘しています。Hugging Faceのようなプラットフォームへの依存は便利ですが、将来的に料金が高騰したり、企業に買収されたり、サービスが停止したりする可能性も考慮し、分散型のバックアップや代替配布方法としてTorrentが有効ではないかと提案しています。

また、共有するモデルのライセンス（Apache-2.0のような再配布可能なもの）や、大規模なモデルを効率的に共有するためのBinary deduplicationやOTAアップデートなどの技術的な課題についても言及しています。

2.  **特に興味深いコメント:**

*   **集中化のリスクと分散型バックアップの必要性:**
    109upvoteのコメントは、Hugging Faceのようなプラットフォームへの集中化のリスクを指摘し、分散型のバックアップの重要性を強調しています。具体的には、料金の高騰、企業への買収、サービス停止といったシナリオを挙げ、Torrentのような分散型システムがそのリスクを軽減する可能性があると述べています。
    さらに、Torrentによる共有のセキュリティ上の懸念（悪意のあるコードの混入）を指摘しつつも、コミュニティによる適切なトラッカーの運営とユーザーフィードバックによるモデレーションによって、この問題を解決できる可能性があると述べています。
*   **IPFSの提案:**
    13upvoteのコメントでは、Torrentの代わりにIPFS（InterPlanetary File System）を使用する方が理にかなっていると主張しています。IPFSはコンテンツアドレス指定を使用し、同じファイルが異なる場所にあってもシードできるため、Torrentよりも効率的な分散型ファイル共有システムになる可能性があると説明しています。
    TorrentのDHT（分散ハッシュテーブル）には多くのデッドリンクが存在し、異なるソフトウェア間での互換性の問題もあることを指摘し、IPFSがこれらの問題を解決する可能性があると述べています。


---

# Benchmark: RTX 3090, 4090, and even 4080 are surprisingly strong for 1-person QwQ-32B inference. (but 5090 not yet)

**Upvotes**: 71



[View on Reddit](https://www.reddit.com/r/LocalLLaMA/comments/1jnjrdk/benchmark_rtx_3090_4090_and_even_4080_are/)

はい、承知いたしました。以下に、ご質問への回答を記述します。

**1. このポストの内容の説明**

このRedditの投稿は、AIコード補完を目的とした、大規模言語モデル(LLM)である`QwQ-32B`推論を、さまざまなGPUを使って実行した場合のベンチマーク結果について述べています。特に、1人で使用する場合に、どのGPUが最も効率的かを検証しています。

*   **背景:** 投稿者は、自身のコードを外部企業に送信せずにAIコード補完を使用したいと考えています。そのため、ローカル環境でLLMをホストすることを選択し、その際のGPUパフォーマンスを評価しました。
*   **方法:** `vLLM`という推論フレームワークと、`QwQ-32B-Q4_K_M`というモデル（量子化されたバージョン）を使用。`median_ttft_ms`（最初のトークンまでの時間の中央値）と`median_otps`（1秒あたりの出力トークン数の中央値）を測定しました。
*   **結果:**
    *   データセンター向けGPU（H200、H100）は、初期のコンテキスト処理では高速だが、出力トークン生成では、より高いクロック速度と冷却性能を持つコンシューマー向けGPU（RTX 4080, 3090Tiなど）に劣る傾向があることを発見しました。
    *   特に、RTX 4080を2枚使用した場合の性能が、H200に匹敵するほど高く、価格を考慮すると非常にコストパフォーマンスが良いことを強調しています。
    *   まだ最適化されていないためか、RTX 5090の結果は期待外れだったようです。
*   **結論:**
    *   RAMのクロック速度が重要であること。オーバークロックが有効であること。
    *   シングルGPU構成よりも、RTX 4080を2枚使用する方が有利である可能性があること。
*   **その他:**
    *   ベンチマークに使用したJSON形式の生データへのリンクと、その他のハードウェア構成の結果へのリンクを提供しています。

**2. このポストに対するコメントのうち、特に興味深いもの**

最も興味深いコメントは、以下のものです。

> My vLLM setup for QwQ-32b-AWQ has around **65 tokens/sec** using a RTX 5090 limited to 400W. I have full 32k token context size, all fitting into 32GB VRAM. I had to use and build FlashInfer as the attention backend. AWQ is the fastest quantization on vLLM. GGUF is not optimized and bnb is slow too.

このコメントが興味深い理由は以下の通りです。

*   **RTX 5090の性能向上:** 投稿者のRTX 5090のパフォーマンスが期待外れだったのに対し、このコメントでは、別の設定（AWQ量子化、FlashInferバックエンド）を使用することで、より高いトークン生成速度（65 tokens/sec）を達成していることを示しています。これにより、RTX 5090のポテンシャルがまだ引き出せていない可能性があることが示唆されます。
*   **具体的な設定:** AWQ量子化とFlashInferという具体的な設定について言及しており、他のユーザーが同様のパフォーマンスを得るためのヒントを提供しています。
*   **技術的な詳細:** vLLMにおける量子化形式（AWQ, GGUF, bnb）のパフォーマンスについても触れており、より深い技術的な議論を促す可能性があります。
*   **32Kコンテキストサイズ:** フル32kトークンコンテキストサイズに対応している点も重要です。大規模なコンテキストサイズは、より複雑なタスクを扱う上で有利になります。
*   **VRAM:** 32GBのVRAMに収まっていることから、メモリ効率の良い設定であることが分かります。

このコメントは、投稿者が直面した問題を解決する可能性のある情報を提供しており、他のユーザーにとっても非常に有益であると考えられます。また、RTX 5090の性能を最大限に引き出すためのさらなる研究の必要性を示唆しています。


---

# MLX fork with speculative decoding in server

**Upvotes**: 24



[View on Reddit](https://www.reddit.com/r/LocalLLaMA/comments/1jnplb1/mlx_fork_with_speculative_decoding_in_server/)

1.  **このポストの内容の説明:**

このRedditの投稿は、mlx-lm（AppleのMLXフレームワークを基にした言語モデルライブラリ）をフォーク（複製・派生）したユーザーが、オリジナルのmlx-lmに機能追加を行ったことを告知するものです。

*   **主な変更点:** speculative decoding（投機的デコード）という技術を、mlx-lmの`server`コマンドに実装しました。
    *   投機的デコードは、より小さな「ドラフトモデル」を使って候補となるトークンを予測し、それを大きな「メインモデル」で検証することで、テキスト生成の速度を向上させる技術です。
*   **目的:** これにより、OpenAIのAPI互換のcompletion endpoint（テキスト生成エンドポイント）を、投機的デコードを活用して高速化できるようになりました。
*   **使用方法:** 投稿には、実際にコマンドを実行する例が示されています。`mlx_lm.server`コマンドに`--draft-model`オプションを追加することで、ドラフトモデルを指定できるようになっています。
*   **性能向上:** 投稿者は、Qwen Coder 0.5Bをドラフトモデル、Qwen Coder 32Bをメインモデルとして使用した場合、90%の速度向上を達成したと報告しています。
*   **今後の予定:** 投稿者は、テストを整理して、この変更をmlx-lmのオリジナルリポジトリにPull Request（変更提案）として提出する予定です。
*   **リポジトリ:** 投稿には、変更が加えられたリポジトリのURL（GitHub）が含まれています。これにより、他のユーザーがコードを試したり、貢献したりできます。

2.  **興味深いコメント:**

最も興味深いコメントは、速度向上に関する具体的な数値データを提供しているものです。

*   **コメント:** "I have an M3 MAX with 128GB memory. Without draft model I was getting 10tks with qwen-coder-32b-8bit. With draft model I get 19tks. This will vary depending on context and other factors."
*   **理由:** このコメントは、投稿者が主張する速度向上を裏付ける具体的な証拠を提供しています。M3 Maxチップ搭載のMacで、Qwen-Coder-32B-8bitモデルを使用した場合、投機的デコードなしで10トークン/秒だった生成速度が、投機的デコードありで19トークン/秒に向上したというデータは、他のユーザーがこの機能を試す動機付けになります。 また、"This will vary depending on context and other factors."という注意書きは、結果が環境によって異なる可能性があることを示唆しており、この技術の限界や実験の必要性を理解する上で重要です。


---

# I built a coding agent that allows qwen2.5-coder to use tools

**Upvotes**: 62

![Image](https://i.redd.it/1erih6euuure1.png)

[View on Reddit](https://www.reddit.com/r/LocalLLaMA/comments/1jngj5u/i_built_a_coding_agent_that_allows_qwen25coder_to/)

はい、承知いたしました。以下に、ご質問への回答を順を追って詳細に説明します。

**1. ポストの内容の説明**

このRedditポストは、投稿者（huytd）が作成したコーディングエージェント「Supercoder」に関するものです。

*   **背景:** LM Studioを使っているユーザーは、qwen2.5-instructのようなツール呼び出しをサポートするモデルがコーディングにおいてはあまり得意ではないことを知っているかもしれません。一方、qwen2.5-coderはコーディング能力は高いものの、ツール呼び出しをサポートしていません。
*   **Supercoderの概要:** 投稿者は、qwen2.5-coderがツールを使用できるようにするターミナルベースのコーディングエージェント「Supercoder」を開発しました。
*   **特徴:**
    *   OpenRouter上の無料モデルでも使用可能。
    *   ソースコードからコンパイルするか、プリビルドされたバイナリ（macOS、Linux、Windowsをサポート）をダウンロードして使用できる。
*   **目的:** ユーザーからのフィードバックを求めています。

**2. 特に興味深いコメント**

このポストに対するコメントの中で、特に興味深いのは以下の2つです。

*   **cline/rooのプロンプト管理:** 最初のコメントは、Supercoderの開発者に対して、`cline`と`Roo-Code`というプロジェクトのプロンプト管理方法が興味深いかもしれないと指摘し、関連するGitHubリポジトリへのリンクを提供しています。これは、プロンプト設計に関する貴重な情報源となる可能性があり、Supercoderの改善に役立つかもしれません。
*   **smolagentsとqwen2.5 coder 32b instruct:** 2番目のコメントは、`smolagents`がqwen2.5 coder 32b instructを使用してツール呼び出しを実現している点を指摘しています。投稿者はqwen2.5 coderにはツール呼び出し機能がないという前提でSupercoderを開発していますが、実際には特定のモデルでツール呼び出しが可能である可能性を示唆しており、投稿者の前提を覆す情報となっています。

これらのコメントは、Supercoderの開発者にとって、プロンプト設計の改善や、qwen2.5-coderモデルのツール呼び出し能力に関する再検討のきっかけとなるかもしれません。


---

# MacBook M4 Max isn't great for LLMs

**Upvotes**: 380



[View on Reddit](https://www.reddit.com/r/LocalLLaMA/comments/1jn5uto/macbook_m4_max_isnt_great_for_llms/)

1.  **ポストの内容の説明**

このRedditのポストは、MacBook M4 Maxがローカルでの大規模言語モデル（LLM）の実行には期待ほどではないという内容です。投稿者は以前のM1 MaxからM4 Maxにアップグレードしたことで、推論速度が大幅に向上（約3倍）したものの、5年前のNvidia RTX 3090（700ドル程度で入手可能）と比較すると、まだかなり遅いと述べています。

投稿者は、大規模モデルをロードできる点は評価しつつも、実用性には疑問を呈しています。例えば、比較的小さな14Bの蒸留されたQwenモデルの4bit量子化版でさえ、コーディングにはかなり遅く（40トークン/秒）、品質も低いと指摘しています。32Bモデルは速度が遅いため、Roo CodeやCline経由ではほとんど使用できないとのことです。

高価なMacBook Proの最上位モデルであっても、ローカルAIの実行には現実的ではないという主張です。本当にローカルAIが必要な場合は、1〜2世代前のNvidiaリグを入手するか、レンタルするか、APIを使用する方が、初期費用なしで品質と速度が大幅に向上すると述べています。

MacBook Proを購入するのであれば、必要最小限のRAMと少し多めのSSDを選択し、ローカルAIには特化したハードウェアを使用することを推奨しています。投稿者はMacBook自体は長年愛用しており素晴らしいと考えていますが、高額な最上位モデルが期待するほどのAI性能を発揮しない可能性があると指摘しています。以前のM1 Maxでも同様の経験があり、最初は大規模モデルを実行できることに感動したものの、上記のような理由から結局使わなくなったと述べています。M4 MaxはM1 Maxより高速ですが、同じような感覚を抱いているとのことです。

2.  **特に興味深いコメント**

*   **パフォーマンス比較に関するコメント（250 upvotes）:** M4 MaxとNvidia製GPUとの性能比較を具体的に示している点が興味深いです。
    *   M4 Maxは、Nvidia P40より約50%高速（計算スループットとメモリ帯域幅の両方）。
    *   Nvidia 3060と比較して、計算スループット（FP16）では約2.5倍遅く、メモリ帯域幅では50%高速。
    *   Nvidia 3090と比較して、計算スループット（FP16）では約7〜8倍遅く、メモリ帯域幅では2倍高速。

    このコメントは、M4 MaxのAI性能に対する客観的な指標を提供し、期待値を適切に設定するのに役立ちます。GPUの種類ごとの比較をすることで、ユーザーが具体的な用途に合わせてハードウェアを選択する際の判断材料になります。

*   **LMStudioと投機的デコーディングに関するコメント（90 upvotes）:** LMStudioの使用と、投機的デコーディングによる速度向上を提案している点が興味深いです。LMStudioとMLXを使用し、0.5Bのドラフトモデルを14Bモデルの投機的デコーディングに使用することで、M1 Maxでの速度が3倍になったという具体的な結果が示されています。これは、ハードウェアの限界をソフトウェアで補う方法の提案であり、MacBookユーザーにとって有益な情報です。


---

# I think I found llama 4 - the "cybele" model on lmarena. It's very, very good and revealed it name ☺️

**Upvotes**: 76



[View on Reddit](https://www.reddit.com/r/LocalLLaMA/comments/1jnbhdl/i_think_i_found_llama_4_the_cybele_model_on/)

はい、承知いたしました。以下に回答いたします。

1.  **このポストの内容の説明**

このRedditのポストは、あるユーザーがlmarenaというプラットフォーム上で「cybele」というモデルを見つけ、それがLlama 4である可能性があると示唆するものです。ユーザーは、このモデルが非常に優れていると感じ、他のユーザーに同様の経験があるかどうか尋ねています。

*   **タイトル:** 「Llama 4を見つけたと思う - lmarena上の"cybele"モデル。非常に、非常に優れていて、名前を明らかにした。」
*   **投稿文:** 「このモデルで同様の経験をした人はいますか？」

つまり、このポストは、新しいモデル（cybele）の発見と、その性能に対する最初の印象を共有し、他のユーザーからのフィードバックや意見を求めているものです。

2.  **このポストに対するコメントのうち、特に興味深いもの**

興味深いコメントは、cybeleモデルに対する具体的な評価と、モデルの能力に関する詳細な議論を提供している最後のコメントです。

*   **コメント:** 15 upvotesのコメントは、cybeleモデルが他のチャットボットよりも優れているという個人的な経験を述べています。さらに、複雑な技術的な質問（「on-policy reinforcement learningにおけるsample inefficiencyとは何か」）に対するcybeleの回答を引用しています。この回答は非常に詳細で、専門的な知識を示しており、モデルの高度な理解力と説明能力を強調しています。このコメントが特に興味深いのは、実際の使用例を通してモデルの性能を具体的に示している点です。回答の正確さと深さは、cybele（またはLlama 4である可能性）が、高度な言語理解と生成の能力を持っていることを示唆しています。また、回答にはユーモアが交えられており、モデルが人間らしい自然な対話ができることも示唆されています。

