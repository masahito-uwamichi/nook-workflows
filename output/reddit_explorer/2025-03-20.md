
# [D] ICCV 2025 Desk Reject for Appendix in Main Paper – Anyone Else?

**Upvotes**: 30



[View on Reddit](https://www.reddit.com/r/MachineLearning/comments/1jf2rg4/d_iccv_2025_desk_reject_for_appendix_in_main/)

はい、承知いたしました。以下に質問への回答を記述します。

1.  **このポストの内容の説明**

このRedditのポストは、ICCV 2025（国際コンピュータビジョン会議）に論文を投稿したユーザーが、論文が「desk reject（内容を審査される前に却下されること）」されたことについて述べています。

*   **問題点:** 著者は、論文のメインPDFに補足資料（supplementary material）をAppendix（付録）として含めたことが原因で、ページ数制限を超過したとして却下されました。
*   **背景:** 今年のICCVでは、メインの論文と補足資料を同じ期日に提出する必要があったため、著者は（誤って）それらを同じドキュメントに含めるべきだと解釈しました。
*   **著者の主張:** NeurIPSやACLなどの他の主要な会議では、補足資料の締め切りがメイン論文と同じ場合、AppendixをメインPDFに含めることは一般的です。そのため、今回の却下は不当だと感じています。
*   **質問:** 著者は、同じような間違いを犯した人が他にいるかどうか、また、その論文も却下されたかどうかを知りたいと考えています。

要するに、ICCVのフォーマットに関するルール解釈の誤りが原因で論文が却下されたことに対する不満と、同様の経験をした人がいるかどうかを尋ねる内容です。

2.  **特に興味深いコメント**

以下の2つのコメントが特に興味深いと思われます。

*   **16 upvotesのコメント:** このコメントは、ICCVには投稿していないものの、投稿者が遭遇した経験から、著者の状況に共感しています。
    *   「会議の半分以上が、付録を別のファイルとして要求するのが一般的である」と述べており、ICCVのルールが特別ではない可能性を示唆しています。
    *   「supplementary material」と「appendix」はしばしば異なるものとして扱われることを指摘し、ICCVがこれらの区別に厳格である可能性を示唆しています。例えば、ICMLではメインPDFにappendixを含めることはできるが、それとは別にsupplementary material（別の付録、コード、Excelファイルなど）も提出できるとのことです。
    *   このコメントは、著者が「supplementary material」と「appendix」を混同した可能性を示唆し、desk rejectの理由に対する洞察を与えています。
*   **3 upvotesのコメント:** WACV（IEEE Winter Conference on Applications of Computer Vision）でも同様の経験をした人がいることを報告しており、同様の問題が他の会議でも発生していることを示しています。これは、著者の不満を裏付ける証拠となります。

これらのコメントは、著者にとって、①ICCVのルールがそれほど特殊ではないこと、②「supplementary material」と「appendix」の区別の重要性、③同様の問題に直面した人が他にもいる、という点で有益な情報を提供していると考えられます。


---

# [D] resources for the score based generative models?

**Upvotes**: 4



[View on Reddit](https://www.reddit.com/r/MachineLearning/comments/1jf6lxm/d_resources_for_the_score_based_generative_models/)

1. **ポストの内容の説明**

このRedditのポストは、ユーザーが「スコアベース生成モデル」について、初心者向けの学習リソースを探しているものです。

*   **スコアベース生成モデルとは何か？:** スコアベース生成モデルとは、機械学習における生成モデルの一種です。生成モデルは、既存のデータセットから学習し、そのデータに似た新しいデータを生成することを目的とします。スコアベース生成モデルは、データの「スコア関数」（データの密度関数の勾配）を学習することによって、データを生成します。

*   **投稿の背景:** 投稿者は、スコアベース生成モデルを学びたいと考えていますが、既存の資料（ビデオ、ブログ、論文など）は数学的な説明に偏っており、理解するのが難しいと感じています。そのため、より初心者でも理解しやすい入門的な資料を探しています。

2. **ポストに対するコメントのうち、特に興味深いもの**

このポストにはまだコメントがありません。


---

# [D] Should my dataset be balanced?

**Upvotes**: 15



[View on Reddit](https://www.reddit.com/r/MachineLearning/comments/1jeueo1/d_should_my_dataset_be_balanced/)

1.  **ポストの内容の説明**

このRedditポストは、機械学習プロジェクトにおけるデータセットのバランスに関する質問です。投稿者は、水漏れを検知するデータセットを作成中で、チーム内でデータセットのバランスをどうすべきかで意見が分かれています。

*   **問題提起:** データセットをバランスさせる（水漏れあり/なしのサンプル数を500/500にする）か、現実世界の状況を反映してアンバランスにする（850/150）か。現実世界では水漏れはそれほど頻繁に起こらないため、アンバランスなデータセットの方がより現実的であると考えられます。

*   **背景:** 投稿者とそのチームは、大学のプロジェクトに取り組んでおり、機械学習の初心者です。データセットのバランスに関する適切なアプローチが分からず、アドバイスを求めています。

2.  **特に興味深いコメント**

このポストに対するコメントで特に興味深いのは以下の3つです。

*   **45 upvotesのコメント:**
    *   アンバランスなデータセット（85%ネガティブ、15%ポジティブ）は、それほど不均衡ではないため、修正する必要はないと述べています。
    *   重要な点として、データセットの分布が実際のデータの分布を反映しているかを問うています。これは、現実世界の状況をモデルに反映させる上で重要な考慮事項です。

*   **42 upvotesのコメント:**
    *   テストセットは実際のデータを代表するものであるべきであり、その評価には精度（accuracy）ではなく、F1スコアやAUCといった指標を用いるべきだと指摘しています。
    *   トレーニングセットについては、バランスを取るためにオーグメンテーションや重み付きサンプリングなどのテクニックを使用できると提案しています。

*   **9 upvotesのコメント:**
    *   50/50のバランスが情報理論的には最適ですが、実際には10:1程度のより大きな不均衡でない限り、必ずしも効果があるとは限らないと経験に基づいたアドバイスをしています。
    *   テストセットが現実を反映する必要があるという意見に対し、それは必ずしも真実ではなく、オーバーサンプリング/アンダーサンプリングの影響は指標を調整することで補正できると述べています。また、現実世界ではクラス比率は常に変動するため、いずれにせよ調整が必要になると指摘しています。

**興味深い理由:**

これらのコメントが興味深いのは、データセットのバランスに関する異なる視点を提供している点です。

*   現実世界の分布を反映することの重要性
*   モデルの評価指標の選択
*   トレーニングデータとテストデータの扱い方の違い
*   バランス調整が常に有効とは限らないという経験則

これらのコメントは、投稿者だけでなく、データセットのバランスに関する問題を抱える他の機械学習初心者にとっても貴重な情報源となるでしょう。


---

# [R] Evaluating Video Models on Impossible Scenarios: A Benchmark for Generation and Understanding of Counterfactual Videos

**Upvotes**: 7



[View on Reddit](https://www.reddit.com/r/MachineLearning/comments/1jeva17/r_evaluating_video_models_on_impossible_scenarios/)

はい、承知いたしました。以下の通り、質問に回答します。

1.  **このポストの内容の説明**

このRedditのポストは、動画生成AIモデルの物理的な整合性を評価するための新しいベンチマーク「IPV-Bench」を紹介しています。

*   **概要:** IPV-Benchは、動画生成モデルが基本的な物理法則や論理をどれだけ理解しているかをテストするために作成されました。重力違反、物体の永続性、論理的な矛盾など、9つのカテゴリーにわたる、物理的にあり得ないシナリオを扱う能力をテストする1,000個のプロンプトが含まれています。
*   **方法論:** モデルに対して「あり得ないことを作成する」プロンプトと「あり得ないことを避ける」プロンプトの両方を使用し、生成された動画を自動指標と人間の評価によって評価しました。Sora、Morph-E、WALTなど、最先端のモデルを複数テストしました。また、物理的にあり得ないシナリオの詳細な分類を作成しました。
*   **主な発見:** 最新のモデルでも、物理法則に従うように明示的に指示された場合でも、20〜40％の確率で物理的にあり得ないコンテンツを生成しました。「変化の不可能性」や「接触の不可能性」に対する精度が特に低く、モデルによって物理的な推論における誤りの種類が異なりました。テキストの理解度が高くても、物理的な推論能力が高いとは限りませんでした。人間は容易にあり得ない点を識別でき、AIと人間の理解のギャップが浮き彫りになりました。
*   **重要性:** 現在の動画生成システムには、人間が自然に身につける直感的な物理的理解が欠けていることを示唆しています。シミュレーション、教育、ロボット工学システムのトレーニングなど、物理的な整合性が重要なアプリケーションにおいて、この問題は重大です。IPV-Benchは、この分野の進歩を体系的に測定する方法を提供し、今後のモデル開発において重要なツールとなると考えられます。

要するに、IPV-Benchは、動画生成モデルが物理的にあり得ない状況を理解しているかどうかをテストする新しいベンチマークであり、現在のモデルは、物理的な世界の仕組みを真に理解していないことを示唆しています。

2.  **このポストに対するコメントのうち、特に興味深いもの**

この投稿にはコメントがありません。


---

# [P] Satellite Image dataset for Cyclone prediction

**Upvotes**: 1



[View on Reddit](https://www.reddit.com/r/MachineLearning/comments/1jf83z9/p_satellite_image_dataset_for_cyclone_prediction/)

1.  **ポストの内容の説明:**

    このRedditのポストは、サイクロン（熱帯低気圧）の予測に使用できる衛星画像データセットに関する質問です。投稿者は、特にインドの特定の州におけるサイクロン予測のために、衛星画像データセットを探しています。彼は、インド気象局 (mausam.imd.gov.in) のウェブサイトからデータセットを作成する方法についてアドバイスを求めています。つまり、投稿者は、サイクロン予測モデルをトレーニングするために利用できる衛星画像データをどのように収集し、整理すればよいかについて助けを求めていることになります。

2.  **ポストに対するコメントのうち、特に興味深いもの:**

    現在、このポストにはコメントがありません。そのため、特に興味深いコメントを挙げることはできません。


---

# [R] Jagged Flash Attention Optimization

**Upvotes**: 79



[View on Reddit](https://www.reddit.com/r/MachineLearning/comments/1je93sv/r_jagged_flash_attention_optimization/)

はい、承知いたしました。以下に質問への回答を示します。

1.  **このポストの内容の説明**

このRedditの投稿は、Metaの研究者たちが開発した「Jagged Flash Attention」という新しい技術を紹介しています。この技術は、大規模な推薦システムの性能とスケーラビリティを大幅に向上させることを目的としています。

*   **Jagged Flash Attentionとは:** Jagged Flash Attentionは、jagged tensor（不均一な形状のテンソル）とFlash Attention（高速なAttentionメカニズム）を組み合わせたものです。
*   **利点:**
    *   **高速化:** 従来のdense attentionと比較して最大9倍の高速化を実現します。Dense Flash Attentionと比較しても3倍の高速化が可能です。
    *   **省メモリ化:** dense attentionと比較して最大22倍のメモリ削減を実現します。Dense Flash Attentionと比較しても53%メモリ効率が向上します。
*   **応用:** 特に、スパースな可変長のバッチサイズを必要とする推薦システムやAttentionモデルに有効です。
*   **実用的な効果:** 実運用モデルにおいて、1秒あたりのクエリ数(QPS)が10%向上し、メモリ使用量が18%削減されることが確認されています。

2.  **特に興味深いコメント**

この投稿に対するコメントの中で、特に興味深いものは以下の3点です。

*   **実用的な効果に関するコメント:** 「The practical impact of these optimizations is substantial, with production models demonstrating a 10% improvement in Queries Per Second (QPS) and an 18% reduction in memory usage.」 このコメントは、Jagged Flash Attentionが実際のシステムにおいてどれほど効果があるかを示しており、非常に重要です。QPSの向上とメモリ使用量の削減という具体的な数値が示されているため、技術的な関心だけでなく、ビジネス的なインパクトも理解することができます。

*   **実装を待つ声:** 「Waiting for the implementation!」多くのユーザーがこの技術の実装を待ち望んでいることがわかります。新しい技術に対する期待の高さを示すものです。

*   **論文へのリンク:** 「[This](https://arxiv.org/abs/2409.15373) seems to be the paper the blog post is based on.」ブログ記事の元となっている論文へのリンクが提供されている点は重要です。ブログ記事だけでなく、論文を読むことで技術的な詳細をより深く理解することができます。


---

# [R] RWKV-7 "Goose" with Expressive Dynamic State Evolution

**Upvotes**: 11



[View on Reddit](https://www.reddit.com/r/MachineLearning/comments/1jenmqz/r_rwkv7_goose_with_expressive_dynamic_state/)

はい、承知いたしました。以下に、ご質問に対する回答を記載します。

1.  **このポストの内容の説明**

このRedditのポストは、新しいシーケンスモデリングアーキテクチャであるRWKV-7 "Goose"を紹介するものです。具体的には、以下の内容が含まれています。

*   **RWKV-7 "Goose" の発表:** 新しいアーキテクチャであるRWKV-7 "Goose"が発表されました。これは、特に多言語タスクにおいて、30億パラメータ規模で最先端の性能を発揮する言語モデルです。従来の最先端の英語言語性能にも匹敵しますが、学習に使用するトークン数は他のトップ3Bモデルよりも大幅に少ないです。
*   **RWKV-7の特徴:** RWKV-7は、メモリ使用量と推論時間がトークンあたり一定であるという利点があります。また、ベクトル値ゲーティングとインコンテキスト学習率を備えたデルタルールの新しい一般化された公式を導入しています。これにより、状態追跡や正規言語の認識が可能になり、並列化可能な学習を維持します。
*   **性能:** RWKV-7は、標準的な複雑性に関する仮説の下では𝖳𝖢0に制限されているTransformerの能力を超えています。
*   **学習データセット:** 3.1兆トークンの拡張されたオープンソース多言語コーパスを用いて、0.19億から29億パラメータまでの4つのRWKV-7モデルを学習させています。
*   **リソースの公開:** モデルとデータセットのリスト、トレーニングおよび推論コードは、Apache 2.0ライセンスの下で公開されています。
*   **関連リンク:** Hugging Face、GitHub、公式ウェブサイトへのリンクが提供されています。

簡単にまとめると、この投稿は、効率的なシーケンスモデリングアーキテクチャであるRWKV-7 "Goose"を発表し、その特徴、性能、利用可能なリソースについて説明しています。

2.  **このポストに対するコメントのうち、特に興味深いもの**

以下のコメントが特に興味深いと考えられます。

*   **「Has anyone fine tuned an rwkv on reasoning traces? Does it work as well as transformers of similar size?」**

    *   RWKVモデルを推論トレースでファインチューンした人がいるかどうか、そして同様のサイズのTransformerと同程度の性能を発揮するかどうかを尋ねています。これは、RWKVの推論能力に関する具体的な質問であり、このモデルの実用的な応用可能性を探る上で重要なポイントです。

*   **「As of Sunday, the creator of RWKV had posted that they have a ~0.5B reasoning model 75% through the training process and a 1.5B model 32% trained; they're calling the model family RWKV7-G1. I'm not sure what methods they're using exactly.」**

    *   RWKVの開発者が、推論モデル（RWKV7-G1）のトレーニングを進めているという情報を提供しています。0.5Bモデルが75%、1.5Bモデルが32%完了しているとのことです。これは、開発の進捗状況に関する貴重な情報であり、今後の展開を予測する上で役立ちます。また、具体的なトレーニング方法については不明であるものの、開発が進んでいることが伺えます。

これらのコメントは、RWKVモデルの実用的な応用や開発の進捗状況に関する具体的な情報を提供しており、RWKV-7 "Goose"の可能性を評価する上で非常に興味深いと言えます。


---

# New RTX PRO 6000 with 96G VRAM

**Upvotes**: 264

![Image](https://i.redd.it/cost3vsw9ppe1.jpeg)

[View on Reddit](https://www.reddit.com/r/LocalLLaMA/comments/1jf5ufk/new_rtx_pro_6000_with_96g_vram/)

1. **ポストの内容の説明**

このRedditのポストは、NVIDIAの新しいグラフィックボード「RTX PRO 6000」に関するものです。

*   **タイトル:** 「New RTX PRO 6000 with 96G VRAM」は、このグラフィックボードが96GBのVRAMを搭載していることを強調しています。
*   **投稿文:** 投稿者は、NVIDIAのGTC（GPU Technology Conference）でこのカードを見たことを報告しています。そのデザインは「5090FE」（未発表のハイエンドGPU）と非常に似ており、冷却システムも同じものが使用されているようです。

要するに、このポストは、新しいハイエンドGPU「RTX PRO 6000」の登場と、その外観に関する初期情報を提供しています。

2. **特に興味深いコメント**

このポストに対するコメントで特に興味深いのは以下の2点です。

*   **「I wonder what makes it 'workstation'.」**: このコメントは、「RTX PRO 6000」が「ワークステーション」向けとされている理由に対する疑問を投げかけています。一般的に、ワークステーション向けのグラフィックボードは、ゲーム用と比較して、特定のプロフェッショナルな用途（CAD、映像編集、科学計算など）に最適化されています。このコメントは、RTX PRO 6000がどのような点でワークステーション用途に特化しているのか、ユーザーが関心を持っていることを示唆しています。
*   **「It’s not that it’s faster, but that now you can fit some huge LLM models in VRAM.」**: このコメントは、96GBという大容量VRAMの重要性について言及しています。特に、大規模言語モデル（LLM）のようなメモリを大量に消費するアプリケーションにおいて、より大きなモデルをVRAMに収めることができるという利点を指摘しています。これは、RTX PRO 6000がAI開発や研究などの分野で大きな可能性を秘めていることを示唆しています。

---

# A man can dream

**Upvotes**: 765

![Image](https://i.redd.it/cw3hsv4mwmpe1.png)

[View on Reddit](https://www.reddit.com/r/LocalLLaMA/comments/1jev3fl/a_man_can_dream/)

1. **ポストの内容の説明**

   *   **タイトル:** 「A man can dream」（人は夢を見ることができる）
   *   **内容の推測:** タイトルから、投稿者が何らかの願望や夢を持っていることがわかります。ただし、具体的な内容はタイトルからは不明です。コメントから推測すると、何らかのAIモデルの新しいバージョンやリリースを夢見ているようです。
   *   **補足:** Redditの文脈では、このタイトルはしばしば皮肉めいたニュアンスを含むことがあります。つまり、実現可能性が低い願望を表現している可能性があります。

2. **興味深いコメントの解説**

   *   **492 upvotes:** 「Appropriate reminder that R1 came out less than 60 days ago.」（R1が60日も経たずにリリースされたことを適切に思い出させてくれる。）

     *   **興味深い点:** このコメントは、AIモデル「R1」が比較的最近リリースされたばかりであることを指摘しています。これは、投稿者が新しいモデル（例えばR1の改良版）を夢見ていることに対して、時期尚早であるというニュアンスを含んでいる可能性があります。つまり、「まだR1が出たばかりなのに、もう新しいものを夢見ているのか」という皮肉めいた反応です。
   *   **110 upvotes:** 「man I'm just waiting for qwen 3 coder」（ああ、qwen 3 coderを待っているんだ。）

     *   **興味深い点:** このコメントは、具体的なAIモデル「qwen 3 coder」を待ち望んでいることを表明しています。これは、投稿者の夢（タイトル）が、特定のAIモデルのリリースに関連している可能性を示唆しています。「coder」という言葉から、プログラミング支援に特化したAIモデルであると推測できます。このコメントは、他のユーザーも同様の期待を抱いていることを示しており、共感を呼んだため、高いupvote数を得ていると考えられます。
   *   **33 upvotes:** 「QwQ-Max」

     *   **興味深い点:** このコメントは、「QwQ」という名称に「-Max」を付け加えたものです。「QwQ」は、目を潤ませている顔文字として使用されることが多く、ここでは何らかの感情（期待、興奮、あるいは落胆など）を表現していると考えられます。「-Max」が何を意味するのかは文脈から判断しづらいですが、元のAIモデル（「QwQ」に関連するもの）の性能向上版を期待している、あるいは皮肉的に表現している可能性が考えられます。


---

# Apache TTS: Orpheus 3B 0.1 FT

**Upvotes**: 110



[View on Reddit](https://www.reddit.com/r/LocalLLaMA/comments/1jf6igq/apache_tts_orpheus_3b_01_ft/)

はい、承知いたしました。以下に質問への回答を順を追って説明します。

**1. このポストの内容の説明**

このRedditのポストは、Apacheライセンスのテキスト読み上げ (TTS) モデルである "Orpheus 3B 0.1 FT" がリリースされたことを告知するものです。

*   **ポイント:**
    *   **Apacheライセンスであること:** これは、モデルがオープンソースであり、比較的自由に使用、変更、配布できることを意味します。
    *   **3B (30億パラメータ) モデルであること:** TTSの分野では、比較的大規模なモデルであり、高品質な音声合成が期待できます。
    *   **Finetuned (ファインチューン) であること:** 特定のデータセットで事前学習されたモデルを、さらに別のデータセットで学習させることで、特定のタスク（この場合は高品質なTTS）に最適化されていることを意味します。
*   **投稿内容:**
    *   投稿者は、このモデルが自身のモデルではないことを明記しています。
    *   モデルの重み (weights) へのリンク、コードリポジトリ、ブログ記事へのリンクを提供しています。
    *   デモサンプルを再現できる点を評価しています。
    *   Canopy Labsが開発したモデルのようです。
*   **全体として:**
    このポストは、オープンソースの高品質なTTSモデルがリリースされたことを歓迎し、その情報を提供することを目的としています。

**2. このポストに対するコメントのうち、特に興味深いもの**

コメントの中で特に興味深いのは、以下の点です。

*   **SesameのCSM-1Bとの比較:** 「Bruh, this basically just killed Sesame's CSM-1B release.」「I've completely forgotten about Sesame by now.」というコメントは、Orpheus 3Bが、既存の類似モデル（SesameのCSM-1B）よりも優れている可能性があることを示唆しています。オープンソース界隈では、より高性能なモデルが登場することで、既存モデルがすぐに陳腐化することがあります。
*   **感情表現の制御:** 「The demo sounds nice. You can put speech modifier tags into the input text (or just let a LLM generate them): happy, normal, digust, disgust, longer, sad, frustrated, slow, excited, whisper, panicky, curious, surprise, fast, crying, deep, sleepy, angry, high, shout」というコメントは、このモデルがテキストに特定のタグを埋め込むことで、音声の感情やスタイルを制御できる可能性を示唆しています。これは、TTSの表現力を高める上で非常に重要な機能です。
*   **インストールに関する問題:** 「The install fails for me at `pip install orpheus-speech` as their extensive dependencies contain the Linux-only version of vLLM. It would've been nice to let users decide for themselves to use regular transformers. The example code in the readme contains something that looks like a copy/paste error and won't work.」というコメントは、実際にモデルを使用しようとしたユーザーが、インストールや実行に問題があることを報告しています。これは、モデルの使いやすさやアクセシビリティに関する重要な情報です。
*   **感情表現タグの認識:** 「I've briefly tested it on the HF demo before it went 404. The speech modifier tags were not recognized, but spoken. Maybe I didn't use them correctly.」というコメントは、前述の感情表現タグが、必ずしも期待通りに機能しない可能性があることを示唆しています。

これらのコメントは、Orpheus 3Bの潜在的な利点と欠点、そして実際の使用感に関する貴重な情報を提供してくれます。


---

# only the real ones remember

**Upvotes**: 382

![Image](https://i.redd.it/dh21r5dq5npe1.jpeg)

[View on Reddit](https://www.reddit.com/r/LocalLLaMA/comments/1jevzm3/only_the_real_ones_remember/)

はい、承知いたしました。順を追って詳細に、分かりやすく説明します。

**1. このポストの内容の説明**

このRedditのポストのタイトルは "only the real ones remember" (本物だけが覚えている) です。これは、特定の人物や出来事を知っていることが、コミュニティ内での特別な存在であることを示唆するフレーズです。

つまり、このポストは、特定の人物（おそらく「TheBloke」というユーザー）について、古くからのコミュニティメンバーだけが知っているだろう過去の功績や状況を振り返ることを意図していると思われます。投稿された画像は、TheBloke氏のHugging Faceページへのブックマークを示しており、この人物がHugging Faceコミュニティにとって重要な存在であったことを暗示しています。

**2. このポストに対するコメントのうち、特に興味深いもの**

以下の3つのコメントが特に興味深いと言えます。

*   **"I bookmarked [https://huggingface.co/TheBloke](https://huggingface.co/TheBloke) so every time I open hugging face it brings me there." (112 upvotes)** このコメントは、投稿者がTheBloke氏のHugging Faceページをブックマークし、Hugging Faceを開くたびにそのページにアクセスするように設定していることを示しています。これは、TheBloke氏に対する非常に強いリスペクトと、彼の活動を常にチェックしたいという気持ちの表れと考えられます。TheBloke氏のHugging Faceページへのブックマークを画像として共有することで、他のユーザーに共感を呼びかけ、TheBloke氏の功績を再認識させる効果もあります。
*   **"What happened to him? Company gone bust? https://bloke.ai/ seems dead" (76 upvotes)** このコメントは、TheBloke氏の現状に対する疑問を投げかけています。彼のウェブサイト（bloke.ai）が機能していないことから、彼の会社が倒産したのではないかと推測しています。このコメントは、TheBloke氏が以前は会社を経営していたこと、そして現在彼の身に何が起こったのかを知りたいというコミュニティの関心を示しています。
*   **"He led us through the dark times when it needed two degrees in mathematics to do quantization." (96 upvotes)** このコメントは、TheBloke氏が過去に量子化を行うのが非常に難しかった時代に、コミュニティを導いた重要な役割を果たしたことを強調しています。つまり、彼は量子化技術の発展に貢献し、多くの人々が利用できるようにした恩人であると言えます。このコメントは、彼の功績を称賛し、彼に対する感謝の気持ちを表しています。

これらのコメントから、TheBloke氏はHugging Faceコミュニティにおいて、量子化技術の普及に貢献した重要な人物であり、現在は活動状況が不明であることがわかります。

---

# If "The Model is the Product" article is true, a lot of AI companies are doomed

**Upvotes**: 273



[View on Reddit](https://www.reddit.com/r/LocalLLaMA/comments/1jex61b/if_the_model_is_the_product_article_is_true_a_lot/)

はい、承知いたしました。以下に、ご質問への回答を記載します。

1.  **ポストの内容の説明**

このRedditのポストは、ブログ記事「The Model is the Product」の内容を元に、AI業界の将来について議論しています。投稿者は、主要なAIラボ（OpenAIやAnthropicなど）が、自社のモデルをよりエージェントのように、つまり自律的にタスクを実行できるように訓練する方向に進んでいると指摘しています。

具体的には、モデルがツールを選択し活用する能力を学習することで、現在多くのAI企業が専門としているアプリケーション層の複雑さが解消される可能性があると述べています。もしこの傾向が続けば、多くのAI企業は、最終的にAIラボ自身と直接競合することになるかもしれません。

DataBricksのAI担当VPの予測として、「クローズドモデルのAIラボは2～3年以内にAPIを閉鎖する」という意見を紹介しています。これは非常に大胆な予測ですが、投稿者は完全にありえない話ではないと考えています。

要するに、この投稿は、AIラボがモデルの能力を向上させるにつれて、アプリケーション層に特化したAI企業のビジネスが脅かされる可能性があるという警告を発しています。

2.  **特に興味深いコメント**

以下に、特に興味深いコメントをいくつか紹介します。

*   **242 upvotes:** "Classic “don’t build a business on someone else’s platform” here."

    このコメントは、他者のプラットフォームに依存したビジネスモデルのリスクを指摘しています。AIラボがAPIを閉鎖した場合、そのAPIに依存してビジネスを構築した企業は大きな打撃を受ける可能性があります。これは、過去のIT業界におけるプラットフォームの変遷を考えると、非常に的を射たコメントです。

*   **84 upvotes:** "It's absolutely what they want to do. Ban open source because "china" and "safety" then once they have ASI and enough compute internally sell $10k/month subscription models... or bid them out. that only works if the altenative is illegal in your country. At that point your country fails in a few years and I'm hoping they are smart enough to just not do this but hedging by bets incase they aren't."

    このコメントは、AIラボがオープンソースを排除し、ASI（人工超知能）と潤沢な計算リソースを手に入れた後、高額なサブスクリプションモデルを販売する可能性を指摘しています。さらに、その独占を維持するために、競合となるオープンソースを規制する可能性まで示唆しており、AIラボの商業戦略に対する非常にシニカルな見方を反映しています。

*   **77 upvotes:** "That's cool. You know what though? It's a little late. We already have some very good open-source models. Open source will always continue to move forward, and I still don't think they will have the moat they think they will."

    このコメントは、AIラボの閉鎖的な戦略に対する反論です。オープンソースモデルの急速な進歩を強調し、AIラボが考えているような独占的な地位を築くことは難しいだろうと予測しています。オープンソースコミュニティの力強さを信じる、楽観的な意見です。

これらのコメントは、AI業界の将来に対する多様な視点を提供しており、投稿内容に対する深い議論を促しています。特に、プラットフォーム依存のリスク、独占戦略の可能性、そしてオープンソースの競争力という、重要なテーマが浮き彫りになっています。

---

# Gemma 3 GRPO now in Unsloth + Bug Fixes

**Upvotes**: 146



[View on Reddit](https://www.reddit.com/r/LocalLLaMA/comments/1jf10ar/gemma_3_grpo_now_in_unsloth_bug_fixes/)

1.  **ポストの内容の説明**

このRedditの投稿は、UnslothというフレームワークがGemma 3という言語モデルのトレーニングと推論をサポートするようになったことを告知しています。特に以下の点が強調されています。

*   **Gemma 3のGRPO（Gradient Restricted Policy Optimization）トレーニング:** UnslothとHugging Faceが協力して、Gemma 3とGRPOを用いて推論モデルをトレーニングするための無料のColabノートブックを作成しました。
*   **バグ修正:** Gemma 3をファインチューニングする際の損失関数の問題を修正し、古いGPU（float16に制限されているもの）でGemma 3のトレーニングが動作しない問題を解決しました。
*   **FP16のサポート:** Unslothは、Gemma 3の推論とトレーニングをFP16マシンで実行できる唯一のフレームワークです。これにより、無料のColab T4 GPUインスタンスでGemma 3のGRPOやSFT（Supervised Fine-Tuning）などが可能になります。
*   **アクティベーションの問題:** Gemma 3はfloat16を使用するとアクティベーション値が非常に大きくなる問題がありましたが、Unslothはこの問題を解決しました。
*   **Colabノートブックの提供:** Gemma 3 (1B) を使用したGRPOのチュートリアルノートブックと、Gemma 3 (4B) を使用した通常のSFTのノートブックが提供されています。
*   **モデルサイズの選択:** Gemma 3 (1B)は推論が高速なためGRPOノートブックに選ばれましたが、モデル名を変更するだけでGemma 3 (4B)または(12B)も使用できます。

要するに、Unslothを使うことで、Gemma 3のトレーニングと推論がより簡単かつ効率的に行えるようになったという内容です。特に、無料のColab環境でGemma 3を扱えるようになった点が大きなメリットです。

2.  **興味深いコメント**

このポストに対するコメントの中で、特に興味深いのは以下のものです。

*   **"Hey Daniel, this is GREAT! Is saving to 4bit merged no longer an option or can we have that please?"**

    このコメントは、投稿者（Daniel）の取り組みに対する賞賛とともに、Unslothの機能に関する具体的な質問をしています。4bitでの保存機能のサポート状況について尋ねており、Unslothの今後の開発に対するユーザーの期待が伺えます。


---

# KBLaM by microsoft, This looks interesting

**Upvotes**: 156



[View on Reddit](https://www.reddit.com/r/LocalLLaMA/comments/1jez456/kblam_by_microsoft_this_looks_interesting/)

1.  **ポストの内容の説明:**

このRedditのポストは、Microsoftが発表した「KBLaM (Knowledge Base-Augmented Language Model)」という技術に関するものです。KBLaMは、大規模言語モデル（LLM）に外部知識を効率的に組み込むための新しいアプローチを提供します。

*   **KBLaMの目的:** LLMに外部知識を組み込み、より正確で信頼性の高い回答を生成できるようにすること。
*   **KBLaMの仕組み:**
    *   外部知識を「キー・バリュー」のベクトルペアとしてエンコードします。
    *   これらのベクトルペアを、モデルの注意層に直接埋め込みます（「矩形注意」メカニズムを使用）。
    *   このアプローチにより、知識ベースのサイズに対するスケーリングが線形になり、RAGのようにコンテキスト長を増やす必要がありません。
*   **KBLaMの利点:**
    *   **効率性:** 大規模な知識ベースを効率的に処理できます（単一のGPUで1万を超える知識トリプルを処理可能）。
    *   **動的な更新可能性:** 再トレーニングなしで知識ベースを更新できます。
    *   **解釈可能性:** モデルがどのように知識を使用しているかを、注意の重みを通じて理解できます。
    *   **信頼性:** 知識ベースにない質問に対して回答を拒否することで、ハルシネーション（もっともらしい嘘をつくこと）を減らすことができます。
*   **RAGとの比較:**
    *   従来のRAG（Retrieval-Augmented Generation）は、外部知識を取得してLLMの入力に追加するため、コンテキスト長が増加し、処理が非効率になることがあります。KBLaMは、知識をモデル自体に直接組み込むため、これらの問題を回避できます。
*   **投稿者の質問:** KBLaMがRAGを置き換えることができるのはどのような状況か？RAGの問題が解決されることは大きな進歩だと考えている。

2.  **特に興味深いコメント:**

*   **「知識を注意層に直接注入することで、RAGのような検索ステップやコンテキスト長の増加が不要になる」**というコメントは、KBLaMの核心的な利点を簡潔に説明しており、RAGの欠点を克服する可能性を示唆しているため、非常に興味深いです。
*   **「自己学習LLMへの道を開くかもしれない」**というコメントは、KBLaMの将来の可能性を示唆しており、LLMが自ら知識を獲得し、進化していく能力を持つようになるかもしれないという期待を抱かせます。特に、ユーザーがアシスタントに特定の分野の専門家になるように指示し、アシスタントが自動的に知識ベースを構築するという例は、非常に魅力的です。
*   **「線形スケーリングはゲームチェンジャーになる」**というコメントは、KBLaMの実用性と影響力を強調しています。RAGが抱えるチャンキングなどの問題を解決できる可能性も指摘されており、技術的な視点からも興味深いです。

これらのコメントは、KBLaMがLLMの分野に大きな変革をもたらす可能性を様々な角度から示唆しており、今後の動向が注目されます。


---

# Why don't we have non-Apple alternative to unified memory?

**Upvotes**: 50



[View on Reddit](https://www.reddit.com/r/LocalLLaMA/comments/1jf4u9e/why_dont_we_have_nonapple_alternative_to_unified/)

## ポストの内容の説明

このRedditのポストは、「なぜAppleのユニファイドメモリのようなものが、Apple以外の製品には存在しないのか？」という疑問を提起しています。投稿者は、GPUメーカーに過剰に搾取されているのではないかと危惧しており、ユニファイドメモリの代替技術がない現状に疑問を感じています。つまり、PC環境でApple製品のようなCPUとGPUがメモリを共有する仕組みが普及していないことを問題提起しています。

## 特に興味深いコメント

このポストに対するコメントで特に興味深いのは以下の2つです。

1.  **ユニファイドメモリの技術的な課題とPC環境への導入の難しさ:**
    *   高速なユニファイドメモリには、広いメモリバスが必要であり、これはソケット型のCPUやメモリモジュールと相性が悪いという指摘。サーバープラットフォームやハイエンドデスクトップPCのような大規模なCPUソケットが必要になり、コストが大幅に増加する可能性があることが述べられています。
    *   PC環境におけるソフトウェアサポートの欠如。Nvidia Jetsonのようなモジュールではユニファイドメモリが機能するものの、Nvidia独自のAPI (cudaMallocManagedなど) を使用する必要があり、汎用的なソフトウェアでは対応できない点が強調されています。
    *   Intel、AMD、Microsoft、Linuxといった主要企業が協力して取り組む必要性があるものの、その努力に見合うだけの価値があるか疑問視する意見もあります。ユニファイドメモリはメモリコピーの削減には役立つものの、それがボトルネックになるケースは稀であり、APU (CPUとGPUを統合したプロセッサ) でしか機能しないため、GPU単体と比較して性能やメモリ帯域幅が劣ることが理由として挙げられています。
2.  **ユニファイドメモリの存在と性能、製造コストに関する背景知識:**
    *   スマートフォンなどの小型プラットフォームではユニファイドメモリがすでに普及しているという指摘。チップの小型化にはユニファイドメモリが不可欠であり、Snapdragonチップを搭載したラップトップにも採用されている例が挙げられています。
    *   問題はユニファイドメモリの有無ではなく、その性能であるという主張。Appleがユニファイドアーキテクチャで先行しているものの、非常に高価な製造プロセスであり、それによってApple製品の価格が高くなっているという認識を持つべきだという意見です。半導体製造のコストは指数関数的に増加することを理解する必要性が強調されています。
    *   投稿者は半導体製造エンジニアとしての10年以上の経験から、この分野に関して一般の人々よりも深い知識を持っていることを示唆しています。

これらのコメントは、ユニファイドメモリの技術的な複雑さ、ソフトウェアサポートの課題、コスト、そして既存のPCアーキテクチャとの互換性といった、多角的な視点から議論を展開しており、非常に興味深いです。特に、半導体製造エンジニアの視点からのコメントは、単に性能だけでなく、製造コストや市場における価格設定といった要素も考慮する必要があることを示唆しており、議論に深みを与えています。


---

# Llama4 is probably coming next month, multi modal, long context

**Upvotes**: 349



[View on Reddit](https://www.reddit.com/r/LocalLLaMA/comments/1jes8ue/llama4_is_probably_coming_next_month_multi_modal/)

1.  **ポストの内容の説明:**

    このRedditの投稿は、Meta社のLLM（大規模言語モデル）であるLlamaの次期バージョン「Llama 4」に関する予測を述べています。投稿者は以下の点を主張しています。

    *   **リリース時期:** Llama 4はおそらく来月（投稿時点から見て）リリースされる。
    *   **機能:** マルチモーダル（テキストだけでなく画像なども扱える）、長文コンテキストに対応している。
    *   **根拠:** Meta社のイベント「Connect 2025 LlamaCon」への招待状（ブログ記事へのリンクが提示されている）から推測している。
    *   **コンテキスト長:** おそらく100万トークン程度のコンテキスト長を持つと予想している。

    つまり、投稿者はMeta社のイベント情報を元に、Llama 4が近い将来にリリースされ、大幅な機能向上が見込まれると予測している内容です。

2.  **興味深いコメント:**

    この投稿に対するコメントで特に興味深いのは以下の3点です。

    *   **「Hope, Deepseek doesn't release R2 before that.」:** このコメントは、Deepseekという別の企業が開発しているLLM「R2」のリリース時期に言及しています。Llama 4がリリースされる前にR2がリリースされることへの懸念を示しており、LLM市場における競争の激しさを伺わせます。つまり、LLMの開発競争において、Llama4のリリースが、競合であるDeepseekのR2のリリースより先であることを願っているということです。
    *   **「Source for the 1M context?」:** このコメントは、Llama 4のコンテキスト長が100万トークンであるという主張の根拠を尋ねています。投稿文では「Probably」と書かれており、確証がない推測であることが示唆されています。このコメントは、情報の信憑性を確認しようとする姿勢を示しています。
    *   **「I hope for some innovation in the architecture, otherwise it will become a model, that is a liiitle bit better tuned for benchmarks compared to Gemma, Mistral etc.」:** このコメントは、Llama 4のアーキテクチャにおける革新への期待を表明しています。単に既存モデルのベンチマークスコアを少し改善するだけでなく、根本的な進化を求めていることがわかります。これは、LLMの性能向上に対するユーザーの期待値が高いことを示唆しています。


---

# New open-source model for transpiling PyTorch to Triton outperforms DeepSeek-R1 and OpenAI o1 on kernelbench - made with reinforcement fine-tuning

**Upvotes**: 75



[View on Reddit](https://www.reddit.com/r/LocalLLaMA/comments/1jezj71/new_opensource_model_for_transpiling_pytorch_to/)

1.  **ポストの内容の説明**

このRedditの投稿は、PyTorchコードをTritonコードに変換するための新しいオープンソースモデル「Predibase-T2T-32B-RFT」を紹介しています。投稿者（Predibase）は、このモデルが、強化学ファインチューニング（Reinforcement Fine-Tuning, RFT）という手法を用いてトレーニングされたことを述べています。具体的には、Qwen2.5-Coder-32B-instructというモデルをGRPO（具体的なアルゴリズム名は不明）に基づいてRFTで調整したとのことです。

重要なのは、このモデルがkernelbenchというベンチマークにおいて、DeepSeek-R1やOpenAIのo1といった既存のモデルよりも約3倍優れた性能を発揮すると主張している点です。

投稿には、モデルのHugging Faceリポジトリへのリンクと、RFTの実装とモデルに関する詳細なブログ記事へのリンクが含まれています。また、性能比較を示すグラフも添付されています。

まとめると、この投稿は、Predibase社が開発した、PyTorchからTritonへのコード変換に特化した新しい高性能なオープンソースモデルの発表であり、強化学ファインチューニングという新しい学習手法の適用と、既存モデルを大幅に上回る性能を強調しています。

2.  **興味深いコメント**

この投稿に対するコメントで特に興味深いのは以下のものです。

*   **"thanks op for sharing, maybe I’m missing the context, but isn’t this what torch.compile() is for?"**

このコメントは、投稿されたモデルの必要性に対する疑問を提起しています。`torch.compile()` は PyTorch に組み込まれた機能で、PyTorch コードを最適化し、より高速に実行することができます。コメント主は、`torch.compile()` が同様の目的を達成できるのではないかと考えているようです。

この疑問は、このモデルがどのような特定のユースケースや状況において `torch.compile()` よりも優れているのかを考える上で重要です。例えば、`torch.compile()` が対応していない特定の種類のPyTorchコードや、Tritonのより細かい制御が必要な場合に、このモデルが役立つ可能性があります。Predibase側の回答があるとより議論が深まると思われます。

