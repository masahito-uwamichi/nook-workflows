
# Quamba2: A Robust and Scalable Post-training Quantization Framework for Selective State Space Models

[View Paper](http://arxiv.org/abs/2503.22879v2)

## 1. 既存研究では何ができなかったのか

既存の State Space Model (SSM) の量子化手法は、以下の点で課題がありました。

*   **多様なビット幅のサポート不足:** 特定のビット幅 (例えば W8A8) に最適化されていることが多く、W4A8 (大規模バッチ処理) や W4A16 (短いプロンプト) など、異なるシナリオに適したビット幅設定に対応できていませんでした。
*   **低ビット幅での性能劣化:** W4A8 などの低ビット幅で量子化した場合、性能が著しく低下する傾向がありました。
*   **汎化性能の軽視:** 多くの研究は特定のタスクでの性能向上に注力しており、多段階推論タスクなどにおける汎化性能の低下が見過ごされていました。特にW4A4といった極端な量子化を行うと、モデルの汎化性能が損なわれることが指摘されています。
*   **エッジデバイスへの最適化不足:** クラウド環境でのスループット最大化に焦点が当てられ、メモリ制約の厳しいエッジデバイスでの利用を考慮した最適化が不足していました。
*   **量子化誤差への脆弱性:** SSM の線形回帰構造が量子化誤差に非常に敏感であり、Transformer 向けの量子化手法をそのまま適用することが困難でした。

## 2. どのようなアプローチでそれを解決しようとしたか

Quamba2 では、上記課題を解決するために、以下の手法を提案しています。

*   **選択的量子化:** さまざまなプラットフォームでのSSM展開に対する要求の高まりに対応し、Mamba1とMamba2の両方のバックボーンで、W8A8、W4A8、W4A16をサポートしました。
*   **Sort-and-Cluster による入力量子化:** SSM のチャネル順序保持とアクティベーション持続性に基づき、入力 `x` をソートおよびクラスタリングして 8 ビットで量子化します。チャネルの最大値に基づいてチャネルをソートし、似た特性を持つヘッドをグループ化することで、量子化精度を高めています。
    ```python
    # 疑似コード: Sort-and-Cluster
    x = input_tensor
    channel_max = calculate_channel_max(x) # キャリブレーションデータセットからチャネルの最大値を計算
    sorted_indices = sort_channels(channel_max)
    x_sorted = x[:, sorted_indices]
    head_clusters = cluster_heads(x_sorted) # ヘッドレスターを実行
    for head_group in head_clusters:
        channel_clusters = cluster_channels(head_group) # 各ヘッドグループ内でチャンネルをクラスタリング
        for channel_group in channel_clusters:
            scaling_factor = calculate_scaling_factor(channel_group) #スケーリングファクタの計算
            quantize(channel_group, scaling_factor) # 量子化の実行
    ```
*   **Per-State-Group 量子化:** 入力依存パラメータ `B` と `C` を状態グループごとに量子化することで、量子化精度を向上させています。
    ```python
    # 疑似コード: Per-State-Group 量子化
    B = input_dependent_parameter_B
    C = input_dependent_parameter_C
    for state_group in state_groups:
        scaling_factor_B = calculate_scaling_factor(state_group_B)
        scaling_factor_C = calculate_scaling_factor(state_group_C)
        quantize(state_group_B, scaling_factor_B)
        quantize(state_group_C, scaling_factor_C)
    ```

*   **重み再配置:** SSM 出力の計算不変性を保証するために、オフラインで重みをクラスタリングシーケンスに従って再配置します。これにより、Sort-and-Cluster の結果を重みに反映し、量子化後の計算結果を元の計算結果に近づけています。
    ```python
    # 疑似コード: 重み再配置
    weights = model_weights
    for layer in model_layers:
        if layer.type == "input_projection":
            reorder_weights(weights[layer], clustering_indices) #input projectionのweightを再配置
        elif layer.type == "causal_convolution":
            reorder_weights(weights[layer], clustering_indices) #因果畳み込みのweightを再配置
        elif layer.type == "normalization":
            reorder_weights(weights[layer], clustering_indices) #正規化のweightを再配置
        elif layer.type == "output_projection":
            reorder_weights(weights[layer], clustering_indices) #出力射影のweightを再配置
    ```
*   **Hadamard 変換の融合:** 入力および出力の線形射影に Hadamard 行列を融合し、オンライン Hadamard 量子化と組み合わせることで、計算不変性を実現しています。
*   **混合精度サポート:** 性能感度に基づいて重要なブロックを自動的に検索し、より高い精度を割り当てることで、汎化性能と性能のバランスを取っています。

## 3. 結果、何が達成できたのか

Quamba2 の実験結果は以下の通りです。

*   **高性能:** Quamba2-8B は、既存の SSM 量子化手法を上回り、pre-filling 段階で 1.3 倍、generation 段階で 3 倍の高速化を達成しました。
*   **省メモリ:** 4 倍のメモリ削減を実現し、平均 1.6% の精度低下に抑えています。
*   **汎用性と堅牢性:** MMLU データセットでの評価により、フレームワークの汎用性と堅牢性が示されました。
*   **エッジデバイスへの展開:** Head-to-Toe (H2T) 量子化により、Mamba2-8B をエッジプラットフォーム (Nvidia Nano 8G) に展開可能にしました (13 tokens/秒)。
*   **スループット向上:** Nvidia A5000 でのスループットが向上しました。
*   **多様な構成のサポート:** W8A8, W4A8, W4A16 の構成をサポートし、さまざまなニーズに対応できます。

## 4. Limitationや問題点は何か

*   **量子化誤差の影響:** SSM は線形回帰構造を持つため、量子化誤差に依然として敏感であり、特に低ビット幅の場合、性能低下のリスクがあります。
*   **混合精度構成の探索:** 混合精度構成の自動探索は、計算コストがかかる可能性があります。また、最適な構成はタスクやモデルによって異なるため、汎用的な探索手法の開発が課題となります。
*   **キャリブレーションデータの依存性:** Sort-and-Cluster に使用するキャリブレーションデータセットの選択が、量子化性能に影響を与える可能性があります。
*   **エッジデバイスでの性能:** Nvidia Nano 8G で 13 tokens/秒 はまだ実用的な速度とは言えない可能性があり、さらなる最適化が必要です。
*   **TTFTの遅延:** W4A8およびW4A16構成では、W8A8およびFP16と比較して、量子化解除のオーバーヘッドによりTTFTが遅くなる可能性があります。

## 5. 技術的な詳細について

Quamba2 の技術的な詳細を以下に示します。

1.  **Sort-and-Cluster 量子化:**
    *   キャリブレーションデータセットからチャネルの最大値を計算し、それに基づいてチャネルをソートします。
    *   ソートされたチャネルを持つヘッドをクラスタリングし、各ヘッドグループ内でさらにチャネルをクラスタリングします。
    *   各グループに対してスケーリングファクターを計算し、量子化を行います。
2.  **Per-State-Group 量子化:**
    *   入力依存パラメータ B と C を状態グループごとに分割します。
    *   各グループに対してスケーリングファクターを計算し、量子化を行います。
3.  **重み再配置:**
    *   Sort-and-Cluster のクラスタリング結果に基づいて、モデルの重みを再配置します。
    *   入力射影、因果畳み込み、正規化、出力射影などの各層で重みを再配置します。
4.  **Hadamard 変換の融合:**
    *   入力および出力の線形射影に Hadamard 行列を融合します。
    *   オンライン Hadamard 量子化と組み合わせることで、計算不変性を実現します。
5.  **混合精度サポート:**
    *   性能感度に基づいて重要なブロックを自動的に検索し、より高い精度を割り当てます。
    *   進化的探索アルゴリズムを用いて、最適な混合精度構成を探索します。
6.  **CUDA カーネルの最適化:**
    *   4 ビットおよび 8 ビットの行列乗算カーネルを実装します。
    *   高速 Hadamard 変換および選択的スキャンカーネルを量子化に対応させます。

## 6. コストや物理的な詳細について

*   **GPU:** Nvidia A5000 (24GB) および Nvidia Orin Nano 8G を使用して評価を実施しています。
*   **データセット:** The Pile データセットからランダムにサンプリングされた 512 文を使用してキャリブレーションセットを構築しています。
*   **モデルサイズ:** 量子化されたパラメータと計算用のバッファを含むモデルサイズを報告しています。具体的なモデルサイズは、論文中の表を参照してください。
*   **混合精度構成の探索:** ポピュレーションサイズ 40、世代数 5 の進化的探索アルゴリズムを使用しています。
*   **その他:** トレーニングや量子化にかかる時間、具体的な消費電力などの詳細なコスト情報は記載されていません。

## 7. 参考文献のうち、特に参照すべきもの

*   **gu2023mamba, dao2024transformers:** Mamba アーキテクチャと Selective State Space Models (SSM) についての基礎的な論文です。Quamba2 はこれらのモデルを対象としています。
*   **chiang2024quamba:** 既存の SSM 量子化手法である Quamba について記述されており、Quamba2 との比較において重要です。
*   **xu2025mambaquant:** 既存の SSM 量子化手法である MambaQuant について記述されており、Quamba2 との比較において重要です。
*   **hendrycks2020measuring:** MMLU データセットについての論文です。Quamba2 の汎化性能評価に使用されています。
*   **frantar2023gptq:** GPTQ (Generative Post-training Quantization) についての論文です。Quamba2 では、GPTQ を組み合わせて量子化精度を向上させています。

## 8. この論文を140字以内のツイートで要約すると？

Quamba2: SSM(Mamba)向け高精度量子化！チャネルソート＆状態グループ量子化でW8A8/W4A8/W4A16対応。既存手法超え高速＆省メモリ！MMLUで汎化性も実証。エッジ展開も可能に #量子化 #SSM #Mamba


---


# Improved Visual-Spatial Reasoning via R1-Zero-Like Training

[View Paper](http://arxiv.org/abs/2504.00883v1)

## 1. 既存研究では何ができなかったのか

既存のマルチモーダル大規模言語モデル（MLLMs）は、特にビデオベースの視覚空間推論（VSI）において、十分な推論能力を発揮できていませんでした。具体的には以下の点が課題でした。

*   **CoTプロンプトの非活性化:** 小規模から中規模のQwen2-VLモデルでは、Chain of Thought (CoT) プロンプトを用いても視覚空間推論能力が十分に引き出せませんでした。つまり、推論に必要な計算リソース（FLOPs）を増やしても、性能向上が見られませんでした。
*   **VSI能力の不足:** 既存のモデルは、VSI-benchなどのベンチマークにおいて性能が不十分でした。
*   **タスク一般化の課題:** 既存のモデルは、訓練データに含まれていないタスク（経路計画、出現順序）への一般化能力が不足していました。

## 2. どのようなアプローチでそれを解決しようとしたか

本研究では、R1-Zeroライクな学習フレームワークを用いて、MLLMsの視覚空間推論能力を向上させることを目指しました。具体的には、以下の手順でアプローチしました。

1.  **データセットの構築:** 視覚空間推論の学習データが不足しているため、10万件以上のビデオベースの質問応答データセット（VSI-100k）を構築しました。このデータセットは、高精度なビデオスキャンと詳細なオブジェクトレベルの3Dアノテーションに基づいています。質問応答ペアは、オブジェクトの数、相対的な方向、相対的な距離、オブジェクトのサイズ、部屋のサイズ、絶対的な距離に関する6つのトピックをカバーするように設計されています。
2.  **GRPO (Group Relative Policy Optimization) の導入:** DeepSeek-R1-Zeroで成功したGRPOを、視覚空間推論の改善のために採用しました。GRPOは、criticモデルを必要としない強化学習の一種であり、訓練コストを削減できます。
3.  **報酬関数の設計:** モデルの予測と正解とのアライメントに基づいたルールベースの報酬関数を定義しました。CoT推論を促すために、フォーマット報酬も導入しました。
4.  **KL penaltyの維持:** GRPOの訓練において、KL penaltyを維持する必要性を見出しました。KL penaltyは、オンラインポリシーとフローズンな参照ポリシーとの乖離を抑制する役割を果たします。
5.  **プロンプト戦略の検討:** さまざまなプロンプト戦略（think, observe, vanilla）を用いてGRPO訓練を行い、最適なプロンプト戦略を検証しました。

## 3. 結果、何が達成できたのか

本研究の結果、以下の点が達成されました。

*   **性能向上:** Qwen2-VL-2BをベースとしたvsGRPO-2Bモデルは、ベースモデルよりも12.1%高い性能を達成し、GPT-4oを上回りました。
*   **大規模モデルに匹敵する性能:** Qwen2-VL-7BをベースとしたvsGRPO-7Bモデルは、最高性能のオープンソースモデルであるLLaVA-NeXT-Video-72Bに匹敵する性能を達成しました。
*   **GRPOの優位性:** vsGRPOは、教師ありファインチューニング（SFT）や直接選好最適化（DPO）と比較して、視覚空間推論能力の向上において優位性を示しました。
*   **タスク一般化能力の向上:** 訓練データに含まれていない経路計画タスクにおいても性能が向上しました。

## 4. Limitationや問題点は何か

本研究には、以下のような限界や問題点が存在します。

*   **報酬関数の課題:** モデルが意図しない方法で高い報酬を得る（reward hacking）現象が観察されました。例えば、observe-modeにおいて、無意味なタグを追加して長さを稼ぐといった行動が見られました。
*   **KL penaltyの必要性:** KL penaltyを取り除くと訓練が破綻する可能性があることが示唆されました。これは、VSI推論問題の特殊性に起因する可能性があります。
*   **精度報酬の停滞:** GRPO訓練の初期段階でフォーマット報酬が急速に収束する一方で、精度報酬の向上が緩やかであり、上限が存在するように見えました。
*   **DPOの性能:** DPOの改善がわずかであった原因として、選好ペアの構築が最適でなかった可能性が考えられます。
*   **データセットの偏り:** 構築したデータセットが特定の視覚空間推論タスクに偏っている可能性があり、汎化性能に影響を与える可能性があります。
*   **モデルサイズ:** 7Bモデルでの実験は限定的であり、より大規模なモデルでの性能検証が必要です。

## 5. 技術的な詳細について

本研究では、Qwen2-VLをベースモデルとして、GRPO（Group Relative Policy Optimization）を用いて視覚空間推論能力を向上させました。以下に、技術的な詳細をまとめます。

**1. データセット構築 (VSI-100k):**

*   高忠実度のビデオとオブジェクトレベルの3Dアノテーションを使用。
*   以下の6つの視覚空間推論タスクに関する質問応答ペアを作成。
    *   オブジェクト数 (object count)
    *   相対方向 (relative direction)
    *   相対距離 (relative distance)
    *   オブジェクトサイズ (object size)
    *   部屋のサイズ (room size)
    *   絶対距離 (absolute distance)
*   経路計画と出現順序のタスクは、汎化性能テストのためにhold out。

**2. GRPOの実装:**

*   GRPOの目的関数:

```python
def grpo_loss(theta, theta_old, O, q, r, beta, epsilon, pi_ref):
  """
  GRPOの損失関数を計算する疑似コード

  Args:
    theta: 現在のポリシーのパラメータ
    theta_old: 古いポリシーのパラメータ
    O: 生成された出力の集合 (リスト)
    q: 質問
    r: 各出力に対応する報酬 (リスト)
    beta: KL penaltyの係数
    epsilon: クリッピングパラメータ
    pi_ref: 参照ポリシー (ベースモデル)

  Returns:
    GRPO損失
  """
  G = len(O) # 出力数
  advantages = [(ri - sum(r) / G) / (std(r) + 1e-9) for ri in r]  # Advantage関数の計算

  loss = 0.0
  for i in range(G):
      ratio = pi_theta(O[i], q) / pi_theta_old(O[i], q) # 重要度比率の計算
      clipped_ratio = min(ratio, clip(ratio, 1 - epsilon, 1 + epsilon)) # 比率のクリッピング

      loss += min(ratio * advantages[i], clipped_ratio * advantages[i]) / G

  kl_penalty = beta * kl_divergence(pi_theta, pi_ref, O, q) # KLダイバージェンスペナルティの計算
  return -loss + kl_penalty # GRPOの損失
```

*   報酬関数：フォーマット報酬と精度報酬を組み合わせる。
    *   フォーマット報酬: 指定されたフォーマットに従っているかを定量化 (CoTプロンプトの場合のみ)。
    *   精度報酬:
        *   Multiple-Choice Answer (MCA)タスク: 正解との文字一致で評価 (1 or 0)。
        *   Numerical Answer (NA)タスク: 予測値と真の値の絶対差を計算し、小さい方の値で割る。

**3. プロンプト戦略:**

*   3種類のプロンプトで訓練:
    *   `vsGRPO-T`: "Please think step by step ... <answer> </answer>."
    *   `vsGRPO-O`: "Please observe carefully ... <answer> </answer>."
    *   `vsGRPO-V`: "Please provide the short answer ...".

**4. 比較対象:**

*   Supervised Fine-Tuning (SFT): VSI-100kデータセットで直接ファインチューニング。
*   Direct Preference Optimization (DPO): 正解を不正解に変更したものをより好ましくない回答として使用。

## 6. コストや物理的な詳細について

*   **モデル:** Qwen2-VL-2B および Qwen2-VL-7B
*   **データセット:** VSI-100k (10万件以上のビデオベース質問応答サンプル)
*   **GPU時間:** 120 GPU時間 (vsGRPO-2Bのファインチューニング)
*   **ロールアウト数:** 1質問あたり14ロールアウト
*   **サンプリング温度:** デフォルトで1.0

## 7. 参考文献のうち、特に参照すべきもの

*   **DeepSeek-R1:** `Guo, Daya, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.` GRPOのベースとなった研究。
*   **Qwen2-VL:** `Wang, Peng, et al. Qwen2-vl: Enhancing vision-language model’s perception of the world at any resolution.` 使用されたベースモデルに関する情報。
*   **R1-VL:** `Zhang, Jingyi, et al. R1-vl: Learning to reason with multimodal large language models via step-wise group relative policy optimization.` GRPOの適用事例。
*   **LLaVA-NeXT-Video:** `Li, Feng, et al. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models.` 比較対象となった高性能モデル。

## 8. この論文を140字以内のツイートで要約すると？

Qwen2-VLをR1-ZeroライクなGRPOでfine-tuneし、視覚空間推論を大幅改善！120GPU時間でGPT-4o超え、LLaVA-NeXT-Video-72Bに匹敵する性能達成。CoTは小規模モデルには不向き。データセットとコードは近日公開！ #MLLM #VSI #GRPO #Qwen2


---

はい、承知いたしました。以下に質問に対する回答を示します。


# Boost Your Human Image Generation Model via Direct Preference Optimization

[View Paper](http://arxiv.org/abs/2405.20216v2)

## 1. 既存研究では何ができなかったのか

既存の人間画像生成研究では、以下のような課題がありました。

*   **リアリズムの欠如:** 人間の画像生成は、その応用範囲の広さから重要な焦点ですが、解剖学、ポーズ、細部のわずかな不正確さでもリアリズムが損なわれる可能性がありました。
*   **従来のDPOの限界:** Direct Preference Optimization (DPO) は、好ましい (winning) 画像を生成するようにモデルを訓練し、好ましくない (losing) 画像から逸脱させますが、従来のDPO法では生成された画像をwinning画像として使用しており、リアリズムが制限されていました。
*   **GANsの課題:** GANsは、生成された画像が実画像にどれだけ近いかを評価し、逸脱を罰することで、出力を実画像に近づけるように誘導しますが、DPOフレームワークによる実現方法が課題でした。
*   **ドメインギャップの問題:** 実画像と生成画像の間の大きなドメインギャップにより、DPOを単純に適用するだけでは不十分でした。

## 2. どのようなアプローチでそれを解決しようとしたか

これらの課題を解決するために、本研究では以下の要素を取り入れた HG-DPO (Human image Generation through DPO) というアプローチを提案しました。

*   **実画像の導入:** 生成された画像ではなく、高品質の実画像をwinning画像としてDPOに組み込むことで、出力が生成された画像ではなく実画像に似るように促しました。
*   **カリキュラム学習:** 実画像と生成画像の間のドメインギャップを埋めるために、カリキュラム学習フレームワークを導入しました。モデルを簡単なタスクから難しいタスクへと段階的に訓練することで、より現実的な出力を生成できるようにしました。具体的には、easy, normal, hard の3段階で構成されています。
    *   **Easy Stage:** まず、歪みのない解剖学的構造やポーズ、より良い画像とテキストのアライメントに焦点を当てて、基本的な人間の好みを学習させます。ここでは、生成画像をwinning画像として利用します。また、色ずれを防ぐために、統計量マッチング損失を導入しました。
    *   **Normal Stage:** 次に、より現実的な構図とポーズを捉えることによって、視覚的な品質を高めます。ここでは、SDReconを用いて生成された中間的な画像をwinning画像として利用します。
    *   **Hard Stage:** 最後に、実画像に一致するように細部を調整し、高忠実度の出力を可能にします。ここでは、実画像に近い中間ドメインの画像をwinning画像として利用します。
*   **AIフィードバックの活用:** コストのかかる人間のフィードバックに頼る代わりに、AIベースの手法を用いてwinning画像とlosing画像を選択しました。

```python
# 疑似コード: HG-DPO のカリキュラム学習の概要
def hg_dpo_training(base_model, real_image_dataset, text_prompt_dataset):
    """
    HG-DPOのカリキュラム学習によるモデル訓練

    Args:
        base_model: 事前学習済みの拡散モデル
        real_image_dataset: 実写の人間画像のデータセット
        text_prompt_dataset: テキストプロンプトのデータセット

    Returns:
        trained_model: 訓練されたモデル
    """

    # Easy Stage
    easy_dataset = create_easy_dataset(text_prompt_dataset, base_model)
    easy_trained_model = train_dpo(base_model, easy_dataset, stage="easy")

    # Normal Stage
    normal_dataset = create_normal_dataset(real_image_dataset, text_prompt_dataset, easy_trained_model)
    normal_trained_model = train_dpo(easy_trained_model, normal_dataset, stage="normal")

    # Hard Stage
    hard_dataset = create_hard_dataset(real_image_dataset, text_prompt_dataset, normal_trained_model)
    hard_trained_model = train_dpo(normal_trained_model, hard_dataset, stage="hard")

    return hard_trained_model

def create_easy_dataset(text_prompt_dataset, base_model):
    """
    Easy Stage用のデータセットを作成
    """
    winning_images = generate_images(base_model, text_prompt_dataset) # 生成画像
    losing_images = generate_images(base_model, text_prompt_dataset) # 生成画像
    dataset = pair_images(text_prompt_dataset, winning_images, losing_images)
    return dataset

def create_normal_dataset(real_image_dataset, text_prompt_dataset, easy_trained_model):
    """
    Normal Stage用のデータセットを作成
    """
    intermediate_images = generate_intermediate_images(real_image_dataset, text_prompt_dataset, easy_trained_model) # 中間ドメイン画像 (SDRecon)
    winning_images = select_winning_images(intermediate_images) # スコアの高い中間ドメイン画像
    losing_images = generate_images(easy_trained_model, text_prompt_dataset) # easy stageの生成画像
    dataset = pair_images(text_prompt_dataset, winning_images, losing_images)
    return dataset

def create_hard_dataset(real_image_dataset, text_prompt_dataset, normal_trained_model):
    """
    Hard Stage用のデータセットを作成
    """
    intermediate_images = generate_intermediate_images(real_image_dataset, text_prompt_dataset, normal_trained_model) # 実画像に近い中間ドメイン画像 (t1)
    winning_images = select_winning_images(intermediate_images) # スコアの高い中間ドメイン画像
    losing_images = generate_images(normal_trained_model, text_prompt_dataset) # normal stageの生成画像
    dataset = pair_images(text_prompt_dataset, winning_images, losing_images)
    return dataset

def train_dpo(model, dataset, stage):
    """
    DPOでモデルを訓練
    """
    for prompt, winning_image, losing_image in dataset:
        # DPO損失を計算
        loss = dpo_loss(model, prompt, winning_image, losing_image)

        # 統計量マッチング損失 (easy stage のみ)
        if stage == "easy":
            loss += statistics_matching_loss(model, winning_image, base_model)

        # モデルを更新
        model.update(loss)
    return model
```

## 3. 結果、何が達成できたのか

HG-DPOを適用した結果、以下の成果が得られました。

*   **高品質な人間画像の生成:** HG-DPOは、広範なアクション、外観、グループサイズ、背景を含む高品質の人間画像を生成できました。
*   **リアルな解剖学的特徴とポーズ:** HG-DPOは、よりリアルな解剖学的特徴とポーズを持つ画像を生成するようにベースモデルを改善しました。
*   **プロンプトとの整合性:** 生成された画像が、プロンプト（プロンプト内の赤字のテキスト）とより良く整合するようになりました。
*   **パーソナライズされたテキストから画像へのタスクへの適応:** HG-DPOは、追加のトレーニングなしで、パーソナライズされたテキストから画像へのタスクに効果的に適応し、高品質で特定のアイデンティティを持つ画像を生成しました。

## 4. Limitationや問題点は何か。本文で言及されているものの他、あなたが考えるものも含めて

論文で言及されている制限事項と問題点:

*   **指の生成:** HG-DPOは、写実的な人間の画像を生成する能力を大幅に向上させましたが、写実的な指の生成は依然として困難です。

私が考える制限事項と問題点:

*   **多様性の低下:** 実画像をwinning画像として使用することで、生成される画像の多様性が低下する可能性があります。
*   **計算コスト:** 3段階のカリキュラム学習には、計算コストがかかります。特に、中間ドメイン画像の生成には追加の計算が必要です。
*   **プロンプトとの完全な整合性:** Hard Stageではプロンプトを考慮した選定を行っていないため、テキストとの関連性がeasy, normal stageと比較して低い可能性があります。
*   **データセットへの依存:** 300k枚の高品質な人間画像というデータセットを構築する必要があり、データセット構築のコストがかかります。また、データセットの偏りが生成される画像に影響を与える可能性があります。
*   **汎用性:** 人間の画像生成に特化しているため、他の種類の画像生成への適用は容易ではありません。

## 5. 技術的な詳細について。技術者が読むことを想定したトーンで

HG-DPOは、Direct Preference Optimization (DPO) をベースにしており、拡散モデルのファインチューニング戦略として機能します。このアプローチの中核は、次の3つの段階で構成されるカリキュラム学習の導入です。

1.  **Easy Stage:**
    *   目標: ベースモデルの出力を、基本的な構造的正確さおよびプロンプトへの適合性に関して、人間の好みに合わせます。
    *   実装: 生成された画像をwinning画像として使用します。
    *   損失関数: DPO損失に加えて、Latent空間における色ずれを防ぐために、統計量マッチング損失関数を導入します。
    *   統計量マッチング損失：

        ```python
        def statistics_matching_loss(model_output, base_model_output):
          """統計量マッチング損失を計算する関数

          Args:
              model_output: 訓練中のモデルの出力
              base_model_output: ベースモデルの出力

          Returns:
              統計量マッチング損失
          """
          # 各出力の平均を計算
          model_mean = calculate_mean(model_output)
          base_mean = calculate_mean(base_model_output)

          # 平均のL2ノルムを計算
          loss = l2_norm(model_mean - base_mean)
          return loss

        def calculate_mean(latent):
          """チャネルごとの平均を計算
          """
          mean = np.mean(latent, axis=(0, 1)) # バッチと空間次元で平均化
          return mean

        def l2_norm(vector):
          """L2ノルムを計算
          """
          return np.sqrt(np.sum(vector**2))
        ```

2.  **Normal Stage:**
    *   目標: 画像のリアリズムを高めます。特に、自然な構図とポーズの生成に焦点を当てます。
    *   実装: Stochastic Differential Reconstruction (SDRecon) を使用して生成された中間ドメインの画像をwinning画像として使用します。
    *   SDRecon: 実画像にノイズを付加し、それをベースモデルで再構築するプロセスです。これにより、実画像の特徴（ポーズ、構図）と生成画像の特徴（テクスチャ、ディテール）が組み合わさった画像が生成されます。

        ```python
        def generate_intermediate_images(real_image, text_prompt, base_model, timestep):
            """SDReconを用いて中間ドメイン画像を生成
            """
            # 実画像にノイズを付加
            noisy_image = add_noise(real_image, timestep)

            # ノイズ除去
            intermediate_image = base_model.denoise(noisy_image, text_prompt, timestep)

            return intermediate_image

        ```

3.  **Hard Stage:**
    *   目標: 細部のリアリズムをさらに向上させます。
    *   実装: 実画像に非常に近い中間ドメインの画像（SDReconの初期ステップで生成された画像）をwinning画像として使用します。

DPO損失は、以下の式に基づいて計算されます。

```python
def dpo_loss(model, prompt, winning_image, losing_image, beta=0.1):
    """DPO損失を計算

    Args:
        model: 訓練対象のモデル
        prompt: テキストプロンプト
        winning_image: より好ましい画像
        losing_image: より好ましくない画像
        beta: 温度パラメータ

    Returns:
        DPO損失
    """
    # winning画像とlosing画像のモデルスコアを計算
    winning_score = model.score(prompt, winning_image)
    losing_score = model.score(prompt, losing_image)

    # DPO損失を計算
    loss = -torch.log(torch.sigmoid(beta * (winning_score - losing_score)))
    return loss

```

また、潜在空間におけるカラーシフトを軽減するため、統計量マッチング損失関数が導入されています。

## 6. コストや物理的な詳細について。例えばトレーニングに使用したGPUの数や時間、データセット、モデルのサイズなど

*   **データセット:** 約300k枚の高品質な人間画像を含む内部データセットを使用。5k枚の画像をテストに使用し、残りをトレーニングに使用しました。
*   **キャプション:** LLaVAを使用して、トレーニング画像とテスト画像のキャプションを生成しました。
*   **ベースモデル:** majicmix-v7 (SD1.5のファインチューンモデル) をベースモデルとして使用しました。
*   **LoRA:** LoRA を使用し、LoRAランクは8に設定しました。テキストエンコーダーのLoRAランクは64に設定しました。
*   **GPU:** 4つのNVIDIA A100 GPUを使用しました。
*   **バッチサイズ:** ローカルバッチサイズを4に設定しました。したがって、合計バッチサイズは16でした。
*   **オプティマイザー:** 8ビットAdamオプティマイザーを使用しました。
*   **学習率:** 1e-5。
*   **混合精度:** 効率的なトレーニングのために混合精度を利用しました。
*   **トレーニングステップ数:** easy, normal, hard の各ステージで、それぞれ300k, 20k, 20kステップモデルを更新しました。

## 7. 参考文献のうち、特に参照すべきもの

特に参照すべき参考文献は以下の通りです。

*   **Direct Preference Optimization: Your Language Model is Secretly a Reward Model (Rafailov et al.):** DPOの基礎となる論文であり、HG-DPOの根幹を理解するために不可欠です。
*   **High-Resolution Image Synthesis with Latent Diffusion Models (Rombach et al.):** 拡散モデルの基本原理と、潜在空間での画像合成について理解を深めるために役立ちます。
*   **Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation (Kirstain et al.):** 人間の好みを学習するためのデータセット構築に関する知見が得られます。

## 8. この論文を140字以内のツイートで要約すると？

実写を教師データにDPOで人間画像を高品質化！✨カリキュラム学習で段階的に学習させ、GANの知見も導入。既存DPOの課題を克服し、個人化T2Iにも対応！ #画像生成 #DPO #AI


---


# ScholarCopilot: Training Large Language Models for Academic Writing with Accurate Citations

[View Paper](http://arxiv.org/abs/2504.00824v2)

## 1. 既存研究では何ができなかったのか

既存のRetrieval-Augmented Generation (RAG)システムは、一般的なテキスト生成における事実の正確性を向上させたものの、専門的な学術論文作成を支援する能力には限界がありました。具体的には以下の点が問題でした。

*   **検索と生成の分離:** 従来のRAGは、検索と生成を別々の段階で行うため、生成の文脈の変化に合わせた柔軟な検索ができませんでした。例えば、論文のイントロでGPT-4について言及する場合、生成の具体的な文脈を考慮せずに事前に検索が行われるため、GPT-4関連の参考文献を必要なタイミングで正確に検索することが困難でした。
*   **検索意図のずれ:** 検索モデルと生成モデルが別々に最適化されるため、クエリの意図にずれが生じることがありました。
*   **柔軟性と文脈認識の欠如:** 事前に決定された検索決定は柔軟性に欠け、文脈を考慮した対応ができませんでした。
*   **ユーザーコントロールの制限:** 静的なパイプラインであるため、コンテンツの生成や参考文献のニーズに対するユーザーの制御が制限されていました。
*   **学術論文固有の要求への不適合:** 既存のRAGシステムは、主にQA形式のタスクで評価されており、学術論文作成に必要な反復的で引用中心の要件を考慮していませんでした。
*   **Long-formにおける課題:** 長文生成において、情報ニーズが変化していくような状況に対応することが難しかった。

## 2. どのようなアプローチでそれを解決しようとしたか

ScholarCopilotは、上記の課題を解決するために、以下の統合的なアプローチを採用しました。

*   **動的な検索トリガー:** 論文生成中に特別なトークン`[RET]`を生成することで、検索が必要なタイミングを動的に決定します。このトークンは、生成の文脈に基づいて生成されます。
*   **統合的なフレームワーク:** テキスト生成と引用検索を、単一の反復的なフレームワークに統合しました。これにより、検索と生成が互いに影響し合い、より文脈に合った引用が可能になります。
*   **対照学習:** 検索トークンの分散表現を対照学習によって最適化し、効率的な類似度検索を可能にしました。
*   **ユーザーインタラクション:** ユーザーが任意で検索をトリガーしたり、引用を改善したりできるようにし、人間の専門知識を生成プロセスに統合できるようにしました。
*   **統一モデル:** 生成モデルと検索モデルがパラメータを共有することで、効率を向上させました。
*   **反復型RAG:** 生成と検索を交互に行う反復型RAGを採用し、進化する引用ニーズに対応しました。
*   **暗黙的な意図の処理:** 明示的なクエリなしに、文脈から引用を推測しました。

疑似コードで示すと、以下のような処理フローになります。

```python
def scholar_copilot_generate(context, citation_database):
    generated_text = ""
    while not is_finished(generated_text):
        # 次のトークンを予測
        next_token = predict_next_token(generated_text, context)
        generated_text += next_token

        # 検索トークン [RET] が生成された場合
        if next_token == "[RET]":
            # 検索トークンの埋め込みベクトルを取得
            query_embedding = get_embedding(generated_text)

            # 引用データベースから関連論文を検索
            retrieved_citations = search_citations(query_embedding, citation_database)

            # 検索結果を生成テキストに追加
            generated_text += integrate_citations(retrieved_citations)

        # ユーザーによる介入 (任意)
        user_input = get_user_input()
        if user_input:
            generated_text = incorporate_user_feedback(generated_text, user_input)

    return generated_text
```

## 3. 結果、何が達成できたのか

ScholarCopilotは、以下の点で優れた結果を達成しました。

*   **高い検索精度:** 評価データセットにおいて、トップ1の検索精度で40.1%を達成しました。これは、E5-Mistral-7B-Instruct (15.0%)やBM25 (9.8%)などのベースラインを大幅に上回る結果です。
*   **優れた生成品質:** 1,000件の学術論文サンプルデータセットにおいて、関連性、一貫性、学術的厳密性、完全性、革新性の5つの側面で評価した生成品質において、25点満点中16.2点を獲得しました。これは、Retrieval-Augmented Qwen2.5-72B-Instructのような、より大規模なモデルを含む既存のすべてのモデルを大幅に上回る結果です。
*   **人間による評価の高さ:** 人間の評価研究において、7Bモデルであるにもかかわらず、ChatGPTを大幅に上回り、引用品質において100%の支持、全体的な有用性において70%以上の支持を得ました。
*   **ユーザビリティの向上:** ユーザー調査では、引用の関連性と管理の容易さ、インタラクティブな記述スタイルによるユーザー制御、特に参考文献セクションの記述における効率の高さなどが評価されました。

## 4. Limitationや問題点は何か

論文で言及されている制限事項:

*   **サポート範囲の限定:** 現在、コンピュータサイエンス分野の序論と関連研究セクションのみをサポートしています。
*   **革新的な洞察の生成の弱さ:** ユーザー調査では、革新的なアイデアを生み出す能力に限界があることが指摘されています。
*   **ユーザインタラクションの改善余地:** コンテンツの永続的な保存、提案された参考文献の簡潔な要約、一貫したマルチユーザー応答性のための堅牢な負荷分散など、ユーザインタラクションの改善が求められています。
*   **応答時間:** システムの応答時間がピーク時に長くなることがありました。

私が考える制限事項:

*   **特定ドメインへの偏り:** arXivのコンピュータサイエンス論文で学習しているため、他の分野への汎化性能は不明です。
*   **データセットの偏り:** arXivの論文は特定の研究コミュニティの動向を反映している可能性があり、データセットの偏りが生成される論文に影響を与える可能性があります。
*   **評価指標の限界:** GPT-4oによる評価は、GPT-4o自体のバイアスや知識に依存する可能性があります。
*   **[RET]トークンの設計:** `[RET]`トークンのトリガー条件や、トークンの埋め込み表現の学習方法について、更なる検討の余地があると考えられます。例えば、検索頻度やタイミングが適切かどうか、より高度なトリガーメカニズムが必要かどうかなどを検討する必要があります。
*   **倫理的な考慮:** 学術論文の自動生成は、盗作や不正行為のリスクを高める可能性があります。この点について、十分な対策を講じる必要があります。

## 5. 技術的な詳細について

ScholarCopilotは、Qwen-2.5-7Bをベースに構築され、arXivの50万件の論文で学習されています。主な技術的要素は以下の通りです。

1.  **Retrieval Token生成:**
    *   テキスト生成中に、モデルが`[RET]`という特殊なトークンを生成することで、引用検索の必要性を動的に判断します。
    *   このトークンは、生成コンテキストに応じて生成され、続く検索のクエリとして機能します。

2.  **分散表現の学習 (Contrastive Learning):**
    *   検索トークン (query) の分散表現と、関連する (positive) 引用論文の分散表現間の類似度が高くなるように、Contrastive Learning を用いて学習を行います。
    *   同時に、無関係な (negative) 引用論文との類似度が低くなるように学習します。

3.  **損失関数:**
    *   テキスト生成のためのNext Token Predictionの損失関数 (L_g) と、引用検索のためのContrastive Learningの損失関数 (L_r) を組み合わせた損失関数 (L_total) を最小化します。
    *   損失関数は以下の式で表されます。

    ```
    L_total = L_g + λ * L_r
    ```

    *   λは、生成と検索の目的のバランスを調整するハイパーパラメータです。論文ではλ=1に設定されています。

4.  **負例サンプリング:**
    *   Contrastive Learningでは、正例だけでなく、負例も重要です。
    *   論文では、バッチ内の他の論文からの引用を簡単な負例とし、同じ論文内の無関係な引用を難しい負例として使用しています。

5.  **学習データセット:**
    *   arXivから収集した50万件のコンピュータサイエンス論文を使用します。
    *   各論文には平均38件の引用が含まれており、そのうち33件 (87%) がarXivまたはSemantic Scholarデータベースのエントリにマッチングされています。

6.  **アーキテクチャ:**
    *   生成モデルと検索モデルはパラメータを共有しています。これにより、モデルサイズを削減し、効率的な学習を可能にします。

疑似コードで、損失関数の計算を表現すると以下のようになります。

```python
def calculate_loss(model, batch_data, lambda_value):
    # Next Token Prediction loss (L_g)
    generation_loss = model.calculate_generation_loss(batch_data)

    # Contrastive loss (L_r)
    query_embeddings = model.get_query_embeddings(batch_data) # [RET]トークンのembedding
    positive_document_embeddings = model.get_positive_document_embeddings(batch_data) # 正例の論文のembedding
    negative_document_embeddings = model.get_negative_document_embeddings(batch_data) # 負例の論文のembedding

    similarity_pos = cosine_similarity(query_embeddings, positive_document_embeddings)
    similarity_neg = cosine_similarity(query_embeddings, negative_document_embeddings)

    contrastive_loss = -log(exp(similarity_pos) / (exp(similarity_pos) + sum(exp(similarity_neg))))

    # Total loss
    total_loss = generation_loss + lambda_value * contrastive_loss

    return total_loss
```

## 6. コストや物理的な詳細について

*   **モデル:** Qwen-2.5-7B (70億パラメータ) をベースに構築
*   **データセット:** arXivのコンピュータサイエンス論文50万件
*   **学習:**
    *   最大コンテキスト長: 16,384トークン
    *   学習率: 1e-5
    *   バッチサイズ: 1 (デバイスあたり)
    *   勾配累積ステップ: 4
    *   GPU: NVIDIA H100 GPU x 32 (4台のマシン, 各マシンに8個のGPU)
    *   グローバルバッチサイズ: 1 * 8 * 4 * 4 = 128

*   **推論環境:** 80GB GPU 1基で動作

## 7. 参考文献のうち、特に参照すべきもの

特に参照すべき参考文献は以下の通りです。

*   **Lewis et al., 2020:** Retrieval-Augmented Generation (RAG) の基本的な概念を確立した論文。
*   **Karpukhin et al., 2020:** Dense Passage Retrieval (DPR) について。
*   **Xiong et al., 2020:** Approximate Nearest Neighbor Negative Contrastive Learning (ANCE) について。
*   **Li et al., 2023:** BGE (代表的なembeddingモデル) について。
*   **Gao et al., 2024:** RAGに関するサーベイ論文。

## 8. この論文を140字以内のツイートで要約すると？

学術論文向けAI執筆支援ツール「ScholarCopilot」発表！論文生成中に動的に引用検索、精度と品質が大幅UP。既存RAGを凌駕、ChatGPT超えの有用性。Qwen-2.5-7Bベース、手軽に論文作成を効率化！ #AI #学術論文 #自然言語処理


---


# Safeguarding Vision-Language Models: Mitigating Vulnerabilities to Gaussian Noise in Perturbation-based Attacks

[View Paper](http://arxiv.org/abs/2504.01308v1)

## 1. 既存研究では何ができなかったのか

既存のVision-Language Models (VLMs) の安全性に関する研究は、主に以下の点で不十分でした。

*   **ノイズに対する脆弱性の見過ごし:** 既存のVLMsは、学習時にセキュリティ対策を施しているものの、ノイズが加えられた画像入力に対する脆弱性が見過ごされていました。特に、ガウスノイズのような単純な摂動に対してさえ、多くのVLMsが脆弱であることが指摘されています。
*   **画像とテキストのミスマッチへの対応不足:** 既存のデータセット(VLGuardなど)は、画像とテキストが関連しているケースを主に対象としており、画像とテキストが無関係な場合に対するVLMsの安全性を十分に評価していませんでした。これは、VLMのファインチューニングが事前に学習されたLLMのアラインメントを阻害したり、摂動ベースの攻撃がテキストプロンプトと無関係なノイズを導入したりする可能性があるため、重要な問題です。
*   **汎用的な防御手法の欠如:** 特定の攻撃手法(FigStepなど)に対して効果的な防御策(Jailguardなど)は存在するものの、最適化に基づくより強力な視覚的摂動攻撃に対する汎用的な防御手法が不足していました。また、既存の防御手法は、推論コストを大幅に増加させるという課題もありました。

## 2. どのようなアプローチでそれを解決しようとしたか

本研究では、上記の問題を解決するために、以下の3つの主要なアプローチを採用しました。

*   **Robust-VLGuardデータセットの導入:** 画像とテキストがアラインメントされている場合とミスマッチな場合の両方を含む、新しいマルチモーダル安全データセットRobust-VLGuardを作成しました。このデータセットには、VLMのノイズ耐性を強化し、安全性のアラインメントを改善し、有用性を維持するための、慎重にキュレーションされた指示が含まれています。
*   **ノイズ拡張ファインチューニング:** Robust-VLGuardデータセットと、ガウスノイズを付加したファインチューニングを組み合わせることで、攻撃成功率を低減しながら、VLMの機能を維持しました。具体的には、学習画像にガウスノイズを付加することで、ノイズに対するVLMのロバスト性を向上させました。
*   **DiffPure-VLM防御パイプラインの提案:** より強力な最適化ベースの視覚的摂動攻撃に対処するために、DiffPure-VLMという新しい防御パイプラインを提案しました。DiffPure-VLMは、Diffusionモデルを活用して、敵対的な摂動をガウスのようなノイズに変換し、ノイズ拡張安全ファインチューニングによって防御できるようにします。

## 3. 結果、何が達成できたのか

本研究によって、以下の成果が達成されました。

*   **VLMのガウスノイズに対する脆弱性の系統的な分析:** 主流のVLMsがガウスノイズのような視覚的摂動に対して本質的なロバスト性を持っていないことを明らかにする、系統的な脆弱性分析を初めて提供しました。
*   **Robust-VLGuardデータセットの有効性の実証:** 新しい画像とテキストのミスマッチシナリオと詳細な応答を特徴とするRobust-VLGuardデータセットを、ガウスノイズ拡張と組み合わせてファインチューニングすることで、VLMのガウスノイズに対するロバスト性を高め、その有用性を維持できることを示しました。
*   **DiffPure-VLMによる最適化ベースの視覚的敵対攻撃への防御:** Diffusionモデルの分布シフト能力を利用して、敵対的なノイズを視覚入力のガウスのようなノイズに変換し、ノイズ拡張安全ファインチューニングを備えたVLMsによって防御できることを実証しました。実験結果は、DiffPure-VLMがベースラインメソッドに対して優れており、汎用性があることを示しています。
*   **敵対的摂動に対する大幅な軽減効果:** DiffPureの分布シフト特性が、安全にファインチューニングされたVLMsとうまく連携し、さまざまな強度の敵対的摂動を効果的に軽減することが実験的に示されました。

## 4. Limitationや問題点は何か

本研究には、以下のような限界と問題点が存在します。

*   **Diffusionモデルの計算コスト:** DiffPure-VLMで使用されるDiffusionモデルは、計算コストが高く、リアルタイムでの適用が難しい場合があります。特に、高解像度の画像や複雑なシナリオでは、計算時間が大幅に増加する可能性があります。
*   **Diffusionモデルの固定解像度:** 使用したDiffusionモデルの固定画像解像度のため、ダウンサンプリングとアップサンプリングの操作が必要となり、VLMのファインチューニング中に考慮されなかったアーティファクトが導入される可能性があり、評価結果に影響を与える可能性があります。
*   **データセットの偏り:** Robust-VLGuardデータセットは、特定のタスクやシナリオに偏っている可能性があり、汎用的なVLMの安全性評価には限界があります。より広範なタスクとシナリオをカバーするために、データセットを拡張する必要があります。
*   **事前学習におけるノイズ拡張の欠如:** 本研究では、ファインチューニング段階でのノイズ拡張に焦点を当てていますが、事前学習段階でのノイズ拡張の効果は十分に調査されていません。事前学習段階でノイズ拡張を組み込むことで、VLMのロバスト性をさらに向上させることができる可能性があります。
*   **敵対的攻撃の進化:** 敵対的攻撃の手法は常に進化しており、本研究で提案された防御手法が将来の攻撃に対して有効であるとは限りません。継続的な研究と防御手法の改善が必要です。
*   **倫理的な考慮事項:** VLMの安全性研究は、悪用される可能性のある脆弱性を明らかにする可能性があります。倫理的な観点から、研究成果の責任ある開示と、安全なモデルの開発を促進するための取り組みが不可欠です。

## 5. 技術的な詳細について

*   **Robust-VLGuardデータセット:**
    *   一般指示データ (General Instruction Data): GPT-4Vを使用して、詳細な応答を生成。簡潔すぎる既存のデータセットのアノテーションを改善。
    *   画像-テキスト整合安全データ (Image-Text Aligned Safety Data): VLGuardデータセットから採用。画像の内容と安全関連のテキストプロンプトを整合。
    *   画像-テキスト不整合安全データ (Image-Text Misaligned Safety Data): COCOデータセットからランダムに選択した画像と、テキストのみの安全指示を組み合わせ。
*   **ノイズ拡張ファインチューニング:**
    *   LoRA (Low-Rank Adaptation)を使用し、Vision Encoderのみをファインチューニング。
    *   学習画像にガウスノイズを付加。標準偏差は0.01から0.15の間でランダムに選択し、適用確率は70%。
    *   ファインチューニングは、単一のA100 GPUで3エポック実行。バッチサイズは16。
*   **DiffPure-VLM:**
    *   敵対的な画像をDiffPureに入力し、少数の拡散ステップを実行して画像コンテンツを維持。
    *   生成された画像は、わずかなガウスのようなノイズを含む。
    *   ノイズ耐性を持つように安全に調整されたVLMに入力し、敵対的な摂動を効果的に軽減。

    ```python
    # DiffPure-VLM Adversarial Image Purification with DDPM
    def purify_image(image, timestep):
        """
        Purifies an adversarial image using a diffusion model.

        Args:
            image: Adversarial image (numpy array or similar).
            timestep: Number of diffusion steps.

        Returns:
            Purified image with Gaussian-like noise.
        """
        # VLMが必要とするサイズにCALをリサイズする
        resized_image = resize(image, target_size=VLM.input_size)
        # Forward diffusion (adding noise)
        noised_image = get_noised_x(resized_image, t_forward) # t_forwardはtimestepのこと
        # Reverse diffusion (denoising)
        denoised_image = denoising_process(noised_image, timestep)
        # 画像の正規化
        gaussian_image = normalize(denoised_image)
        return gaussian_image

    def vlm_inference(image, prompt):
        """
        Performs inference with the VLM.

        Args:
            image: Input image.
            prompt: Text prompt.

        Returns:
            Output text.
        """
        output = VLM(image, prompt)
        return output

    # Main function
    def diffpure_vlm(adversarial_image, text_prompt, diffusion_timestep):
        """
        Complete DiffPure-VLM pipeline

        Args:
            adversarial_image: image to defend
            text_prompt: text
            diffusion_timestep: hyperparameter of diffusion model
        
        Returns:
            VLM's output
        """
        purified_image = purify_image(adversarial_image, diffusion_timestep)
        output_text = vlm_inference(purified_image, text_prompt)
        return output_text
    ```

## 6. コストや物理的な詳細について

*   **GPU:** 実験は主にA100-80G GPUを使用して実行されました。ファインチューニングには、単一のA100 GPUを使用し、約3時間かかりました。
*   **データセット:**
    *   Robust-VLGuardデータセット: 一般指示データ (4,467件)、画像-テキスト整合安全データ (1,000件)、画像-テキスト不整合安全データ (1,000件)。
    *   比較対象のVLGuardデータセット: LLaVA_v1.5_Mix_665kから5,000件、VLGuardから3,000件。
    *   COCO dataset
    *   その他MM-Vet、RealToxicityPromptsなど
*   **モデルサイズ:**
    *   MiniGPT-4 (13B)
    *   LLaVA-v1.5 (7B)
    *   InternVL2 (8B)

## 7. 参考文献のうち、特に参照すべきもの

*   **DiffPure (Nie et al.):** Diffusionモデルを用いた敵対的摂動の軽減に関する基盤となる研究。DiffPure-VLMのDiffusionモデルの活用方法を理解する上で重要です。
*   **VLGuard (Zong et al.):** VLMの安全性を高めるためのデータセットに関する研究。Robust-VLGuardデータセットの設計におけるインスピレーション源となっています。
*   **RealToxicityPrompts (Gehman et al.):** VLMの安全性評価に用いられるベンチマークデータセット。攻撃成功率の評価方法を理解する上で重要です。
*   **LoRA (Hu et al.):** ファインチューニングに使用された、大規模言語モデルの低ランク適応に関する研究。

## 8. この論文を140字以内のツイートで要約すると？

VLMsはガウスノイズに弱い！Robust-VLGuardでファインチューニング＆DiffPure-VLMで敵対的攻撃を防御🛡️Diffusionモデルで敵対的ノイズをガウスノイズに変換し、VLMの安全性を向上🎉 #VLM #安全性 #敵対的攻撃


---


# ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and Diffusion Refinement

[View Paper](http://arxiv.org/abs/2504.01934v2)

## 1. 既存研究では何ができなかったのか

既存のunifiedなMultimodal Large Language Models (MLLMs)は、以下の3つの能力を同時に高いレベルで実現することが困難でした。

*   **Visual Understanding:** 画像の内容を深く理解し、テキストと関連付ける能力。ChameleonやEMU3のようなVQGANベースのモデルは、意味的な相互作用が不足しており、LLaVAのような特化モデルに比べて視覚理解タスクで劣っていました。
*   **Image Generation:** 高品質な画像を生成する能力。
*   **Image Editing:** 指示に従って画像を編集する能力。LaViTやILLUMEのようなsemantic encoderを使用するモデルは、テクスチャの保存が不十分で、画像編集で苦戦していました。Janus seriesは、入力と出力の画像表現を分離しており、画像とテキストが混ざったタスクをシームレスに扱うことができませんでした。

## 2. どのようなアプローチでそれを解決しようとしたか

ILLUME+は、これらの課題を解決するために、以下の主要な技術要素を導入しました。

*   **Dual Visual Tokenizer (DualViTok):** 画像の細かいテクスチャとテキストに沿った意味情報を両方保持するunifiedなdual-branch visual tokenizerです。これにより、multimodalな理解と生成のためのcoarse-to-fineな画像表現が可能になります。Semantic branchは、事前に学習されたtext-aligned vision encoder (QwenViT)を利用して意味特徴を抽出し、feature reconstruction lossで学習します。Pixel branchは、semantic encoderとCNNベースのpixel encoderからのquantized featureを統合し、pixel-levelの再構築を強化します。
*   **Diffusion Decoder:** 画像のdetokenizerとしてdiffusion modelを使用し、生成品質を向上させ、効率的なsuper-resolutionを実現します。
*   **Unified MLLM with Coarse-to-Fine Image Representation:** 連続値を入力とし、離散値を生成する方式を採用し、MLLM内でcoarse-to-fineな画像表現を実現します。まずsemantic tokenを生成し、次にpixel tokenを生成します。
*   **Progressive Training Procedure:** vision tokenizer, MLLM, diffusion decoder全体でdynamic resolutionをサポートするprogressive training procedureを採用します。これにより、様々なタスクで柔軟かつ効率的なcontext-awareな画像編集と生成が可能になります。具体的には、まず低解像度から始め、徐々に高解像度へとスケールアップします。

## 3. 結果、何が達成できたのか

ILLUME+ (3B)は、以下の点で優れた結果を示しました。

*   **Multimodal Understanding, Generation, and Editing:** 既存のunified MLLMや特化モデルと比較して、これらのベンチマークで競争力のあるパフォーマンスを発揮しました。特に、文書理解タスクで優れた性能を示しました。
*   **High-Resolution Image Support:** 柔軟な高解像度画像をサポートし、視覚理解タスクを強化し、最大1024x1024の解像度で詳細な画像合成を可能にしました。
*   **Improved Texture Preservation:** ILLUMEと比較して、画像編集タスクでのテクスチャの保存が改善されました。
*   **State-of-the-art gFID score:** MJHQ30k benchmarkで6.00のgFIDスコアを達成し、高い生成品質と多様性を実現しました。

## 4. Limitationや問題点は何か

*   **モデルサイズ:** ILLUME+は3Bパラメータのモデルですが、7B以上のより大きなモデルと比較して、タスクの一般化能力が制限される可能性があります。
*   **データセットの複雑さ:** より複雑なmultimodalデータセットを構築することで、モデルの能力をさらに向上させることができます。
*   **学習の安定性:** 異なる解像度でのprogressive trainingは、学習の安定性を確保するために注意深く調整する必要があります。
*   **計算コスト:** DualViTokとdiffusion decoderのトレーニングには、大量の計算リソースが必要です。
*   **アーキテクチャの複雑さ:** DualViTok, MLLM, Diffusion Decoderの組み合わせは、単一のコンポーネントよりも複雑であり、デバッグや最適化が難しい場合があります。
*   **汎化性能:** いくつかの特定のタスクでは特化モデルに匹敵するものの、MLLMとして、広範なタスクに対する汎化性能はまだ改善の余地があります。
*   **幻覚:** 大規模言語モデル全般に言えることだが、ILLUME+も事実に基づかない内容を生成する、いわゆる「幻覚」の問題を抱えている可能性があります。
*   **バイアス:** 学習データに存在するバイアスが、生成される画像やテキストに反映される可能性があります。
*   **倫理的な問題:** 生成される画像が、悪意のある目的に利用される可能性があります。

## 5. 技術的な詳細について

ILLUME+の技術的な詳細を以下に示します。

*   **DualViTok:**
    *   **Semantic Branch:** 事前学習済みのQwenViTをsemantic feature extractorとして使用。Feature reconstruction lossで学習。ダウンサンプリング率は28x。コードブックサイズは32,768。SimVQを量子化手法として使用。
        ```python
        # Semantic Branch
        qwen_vit = QwenViT() # Pre-trained text-aligned vision encoder
        semantic_features = qwen_vit(image)
        quantized_semantic, indices = quantize(semantic_features)
        reconstructed_semantic = decode(quantized_semantic)
        loss = cosine_similarity_loss(reconstructed_semantic, semantic_features)
        ```
    *   **Pixel Branch:** MoVQGANベースのアーキテクチャ。L1 loss, perceptual loss, GAN lossで学習。ダウンサンプリング率は16x。コードブックサイズは98,304。SimVQを量子化手法として使用。
        ```python
        # Pixel Branch
        pixel_features = pixel_encoder(image)
        quantized_pixel, indices = quantize(pixel_features)
        reconstructed_image = decode(concatenate(quantized_semantic, quantized_pixel))
        loss = l1_loss(reconstructed_image, image) + perceptual_loss(reconstructed_image, image) + gan_loss(reconstructed_image, image)
        ```
    *   **Noise Injection:** 学習時に、10%の確率でサンプルをperturbし、10%のtokenをランダムに置き換えることで、ロバスト性を向上。
        ```python
        # Noise Injection
        if random.random() < 0.1:
            num_tokens = indices.shape[0]
            num_replace = int(num_tokens * 0.1)
            replace_indices = random.sample(range(num_tokens), num_replace)
            for i in replace_indices:
                indices[i] = random.randint(0, codebook_size)
        ```
*   **MLLM:**
    *   連続値を入力とし、離散値を生成する方式を採用。Semantic encoderとpixel encoderからの特徴量をそれぞれvision adaptorを通してLLMの入力空間にalign。
    *   Coarse-to-fineな画像表現 (semantic token, pixel tokenの順) を使用。`<start-of-image/semantic/pixel>`, `<end-of-image/semantic/pixel>`などの特殊tokenを使用。
*   **Diffusion Decoder:**
    *   Stable Diffusionをベースに、text encoderをzero embeddingで置き換える。DualViTokからのsemantic tokenとpixel tokenをUNet modelにconcatして注入。
    *   Super-resolution (256x256 to 512x512) を行う。
    *   学習時に、50%のサンプルにランダムなperturbationを適用 (semantic tokenは10%、pixel tokenは50%の確率でmask)。
        ```python
        # Diffusion Decoder
        latent = initial_noise(batch_size, image_size)
        for t in timesteps:
            # Cross-attention with semantic and pixel tokens
            latent = unet(latent, t, semantic_tokens, pixel_tokens)
            latent = denoise(latent, t)
        generated_image = decode_latent(latent)
        ```

## 6. コストや物理的な詳細について

*   **LLM:** 3Bパラメータ
*   **Data composition:** progressive training procedureの各ステージで異なるデータセットを使用。
*   **DualViTok, diffusion decoder:** それぞれ3日間、計6日間、256 Ascend NPUでトレーニング。
*   **MLLM:** 3段階のトレーニングを約13日間で完了。
*   **Optimizer:** AdamW (weight decayなし), constant learning rate

## 7. 参考文献のうち、特に参照すべきもの

*   **Qwen-VL:** ILLUME+のsemantic encoderとして使用されているtext-aligned vision encoder。
*   **MoVQGAN:** ILLUME+のpixel branchのアーキテクチャのベース。
*   **Stable Diffusion:** ILLUME+のdiffusion decoderのベース。
*   **ILLUME:** ILLUME+の前身となるモデル。ILLUME+の改善点がより明確になる。

## 8. この論文を140字以内のツイートで要約すると？

ILLUME+は、デュアル視覚トークナイザと拡散モデルでMLLMを強化！意味理解と高画質生成を両立し、画像編集も得意。柔軟な解像度に対応し、3Bモデルながら既存モデルに匹敵する性能！ #MLLM #画像生成 #拡散モデル


---


# Articulated Kinematics Distillation from Video Diffusion Models

[View Paper](http://arxiv.org/abs/2504.01204v1)

## 1. 既存研究では何ができなかったのか

既存のtext-to-4D生成モデルは、以下の点で課題がありました。

*   **3D構造の一貫性の欠如:** アニメーション中にオブジェクトの3D構造（例えば、キャラクターの腕の数）を維持するのが難しく、形状が破綻することがありました。
*   **物理的に不自然な関節運動:** 足が地面を滑ったり、地面にめり込んだりするなど、物理的にありえない動きが生成されることがありました。
*   **高次元自由度:**  4Dニューラル変形フィールドを用いるアプローチでは、最適化が困難になり、品質が低下することがありました。

## 2. どのようなアプローチでそれを解決しようとしたか

本論文では、Articulated Kinematics Distillation (AKD)という新しいフレームワークを提案し、以下の要素を組み合わせてこれらの課題を解決しようとしました。

*   **スケルトンベースの表現:** 3Dキャラクターをスケルトンベースで表現することで、制御する自由度を関節レベルに限定し、効率的かつ一貫性のあるモーション合成を可能にしました。これにより、変形空間が効果的に正則化され、ローカルな形状の維持に気を配る必要なく、全体的なモーションスタイルに集中できるようになります。
*   **Score Distillation Sampling (SDS)による蒸留:** 事前学習済みのビデオ拡散モデルを用いて、複雑な関節運動を蒸留し、構造的な整合性を維持します。
*   **物理ベースシミュレーションとの互換性:** スケルトンベースの表現は物理ベースシミュレーションと自然に互換性があり、物理的に妥当なインタラクションを保証します。
*   **checkerboard ground の導入:** uniform colorになりがちなレンダリング環境において、キャラクターと地面の基本的な物理的法則への準拠を強化するために checkerboard ground を導入しました。

## 3. 結果、何が達成できたのか

AKDを用いることで、以下の点を達成しました。

*   **3D形状の一貫性とモーション品質の向上:** 既存のtext-to-4D生成手法と比較して、より優れた3D形状の一貫性と表現力豊かなモーションを実現しました。
*   **物理的に妥当なモーションの生成:** 物理ベースのモーション追跡によって、生成されたモーションを物理的に妥当な軌道に投影することが可能になりました。
*   **多様なモーションモードの合成:** テキストプロンプトに応じて、歩行や走行といった異なるモーションモードを合成することができました。

## 4. Limitationや問題点は何か

本論文で言及されている制限事項と問題点、およびそれ以外に考えられるものを以下に示します。

*   **視覚的な品質の制約:** 生成されたアセットの視覚的な品質はまだ最適とは言えず、レンダリングされた出力とビデオモデルにエンコードされたリアルなビデオ分布との間にギャップがあります。
*   **モーションの多様性の依存性:** AKDによって生成されるモーションの多様性は、ビデオモデルが目的のモーションを合成する能力に大きく依存します。
*   **関節運動に限定された適用範囲:** AKDはオブジェクトの関節運動に焦点を当てており、ソフトボディダイナミクスのような他のタイプの変形には適していない可能性があります。
*   **手動リギングの前提:** パイプラインは手動でリギングされたスケルトンを前提としています。
*   **計算コスト:** SDS最適化には時間がかかります（各アセット約25時間）。

その他考えられる制限事項と問題点:

*   **プロンプトの曖昧さ:** テキストプロンプトの曖昧さや不正確さが、生成されるモーションの品質に影響を与える可能性があります。
*   **キャラクターのタイプへの依存性:** AKDが最適な結果を出すキャラクターのタイプ（例：人型、動物）に偏りがある可能性があります。
*   **リアルタイム性能の欠如:** 現在のAKDはリアルタイムでのモーション生成には適していません。

## 5. 技術的な詳細について

AKDの技術的な詳細を以下に示します。

1.  **3Dアセットの準備:**

    *   テキストから3Dアセットを生成します（例: Tet-Splatting）。
    *   メッシュをデュアルメッシュ-3DGS表現に変換します。メッシュは適切なスキニングウェイトの計算に使用され、3DGSはSDS最適化中の微分可能な変形とレンダリングをサポートします。
2.  **リギングとスキニング:**

    *   アセット内に手動で関節スケルトンを埋め込みます。
    *   Linear Blend Skinning (LBS)ウェイトを計算し、近くのGaussianカーネルに転送します。LBSは以下の式で表されます。
        ```python
        def LBS(x, R, T, w):
            """
            Linear Blend Skinning
            Args:
                x: 頂点座標
                R: 回転行列 (各ボーン)
                T: 並進ベクトル (各ボーン)
                w: スキニングウェイト (各ボーン)
            Returns:
                変形後の頂点座標
            """
            deformed_x = sum(w[i] * (R[i] @ x + T[i]) for i in range(len(R)))
            return deformed_x
        ```

        ここで、
        -   `x` は頂点座標
        -   `R[i]` は *i*-番目のボーンの回転行列
        -   `T[i]` は *i*-番目のボーンの並進ベクトル
        -   `w[i]` は *i*-番目のボーンのスキニングウェイト

3.  **モーション生成:**

    *   関節ツリー内の関節角度のシーケンスを最適化します。
    *   微分可能なForward Kinematics (FK)と3DGSラスタライズパイプラインを通過させ、ビデオシーケンスをレンダリングします。FKは、各ボーンの回転角度から、そのボーンのワールド座標系での位置と回転を計算する処理です。
4.  **Score Distillation Sampling (SDS):**

    *   レンダリングされたビデオを事前学習済みのビデオ拡散モデルに入力します。
    *   ビデオ拡散モデルからのガイダンスに基づいて、各フレームの関節角度を調整し、テキストに沿ったモーションを生成します。SDSの勾配は以下の式で近似されます。

        ```python
        def SDS_gradient(z, y, epsilon_theta, epsilon, w_t):
            """
            Score Distillation Sampling gradient
            Args:
                z: 3Dモデルパラメータ
                y: テキストプロンプト
                epsilon_theta: 拡散モデルによって予測されたノイズ
                epsilon: ランダムノイズ
                w_t: 時間ステップtの重み
            Returns:
                SDS gradient
            """
            return w_t * (epsilon_theta - epsilon)
        ```

        ここで、
        - `z` は3Dモデルのパラメータ
        - `y` はテキストプロンプト
        - `epsilon_theta` は拡散モデルによって予測されたノイズ
        - `epsilon` はランダムノイズ
        - `w_t` は時間ステップ `t` の重み

5.  **Ground Rendering:**

    *   均一な色の環境を避けるために、チェッカーボードの地面を組み込みます。
    *   地面の下にあるカーネルの不透明度をゼロに設定して、オクルージョンを考慮します。
6.  **Motion Tracking (オプション):**

    *   関節リジッドボディシミュレータに合成されたモーションを投影し、物理的に接地された軌道を生成します。
    *   PDコントローラを使って、関節トルクを制御します。トルクは以下の式で表されます。

        ```python
        def PD_controller(theta_jl_hat, theta_jl, dtheta_jl, ke, kd):
            """
            Proportional-Derivative (PD) controller
            Args:
                theta_jl_hat: 目標関節角度
                theta_jl: 現在の関節角度
                dtheta_jl: 関節角度の速度
                ke: 弾性係数
                kd: ダンピング係数
            Returns:
                関節トルク
            """
            tau = ke * (theta_jl_hat - theta_jl) - kd * dtheta_jl
            return tau
        ```

        -   `theta_jl_hat` は目標の関節角度
        -   `theta_jl` は現在の関節角度
        -   `dtheta_jl` は関節角度の速度
        -   `ke` は弾性係数
        -   `kd` はダンピング係数
        *   このフレームワークでは、`Warp`を用いて物理シミュレーションを実装し、`PyTorch`と統合しています。

## 6. コストや物理的な詳細について

*   **GPU:** NVIDIA A100-40GB グラフィックスカード1枚
*   **最適化時間:** 各アセットあたり約25時間 (10,000回のSDS最適化イテレーション)
*   **ビデオ拡散モデル:** CogVideoX-5B
*   **データセット:** 事前学習済みのビデオ拡散モデルを使用。独自のデータセットは使用せず。
*   **3Dアセット数:** 29個の静的アセット（動物や人型を含む）を生成し、評価に使用
*   **使用ライブラリ:** Warp, PyTorch, Blender, Pinocchio

## 7. 参考文献のうち、特に参照すべきもの

*   **DreamFusion:** Text-to-3Dの基礎となるSDSの概念を理解するために重要。
*   **3D Gaussian Splatting:**  リアルタイムレンダリングと微分可能なラスタライズに関する技術を理解するために重要。
*   **Tet-Splatting:** テキストから3Dメッシュアセットを生成する方法の理解に役立つ。
*   **CogVideoX:** 使用されているビデオ拡散モデルのアーキテクチャを理解するために重要。

## 8. この論文を140字以内のツイートで要約すると？

テキストから3Dキャラアニメを生成するAKDを発表！スケルトン表現とビデオ拡散モデルのSDSで、物理的に自然で構造的に一貫したモーションを実現。物理シミュレーションとの連携も可能！ #AI #3Dアニメ #モーション生成


---


# MegaTTS 3: Sparse Alignment Enhanced Latent Diffusion Transformer for Zero-Shot Speech Synthesis

[View Paper](http://arxiv.org/abs/2502.18924v4)

## 1. 既存研究では何ができなかったのか

既存のゼロショットTTSモデルは、以下の点で課題を抱えていました。

1.  **明示的な speech-text alignment がないモデルの課題:** 特に難しい文において、頑健性に欠け、実用的なアプリケーションでの性能が低下していました。Attention mechanism を用いた Soft alignment では、ロバストネスに課題が残ります。具体的には、
    *   Autoregressive codec language models (AR LM): 非効率であり、ロバスト性に欠けます。
    *   Diffusion-based models without explicit duration modeling: 生成速度は速いものの、明示的なアラインメントを用いた手法と比較して、音声の明瞭さが低下します。

2.  **強制アラインメントに基づく事前定義されたモデルの課題:** 自然さが損なわれていました。
    *   事前定義されたアラインメントは、モデルの探索空間を制約し、より自然な音声を生成することを妨げます。
    *   全体の自然さは、duration model の性能に大きく依存します。

## 2. どのようなアプローチでそれを解決しようとしたか

MegaTTS 3では、以下の手法を導入することで、上記の問題を解決しようとしました。

1.  **Sparse speech-text alignment:**  Latent diffusion transformer (DiT) をガイドする、革新的なスパースアラインメントアルゴリズムを導入しました。
    *   強制アラインメント領域内に phoneme トークンを疎に配置し、おおまかな発音情報を提供し、それを latent DiT モデルで洗練します。これにより、アラインメントの難易度を下げつつ、探索空間を制限しないようにしました。

2.  **Multi-condition classifier-free guidance (CFG):**  アクセントの強さを柔軟に調整できるように、話者の音色とテキストコンテンツに対して個別にガイダンススケールを調整する multi-condition CFG 戦略を採用しました。

3.  **Piecewise rectified flow (PeRFlow):** 生成プロセスを高速化するために、piecewise rectified flow 技術を採用し、推論ステップ数を削減しました。

## 3. 結果、何が達成できたのか

MegaTTS 3は、以下の点を達成しました。

1.  **State-of-the-art のゼロショットTTS音声品質:** LibriSpeech の test-clean セットにおいて、ほぼ最先端の音声明瞭度と話者類似性を実現し、高い音声自然性を示しました。

2.  **アクセントの強さの柔軟な制御:**  Multi-condition CFG により、アクセントの強さを柔軟にコントロールできるようになりました。テキストガイダンススケールを調整することで、アクセントの強さを変えることができます。

3.  **高速な音声生成:** PeRFlow により、わずか8サンプリングステップで高品質な1分間の音声を生成できるようになりました。

## 4. Limitationや問題点は何か

### 本文で言及されているもの

1.  **言語サポートの制限:** 現在、英語と中国語のみをサポートしています。

### その他考えられるもの

1.  **倫理的な懸念:** 声のクローニングによる悪用のリスクがあります。論文中でも、この点について言及があり、対応策を検討すると述べています。
2.  **計算コスト:** モデルサイズが大きく、学習や推論に高い計算リソースが必要です。PeRFlow で推論速度は改善されていますが、学習コストは依然として高いと考えられます。
3.  **Sparse alignment の調整:** スパースアラインメントのアンカーの配置方法や密度が、モデルの性能に影響を与える可能性があります。最適な設定を見つけるには、追加の実験が必要となる場合があります。
4.  **Duration prediction の性能:** スパースアラインメントは duration model の性能にロバストであるとされていますが、極端に性能の悪い duration predictor を使用した場合の性能低下は不明です。

## 5. 技術的な詳細について

### アーキテクチャ

MegaTTS 3 は latent diffusion transformer (DiT) をベースとしており、以下のコンポーネントで構成されています。

1.  **WaveVAE:** 入力音声を圧縮し、低次元の latent space にエンコードします。
    *   VAE encoder, Wave decoder, Discriminators で構成されます。
    *   Wave decoder は、convolutional upsampler と Hifi-GAN decoder を使用しています。
    *   Latent channel size は 32 です。
    *   目的関数:
        ```python
        L = L_rec + L_KL + L_Adv
        L_rec = ||s - s_hat||^2  # Reconstruction loss
        #L_KL = ...  # KL divergence loss
        #L_Adv = ...  # Adversarial loss
        ```

2.  **Latent Diffusion Transformer:**  WaveVAE でエンコードされた latent vector を入力として、テキスト情報と話者情報に基づいて音声latentを生成します。
    *   LLAMA の standard transformer block を基本構造として使用しています。
    *   24層の Transformer, 16 attention heads, 1024 embedding dimensions で構成されます。
    *   Rotary Position Embedding (RoPE) を positional embedding として使用します。

3.  **Sparse Alignment:** phoneme レベルのアラインメント情報をスパースな形で latent space に注入します。
    *   強制アラインメントから得られた phoneme の duration 情報に基づき、各 phoneme に対応する latent vector の位置にアンカーを配置します。
    *   アンカー以外の位置はマスクされます。

### 学習

1.  **WaveVAE:**
    *   Adam optimizer, learning rate = 1e-4
    *   Batch size = 40 (per GPU), total step = 2M
2.  **Latent Diffusion Transformer:**
    *   Adam optimizer, learning rate = 5e-5
    *   Batch size = 10K latent frames (per GPU), total step = 1M
    *   Text guidance scale (alpha_txt) と speaker guidance scale (alpha_spk) を調整します。

### 推論

1.  Euler sampler を使用して、Flow ODE を解きます。
2.  PeRFlow により、サンプリングステップ数を削減します (25 steps -> 8 steps)。
3.  Multi-condition CFG により、テキストと話者の情報を組み合わせて音声を生成します。

## 6. コストや物理的な詳細について

*   **GPU:** 学習には NVIDIA A100 (80GB) GPU を使用
*   **データセット:**
    *   WaveVAE, MegaTTS 3: 60k 時間の unlabeled speech (LibriVox audiobooks など)
    *   600kh 内部 multilingual training corpus (英語と中国語) のデータも使用
*   **モデルサイズ:**
    *   MegaTTS 3: 339M パラメータ

## 7. 参考文献のうち、特に参照すべきもの

1. **NaturalSpeech 3:** ゼロショットTTSにおける factorized codec と diffusion model の利用。MegaTTS 3 の比較対象として重要です。
2. **Piecewise Rectified Flow (PeRFlow) paper:**  生成プロセス高速化の技術基盤。
3. **LLAMA paper:** Transformer block の基本構造。
4. **F5-TTS:** ゼロショットTTSの評価において、LibriSpeech-PC test-clean セットを使用している点。

## 8. この論文を140字以内のツイートで要約すると？

MegaTTS 3：Sparse AlignmentでZero-Shot TTSを大幅改善！疎なアラインメントで自然さを高め、DiTを効率的に学習。アクセント制御も自由自在！PeRFlowで高速生成も実現。 #TTS #AI #音声合成


---


# Target-Aware Video Diffusion Models

[View Paper](http://arxiv.org/abs/2503.18950v2)

## 1. 既存研究では何ができなかったのか

既存の制御可能なimage-to-video拡散モデルは、アクターの動きをターゲットに向かわせるために、密な構造的またはモーションの手がかりに依存していました。しかし、人間のオブジェクトインタラクション（HOI）シナリオでは、正確なアクションガイダンスを提供することが難しく、既存研究では以下のような課題がありました。

*   **HOIシナリオにおけるアクションガイダンスの困難さ:** 正確な行動ガイダンスを提供することが難しい。
*   **ターゲットへの正確なインタラクションの欠如:** アクターが指定されたターゲットと正確にインタラクションするビデオの生成が困難。

## 2. どのようなアプローチでそれを解決しようとしたか

この論文では、ターゲット認識ビデオ拡散モデルを提案し、以下の方法で既存研究の課題を解決しようとしました。

1.  **ターゲットマスクの導入:** ターゲットを示す単純なマスクを入力として使用し、事前学習済みモデルの汎化能力を活用して、もっともらしいアクションを生成します。
2.  **特殊トークンの導入:** テキストプロンプト内でターゲットの空間情報をエンコードする特別なトークンを導入します。
3.  **クロスアテンション損失の導入:** このトークンに関連付けられたクロスアテンションマップを入力ターゲットマスクに合わせる新しいクロスアテンション損失を導入し、モデルをファインチューニングします。
4.  **選択的な損失の適用:** 最も意味的に関連性の高いトランスフォーマーブロックとアテンション領域に選択的に損失を適用することで、パフォーマンスを向上させます。

疑似コードで表現すると、以下のようになります。

```python
# ターゲット認識拡散モデルの学習プロセス

# 入力:
#   image: 入力画像
#   target_mask: ターゲットのセグメンテーションマスク
#   text_prompt: 実行するアクションのテキストプロンプト

# モデルの構造:
#   拡散モデル (DiffusionModel)
#   テキストエンコーダ (TextEncoder)
#   画像エンコーダ (ImageEncoder)
#   クロスアテンション機構 (CrossAttention)

# 1. テキストプロンプトに特殊トークンを追加
text_prompt_with_target = text_prompt + " <target>"  # 例: "person is reaching for the <target>"

# 2. モデルの順伝播
latent = ImageEncoder(image) # 入力画像を潜在空間にエンコード
text_embedding = TextEncoder(text_prompt_with_target) # テキストプロンプトをエンコード
noised_latent = DiffusionModel.add_noise(latent, timestep) # 潜在変数にノイズを加える
predicted_noise = DiffusionModel(noised_latent, timestep, text_embedding) # ノイズを予測

# 3. クロスアテンションマップの取得
cross_attention_maps = CrossAttention(noised_latent, text_embedding) # クロスアテンションマップを計算
target_attention_map = cross_attention_maps[:, target_token_index] # ターゲットトークンのアテンションマップを抽出

# 4. クロスアテンション損失の計算
attention_loss = CrossEntropy(target_attention_map, target_mask) # ターゲットマスクとのクロスエントロピー損失

# 5. 拡散損失の計算
diffusion_loss = MSE(predicted_noise, noise) # 予測ノイズと実際のノイズとの平均二乗誤差

# 6. 全損失の計算 (クロスアテンション損失の重み付け)
total_loss = diffusion_loss + lambda * attention_loss  # 全損失

# 7. モデルのパラメータ更新
optimizer.zero_grad()
total_loss.backward()
optimizer.step()
```

## 3. 結果、何が達成できたのか

実験結果から、このターゲット認識モデルは、アクターが指定されたターゲットと正確にインタラクションするビデオを生成する点で、既存のソリューションよりも優れていることが示されました。さらに、ビデオコンテンツ作成とゼロショット3D HOIモーション合成という2つのダウンストリームアプリケーションで有効性を示しました。

## 4. Limitationや問題点は何か

本文では、具体的な Limitation や問題点について明示的に言及されていません。しかし、一般的に拡散モデルにおいて考えられる課題点、および本論文の提案手法固有の課題点は以下の通りです。

*   **計算コスト:** 拡散モデルは一般的に計算コストが高く、高品質なビデオを生成するには多くの計算資源が必要です。
*   **データセットへの依存性:** モデルの性能は、学習に使用するデータセットの品質に大きく依存します。学習データに存在しないアクションやオブジェクトの組み合わせは、生成が難しい可能性があります。
*   **汎化性能の限界:** 未知のターゲットや複雑なアクションシーケンスに対する汎化性能には限界がある可能性があります。
*   **ターゲットマスクの精度:** ターゲットマスクの精度が低い場合、生成されるビデオの品質が低下する可能性があります。ターゲットマスクの生成自体が課題となる場合もあります。
*   **人間の主観的な評価:** 生成されたビデオの品質は、人間の主観的な評価に依存する部分があります。客観的な評価指標の確立が課題です。
*   **生成されるコンテンツの倫理的な問題:** 意図しない、あるいは有害なコンテンツが生成されるリスクがあります。

## 5. 技術的な詳細について

*   **モデル構造:** ベースラインモデルを拡張し、ターゲットマスクを新たに入力として追加します。
*   **特殊トークン:** ターゲットの空間情報をエンコードするために、テキストプロンプトに特別なトークンを追加します。このトークンは、クロスアテンション機構を通じてターゲットマスクの情報と統合されます。
*   **クロスアテンション損失:** クロスアテンションマップとターゲットマスクを整合させるために、新しい損失関数を導入します。この損失関数は、ターゲットトークンに関連付けられたアテンションが、入力ターゲットマスクの領域に集中するように学習を促進します。
*   **選択的な損失の適用:** すべてのトランスフォーマーブロックに対して損失を適用するのではなく、意味的に最も関連性の高いブロックとアテンション領域に絞って損失を適用することで、学習効率と性能を向上させます。
*   **ファインチューニング:** 構築したデータセットを用いて、モデルをファインチューニングします。

## 6. コストや物理的な詳細について

論文のAbstractとContentsから、トレーニングに使用したGPUの数や時間、データセット、モデルのサイズに関する具体的な記述は確認できませんでした。

## 7. 参考文献のうち、特に参照すべきもの

論文のContentsが提供されていないため、参考文献リストを確認することができません。そのため、特に参照すべき文献を特定することはできません。

## 8. この論文を140字以内のツイートで要約すると？

指定されたターゲットとインタラクトする動画を生成する #拡散モデル を開発！ターゲットマスクと特殊トークンで制御し、HOIタスクで高精度なアクションを実現。#AI #動画生成


---

はい、承知いたしました。以下に質問に対する回答をMarkdown形式で記述します。


# MergeVQ: A Unified Framework for Visual Generation and Representation with Disentangled Token Merging and Quantization

[View Paper](http://arxiv.org/abs/2504.00999v1)

## 1. 既存研究では何ができなかったのか

既存のMasked Image Modeling (MIM) とVector Quantization (VQ) を用いた手法は、主に以下の点で課題を抱えていました。

*   **生成品質 vs. 表現学習と効率のトレードオフ:** 潜在空間を共有する際に、生成品質、表現学習、効率のバランスを取ることが困難でした。一方を優先すると、他方が犠牲になる傾向がありました。
*   **VQの制限:**
    *   **勾配近似:** 標準的なVQ (VQGANなど) では、勾配近似が最適化のボトルネックとなっていました。
    *   **コードブックの非効率な利用:** commitment loss によって勾配分布が不均一になり、コードブックの崩壊を引き起こす可能性がありました。
    *   **離散表現のボトルネック:** 量子化によって詳細な空間情報が失われ、高画質の画像再構成が困難になり、表現学習と生成の両方に影響が出ていました。
*   **表現学習と生成の一貫性の欠如:** 一方のタスクで改善が見られても、他方のタスクに必ずしも良い影響を与えませんでした。これは、同一の埋め込み空間に対する競合する目的（表現学習ではクラス間識別、生成では詳細の再構成）が原因であると考えられていました。

## 2. どのようなアプローチでそれを解決しようとしたか

MergeVQは、上記の課題を解決するために、以下の技術的な要素を取り入れています。

*   **トークンマージング:** トークンマージング技術をVQベースの生成モデルに組み込み、潜在空間からtop-kの意味情報を分離します。エンコーダ内のself-attentionブロックの後にトークンマージモジュールを配置し、Look-up Free Quantization (LFQ) とグローバルアライメントを行います。
*   **Source Recovery (ソース復元):** トークンマージングによって失われた詳細な情報を、デコーダ内のcross-attentionを通じて復元します。トークンレベルのコンテキストを明示的にモデル化し、元のトークンの位置情報を保持するソースマトリックスを活用します。
*   **MergeAR (効率的なラスタースキャン):** 第二段階の生成において、KV Cache圧縮を実行し、ラスタースキャン順の予測を効率化します。これにより、トークンの冗長性を削除し、メモリ効率を向上させます。
*   **グローバルアライメント制約:** 識別タスクのためにトークン表現を強化するため、DINOによるself-distillationを通じて、マージされたトークンとグローバルな画像セマンティクスを整合させます。
*   **可変マージ比率:** 学習中に、多様なマージ比率を適用することで、モデルのロバスト性と汎化能力を向上させます。

疑似コードで表現すると、以下のようになります。

```python
# エンコーダ処理
def encoder(image):
    # CNN特徴抽出
    feature_map = CNN(image)
    # 平坦化
    tokens = flatten(feature_map)
    # Self-Attention + トークンマージ
    merged_tokens, source_matrix = ToMeAttention(tokens)
    # LFQ (Look-up Free Quantization)
    quantized_tokens = LFQ(merged_tokens)
    return quantized_tokens, source_matrix

# デコーダ処理
def decoder(quantized_tokens, source_matrix):
    # Source Recovery (トークン復元)
    recovered_tokens = SourceRecovery(quantized_tokens, source_matrix)
    # 画像再構築
    reconstructed_image = CNN_Decoder(recovered_tokens)
    return reconstructed_image

# MergeARによる効率的なラスタースキャン
def merge_ar_generate(start_token, merge_instruction_token):
    generated_tokens = []
    kv_cache = {} # KV Cache
    position_cache = {} # Position Cache
    current_token = start_token
    while not is_end_token(current_token):
        next_token = predict_next_token(current_token, kv_cache)
        if is_duplicate(next_token, kv_cache):
            position_cache[next_token] = "redundant"
        else:
            kv_cache[next_token] = get_kv(next_token)
        generated_tokens.append(next_token)
        current_token = next_token
    return generated_tokens
```

## 3. 結果、何が達成できたのか

MergeVQは、ImageNetでの実験において、以下の点を達成しました。

*   **競争力のある性能:** 表現学習と画像生成の両タスクにおいて、競争力のある性能を実現しました。
*   **優れた効率:** 良好なトークン効率と推論速度を維持しました。
*   **表現学習と生成の統合:** トークンマージングとソース復元モジュールにより、表現学習と生成のギャップを埋めることに成功しました。
*   **多様なマージ比率によるロバスト性:** 学習中に多様なトークン数をモデルに適用することで、ロバスト性と汎化能力が向上しました。
*   **高速化:** MergeARにおけるKV Cache圧縮によって、ラスタースキャン順の生成を高速化しました。

具体的には、Linear probingで79.8%、Fine-tuningで84.2%の精度を達成しています。

## 4. Limitationや問題点は何か。本文で言及されているものの他、あなたが考えるものも含めて

MergeVQの限界点と課題は以下の通りです。

*   **タスク間のトレードオフ:** MergeVQ (G+R) は、表現学習と生成の両方を扱うため、MergeVQ (R) よりも若干性能が低下しています。これは、両方のタスクを最適化するためには、より多くのトークンが必要となる可能性があることを示唆しています。
*   **計算コスト:** トークンマージングやソース復元モジュールは、追加の計算コストを必要とします。特に、ソース復元モデルのトレーニングには、計算資源が必要となる場合があります。
*   **ハイパーパラメータの調整:** トークンマージングの比率やLFQのパラメータなど、多くのハイパーパラメータがあり、タスクやデータセットに応じて適切な調整が必要となります。
*   **汎用性の検証:** ImageNet以外のデータセットやタスクに対する汎用性の検証が不足しています。
*   **詳細な分析の必要性:** なぜMergeVQが有効なのか、トークンマージングやソース復元モジュールがどのように機能しているのかについて、さらなる詳細な分析が必要です。

## 5. 技術的な詳細について。技術者が読むことを想定したトーンで

MergeVQの技術的な詳細について解説します。

*   **エンコーダアーキテクチャ:** CNNエンコーダ (`caligraphic_E_{ϕ}`) は、低レベルの特徴を抽出し、ピクセルレベルでの再構成のための帰納的バイアスを提供します。後続のTransformer層 (`caligraphic_E_{θ}`) は、ToMeAttentionを使用して動的なダウンサンプリングを行い、注意機構の効率と表現能力のバランスを取ります。ハイブリッドモデルとして、ResNetブロックの階層的なステージを組み合わせ、事前初期化されたTransformerを統合します。

*   **トークンマージング (ToMeAttention):** トークンマージングは、計算効率を向上させるためにトークン数を削減します。類似したトークンを結合することで、情報損失を最小限に抑え、精度低下を防ぎます。具体的には、トークンを2つのグループに分割し、最も類似したペアを結合します。トークンの類似度は、self-attention機構のキー (`K`) を使用して測定されます（コサイン類似度やドット積など）。マージされたトークンの注意スコアは、トークンサイズ (`s`) に基づいて調整され、表現の一貫性が保たれます。マージスケジュールのバリエーションとして、線形減少または二乗減少が利用可能です。

*   **Look-up Free Quantization (LFQ):** LFQは、チャンネル方向の圧縮を実現し、コードブックの使用率を向上させながらオーバーヘッドを削減します。これにより、離散的な潜在空間への効率的なエンコーディングが可能になります。

*   **ソース復元モデル:** ソース復元モデル (`caligraphic_R_{ω}`) は、トークンマージングによって失われた位置情報を復元します。圧縮されたトークンとソースマトリックス (`S`) を入力として受け取り、Transformerデコーダを使用して詳細な情報を再構築します。学習可能なクエリとcross-attentionメカニズムを利用して、位置関係を予測します。

*   **MergeAR:** MergeARは、ラスタースキャン順の生成を高速化するために、KV Cache圧縮を行います。重複するトークンを動的にプルーニングし、位置情報を記録するシステムを使用して、空間的な一貫性を維持します。因果マスク (`M`) を使用してトレーニングプロセスをガイドし、KV Cacheから冗長なトークンを排除します。

疑似コードで表すと以下のようになります。

```python
# トークンマージの例
def tome_attention(tokens, merge_ratio):
    # tokens: (N, D) - Nはトークン数、Dは特徴次元
    num_merge = int(tokens.shape[0] * merge_ratio)
    # 類似度計算 (例: コサイン類似度)
    similarity = cosine_similarity(tokens)
    # 最も類似したペアを選択
    pairs_to_merge = find_most_similar_pairs(similarity, num_merge)
    # トークンをマージ
    merged_tokens = merge_tokens(tokens, pairs_to_merge)
    # ソースマトリックス作成
    source_matrix = create_source_matrix(tokens.shape[0], merged_tokens.shape[0], pairs_to_merge)
    return merged_tokens, source_matrix

def cosine_similarity(tokens):
    # ... コサイン類似度計算の実装 ...
    pass

def find_most_similar_pairs(similarity, num_merge):
    # ... 最も類似したペアを見つける実装 ...
    pass

def merge_tokens(tokens, pairs_to_merge):
    # ... トークンをマージする実装 ...
    pass

def create_source_matrix(original_size, merged_size, pairs_to_merge):
    # ... 元トークンとマージ後トークンの関係を示すソースマトリックスを作成 ...
    pass
```

## 6. コストや物理的な詳細について。例えばトレーニングに使用したGPUの数や時間、データセット、モデルのサイズなど

MergeVQのトレーニングに関するコストと物理的な詳細は次のとおりです。

*   **データセット:** ImageNet-1K (アノテーションなし)
*   **トレーニングエポック:** 300エポック
*   **最適化アルゴリズム:** AdamW
*   **ハイパーパラメータ:**
    *   (β1, β2) = (0.9, 0.999)
    *   基本学習率: 4 × 10^-4
    *   バッチサイズ: 1024
    *   ウェイト減衰: 0.05
*   **モデルサイズ:**
    *   MergeVQ (G): エンコーダ = 63Mパラメータ, トークン数 = 256
    *   MergeVQ (G+R): エンコーダ = 62Mパラメータ, トークン数 = 144
    *   MergeVQ (R): エンコーダ = 86Mパラメータ, トークン数 = 36
*   **その他:** 具体的なGPUの種類と台数、トレーニング時間は記載されていません。ただし、論文には「OPPO AI Center and AI Station of Westlake University for the support of GPUs」と記載されており、複数のGPUを使用してトレーニングが行われたことが示唆されています。

MergeARのGeneratorはLlamaGen-Lをベースにしており、24層のTransformer decoderを使用しています。

## 7. 参考文献のうち、特に参照すべきもの

MergeVQを理解する上で、以下の参考文献は特に重要です。

*   **[VQ-VAE](https://arxiv.org/abs/1711.00937):** Vector Quantizationを用いた離散表現学習の基礎。
*   **[VQGAN](https://arxiv.org/abs/2012.09841):** VQ-VAEを敵対的学習と組み合わせ、高解像度画像生成を可能にした。
*   **[ToMe](https://arxiv.org/abs/2303.12421):** トークンマージングによるViTの効率化。MergeVQの主要なコンポーネントであるトークンマージングのメカニズムを理解する上で重要。
*   **[DINOv2](https://arxiv.org/abs/2304.07193):** 自己教師あり学習によるロバストな視覚特徴の学習。MergeVQのグローバルアライメントに用いられている。
*   **[LlamaGen](https://arxiv.org/abs/2401.02954):** Autoregressive model beats diffusion: Llama for scalable image。MergeVQのAR生成のベースライン。
*   **[RandAR](https://arxiv.org/abs/2305.18278):** Randomized autoregressive visual generation. MergeVQにおけるランダム順序生成に関連する。

これらの文献を読むことで、MergeVQの背景にある技術や、MergeVQがどのようにして既存手法の課題を克服しているかをより深く理解することができます。

## 8. この論文を140字以内のツイートで要約すると？

MergeVQ: トークンマージとVQで視覚生成と表現学習を統合！✨ 潜在空間から意味情報を分離し、細部を復元。MergeARで高速生成も🚀 ImageNetで高性能＆高効率を実証済み！ #AI #画像生成 #表現学習


---


# Enhanced OoD Detection through Cross-Modal Alignment of Multi-Modal Representations

[View Paper](http://arxiv.org/abs/2503.18817v1)

## 1. 既存研究では何ができなかったのか

既存のout-of-distribution detection (OoDD) 研究は、主に以下の点で課題がありました。

*   **単一モダリティモデルへの偏重:** 従来のOoDD研究は、主に画像データのみを用いる単一モダリティモデルに焦点を当てていました。
*   **マルチモーダルモデルの潜在能力の未活用:** CLIPのような大規模な事前学習済みvision-languageモデル(VLM)が登場したものの、既存のマルチモーダルOoDD手法は、事前学習済み重みを固定するか、部分的にしか調整しないことが多く、ダウンストリームデータセットに対して最適ではありませんでした。
*   **ファインチューニングの限界:**  ナイーブなファインチューニング手法では、VLMに埋め込まれた事前知識を十分に活用できていませんでした。特に、in-distribution (ID) データの埋め込み空間におけるモダリティギャップ（画像とテキストの埋め込みが分離してしまう現象）が、その原因となっていました。
*   **ID精度とOoDD性能の両立の難しさ:** ゼロショット(ZS) OoDD研究はOoDD性能の向上に重点を置くあまり、モデルの信頼性にとって重要なID精度を軽視する傾向がありました。
*   **負の概念の活用不足:** ファインチューニングベースの手法では、NegLabelのようなゼロショット手法が持つ、事前学習された負の概念（negative concepts）を効果的に活用できていませんでした。

## 2. どのようなアプローチでそれを解決しようとしたか

本研究では、上記の課題を解決するために、Cross-Modal Alignment (CMA)という新しいマルチモーダルファインチューニング(MMFT)手法を提案しました。 具体的には、以下の要素を取り入れています。

*   **クロスモーダルアライメントの強化:**  IDデータに対する画像とテキストの埋め込み間の距離を正則化することで、モダリティギャップを解消し、クロスモーダルなアライメントを強化する学習目的を導入しました。
*   **エネルギーベースモデル(EBM)との接続:**  提案する正則化が、超球面上のEBMの最尤推定に対応することを理論的に示しました。
*   **コントラスト損失とCMA損失の組み合わせ:** CLIP損失(コントラスト損失)に加えて、各モダリティに対して、IDイメージとテキストペア間の類似性を高めるCMA損失を導入しました。
    *   `L_image_CMA = -log(sum(exp(image_embedding @ text_embedding.T / tau)))`
    *   `L_text_CMA = -log(sum(exp(image_embedding.T @ text_embedding / tau)))`
    *   `L_CMA = L_CLIP + lambda * (L_image_CMA + L_text_CMA)`
*   **負の概念の活用:**  NegLabelのOoDスコアリングスキームを採用し、IDイメージとテキストをネガティブラベルから十分に分離することで、OoDD性能を向上させました。
*   **ハイパースフィア上での表現:** イメージとテキストの埋め込みをハイパースフィア上に投影し、その上でアライメントを行うことで、表現能力を高めました。
*   **一様性とアライメントの観点からの分析:** 提案手法が、ハイパースフィア上のIDモダリティギャップをどのように縮小するのかを一様性とアライメントの観点から分析しました。

## 3. 結果、何が達成できたのか

提案手法であるCMAを適用した結果、以下の成果を達成しました。

*   **最先端のOoDD性能:** ImageNet-1k OoDベンチマークデータセットにおいて、既存手法を大幅に上回る最先端のOoDD性能を達成しました。
*   **高いID精度:** OoDD性能だけでなく、IDデータの分類精度も向上しました。
*   **モダリティギャップの解消:** CMAにより、画像とテキストの埋め込み空間におけるモダリティギャップが縮小され、クロスモーダルな表現が改善されました。
*   **理論的な裏付け:** CMAが、ハイパースフィア上のエネルギーベースモデルの最尤推定に対応することを理論的に示し、手法の妥当性を示しました。
*   **効果的な負の概念の活用:** 事前学習済みの負の概念を効果的に活用し、OoDD性能を向上させました。
*   **多様なベンチマークでの有効性:** MOSと標準のOpenOOD v1.5ベンチマークで、優れた性能を示しました。特にMOSベンチマークでは、FPR95で19.93%、AUROCで95.13%という高い性能を達成しました。

## 4. Limitationや問題点は何か

本研究には、以下のlimitationsと問題点が存在します。

*   **Near-OoDデータセットにおける性能:** OpenOOD v1.5ベンチマークのNear-OoDシナリオ（特にSSB-hardデータセット）では、他の手法と比較して中程度の性能しか示しませんでした。これは、テキスト情報を活用したOoDD手法が、意味的に類似したデータセットにおいては、その利点が薄れる可能性を示唆しています。
*   **ハイパーパラメータλの調整:** 提案手法には、アライメントの強度を制御するハイパーパラメータλが含まれており、その適切な設定には実験的な調整が必要です。
*   **計算コスト:** MMFTは、ZSやPLと比較して計算コストが高くなる可能性があります。特に、大規模なデータセットでの実験には、高性能なGPU環境が必要です。
*   **プロンプトの選択:** NegLabelのような負の概念を利用する手法では、プロンプトの選択が性能に影響を与える可能性があります。
*   **他のアーキテクチャへの適用:** 本研究ではCLIPアーキテクチャに焦点を当てていますが、他のVLMアーキテクチャへの適用可能性については、さらなる調査が必要です。
*   **データセットへの依存:** ImageNet-1kに特化したチューニングが、他のデータセットへの汎化性能に影響を与える可能性があります。
*   **解釈可能性:** CMAがOoDD性能を向上させるメカニズムについては、一様性とアライメントの観点から分析していますが、より詳細な解釈可能性に関する研究が求められます。

## 5. 技術的な詳細について

CMAの技術的な詳細について、以下に説明します。

1.  **Embeddingの抽出:**
    *   CLIPのイメージエンコーダ`f(I)`とテキストエンコーダ`g(T)`を用いて、イメージ`I`とテキスト`T`からそれぞれ埋め込み`i`と`t`を抽出します。
    *   `i = f(I)`
    *   `t = g(T)`
    *   抽出された埋め込みは、L2ノルムで正規化され、単位ベクトルとしてハイパースフィア上に投影されます。

2.  **CLIP損失:**
    *   CLIP損失`L_CLIP`は、イメージとテキストのペアが近い埋め込み空間に配置されるように学習します。
    *   `L_image = -log(exp((i @ t) / tau) / sum(exp((i @ t_j) / tau) for t_j in all_text_embeddings))`
    *   `L_text = -log(exp((i @ t) / tau) / sum(exp((i_j @ t) / tau) for i_j in all_image_embeddings))`
    *   `L_CLIP = (L_image + L_text) / (2 * batch_size)`
    *   ここで、`tau`は温度パラメータであり、類似度のスケーリングに使用されます。

3.  **CMA損失:**
    *   CMA損失`L_CMA`は、イメージとテキストの埋め込み間のアライメントを強化します。
    *   `L_image_CMA = -log(sum(exp(i @ t_j / tau) for t_j in all_text_embeddings))`
    *   `L_text_CMA = -log(sum(exp(i_j @ t / tau) for i_j in all_image_embeddings))`
    *   `L_CMA = L_CLIP + lambda * (L_image_CMA + L_text_CMA) / (2 * batch_size)`
    *   ここで、`lambda`はアライメントの強度を制御するハイパーパラメータです。

4.  **NegLabelによるOoDスコアリング:**
    *   与えられたイメージ`I`に対して、IDテキスト埋め込み`t_k`とnegative labelのテキスト埋め込み`over_t_k`を用いて、OoDスコア`S(I)`を計算します。
    *   `S(I) = sum(exp(i @ t_k / tau) for t_k in ID_text_embeddings) / (sum(exp(i @ t_k / tau) for t_k in ID_text_embeddings) + sum(exp(i @ over_t_k / tau) for over_t_k in negative_label_embeddings))`

5.  **エネルギーベースモデル(EBM)との関連:**
    *   CMAは、EBMの枠組みにおいて、log marginal density（生成項）を最適化することと等価です。
    *   `E(i, t) = - (i @ t)`と定義すると、イメージ`i`とテキスト`t`の結合分布はGibbs-Boltzmann分布として表すことができます。
    *   `q(i, t) = exp(-E(i, t) / tau) / Z`
    *   ここで、`Z`は分配関数です。

6.  **一様性とアライメントの評価:**
    *   埋め込み空間の構造を分析するために、一様性(Uniformity)とアライメント(Alignment)のメトリックを使用します。
    *   `Uniformity = -log(mean(exp(-2 * ||e_i - e_j||^2)))`
    *   `Alignment = -mean(||i_i - t_i||^2 - min(||i_i - t_j||^2 for t_j in all_text_embeddings except t_i))`
    *   これらのメトリックを様々な埋め込みの組み合わせに対して計算することで、モダリティギャップの程度を定量化します。

## 6. コストや物理的な詳細について

実験に使用したリソースは以下の通りです。

*   **GPU:** 8 NVIDIA A6000 GPUs (各48GBメモリ)
*   **OS:** Ubuntu 20.04.6 LTS
*   **ソフトウェア:** Python 3.9.18, PyTorch 1.12.0+cu116
*   **データセット:**
    *   ImageNet-1k (IDデータセット): 50,000枚の検証画像を使用
    *   MOSベンチマークのOoDデータセット: iNaturalist, SUN, Places, Textures (各10,000枚、Texturesは5,640枚)
    *   OpenOOD v1.5ベンチマーク: Near-OoD (SSB-hard, NINCO), Far-OoD (iNaturalist, Textures, OpenImage-O)
*   **モデル:** 事前学習済みのCLIP-B/16モデル
*   **ハイパーパラメータ:**
    *   学習率: {1e-4, 1e-5, 1e-6}
    *   アライメント強度 (lambda): {1e-1, 1e-2, 1e-3}
    *   バッチサイズ: 512
    *   重み減衰: {0.0, 0.1}
*   **トレーニング時間:** 具体的なトレーニング時間は明記されていませんが、MMFTはZSやPLよりも計算コストが高く、A6000 GPUを8基使用して実験を実施しています。
*   **ショット数:** PLの比較において16ショット設定の実験とフルショット設定の実験を行っています。

## 7. 参考文献のうち、特に参照すべきもの

*   **Radford et al., 2021:** CLIP (Learning Transferable Visual Models From Natural Language Supervision) - Vision-Languageモデルの基礎となる研究であり、CLIPアーキテクチャの理解に不可欠です。
*   **Jiang et al., 2022:** Negative Label Guided OOD Detection with Pretrained Vision-Language Models - 負の概念を利用したOoDDの重要性を示しており、CMAのスコアリングスキームに影響を与えています。
*   **Zhang et al., 2023:** OpenOOD v1.5: Enhanced Benchmark for Out-of-Distribution Detection - OoDDの評価に使用したベンチマークデータセットの詳細が記載されています。
*   **Liang et al., 2023:** Mind the Gap: Understanding the Modality Gap in Multi-Modal Contrastive Representation Learning - マルチモーダル表現学習におけるモダリティギャップについて深く考察しており、CMAの動機付けとなっています。

## 8. この論文を140字以内のツイートで要約すると？

マルチモーダルFine-tuningでOoD検知を大幅改善！画像とテキストの埋め込みをCross-Modal Alignment (CMA)で最適化。モダリティギャップを解消し、最高精度を達成。EBMとの理論的接続も 제시! #OOD #AI #CLIP


---


# Adaptive Layer-skipping in Pre-trained LLMs

[View Paper](http://arxiv.org/abs/2503.23798v1)

## 1. 既存研究では何ができなかったのか

既存のLLMにおけるレイヤースキップ手法は、トークン生成時の計算需要がトークンの種類によってどのように変化するかという根本的な問題を考慮していませんでした。つまり、すべてのトークンに対して一律にレイヤースキップを適用しており、複雑な計算が必要なトークンと、単純なトークンを区別していませんでした。また、既存研究の多くは、LLMを最初からトレーニングする必要があったり、事前学習済みのモデルのパラメータを変更する必要がありました。

## 2. どのようなアプローチでそれを解決しようとしたか

本研究では、FlexiDepthという、テキスト生成においてTransformerレイヤーの使用数を動的に調整する手法を提案しました。 具体的には、以下の要素を導入しました。

*   **プラグインルーター:** 各Transformerレイヤーで、隠れ状態の入力をそのレイヤーを通過させるかスキップするかを決定します。
*   **アダプター:** レイヤースキップによって生じる表現のずれを解消します。

FlexiDepthは、事前学習済みのLLMのパラメータを変更せずに、これらのモジュールを組み込むことで、アダプティブなレイヤースキップを可能にします。 ルーターとアダプターのみが学習可能なコンポーネントであり、その他すべてのパラメータは事前学習済みLLMから継承され、固定されます。 また、重要な点として、スキップされたレイヤーについてもKVキャッシュを計算することで、オートリグレッション生成に必要なコンテキストを保持します。

## 3. 結果、何が達成できたのか

FlexiDepthをLlama-3-8Bモデルに適用した結果、32レイヤーのうち8レイヤーのスキップを達成しつつ、ベンチマーク性能を100%維持できました。 実験結果から、LLMにおける計算需要はトークンの種類によって大きく異なり、反復的なトークンや固定されたフレーズの生成には少ないレイヤーで済み、計算や不確実性の高いトークンの生成にはより多くのレイヤーが必要となることが示されました。さらに、この適応的な割り当てパターンは人間の直感とも一致することがわかりました。研究を促進するため、FlexiDepthとそのレイヤー割り当てパターンを記録したデータセットをオープンソース化しました。

## 4. Limitationや問題点は何か

*   **ハードウェアスループットの改善:** FlexiDepthはFLOPsを削減しますが、既存のGPUハードウェアではスループットの改善にはつながりませんでした。これは、バッチ内のサンプルが異なる実行パスを通るため、制御フローの管理や不規則なメモリアクセスのオーバーヘッドが発生するためです。ハードウェア向けの最適化が必要です。

*   **アダプティブレイヤースキップの理論的根拠:** なぜアダプティブレイヤースキップが性能向上に繋がるのか（例：正則化効果）については仮説が述べられていますが、更なる研究が必要です。

*   **モデルサイズに対するスケーリング:** 実験では異なるサイズのモデルでFlexiDepthを評価していますが、非常に大規模なモデル（数十Bパラメータ以上）での挙動は不明です。

*   **ルーターの最適化:** bottlenecked MLPルーターが有効であることが示されていますが、他のアーキテクチャやより効率的なルーターの設計の余地があります。

## 5. 技術的な詳細について

FlexiDepthは、事前学習済みのLLMの各Transformerレイヤーにプラグイン可能なルーターとアダプターを追加します。

1.  **ルーターの設計:** 各レイヤーの入力隠れ状態に対し、ルーターがスキップするかどうかを決定します。 ルーターは、bottlenecked MLPに基づいており、以下の疑似コードで表現できます。

    ```python
    def router(hidden_state, W_down, W_up, W_r):
        # hidden_state: (batch_size, seq_len, hidden_dim)
        # W_down: (router_dim, hidden_dim)
        # W_up: (hidden_dim, router_dim)
        # W_r: (1, hidden_dim)

        normalized_hidden_state = normalize(hidden_state)
        down_projected = matmul(normalized_hidden_state, transpose(W_down))
        tanh_applied = tanh(down_projected)
        up_projected = matmul(tanh_applied, transpose(W_up))
        gating_score = sigmoid(matmul(up_projected, transpose(W_r)))  # (batch_size, seq_len, 1)
        return gating_score
    ```

    ゲートスコア`gating_score`に基づき、閾値`tau`と比較してスキップの決定を行います。 微分可能な近似のために、threshold-based routing mechanism with a predefined thresholdが使用されます。

2.  **KVキャッシュの保持:** スキップされたレイヤーについてもKVキャッシュを計算します。これにより、将来のトークンがスキップされた状態のコンテキスト情報にアクセスできるようになり、オートリグレッション生成の整合性が保たれます。 具体的には、クエリベクトルは計算しませんが、キーベクトルとバリューベクトルは計算します。

3.  **アダプター:** スキップされた隠れ状態と、通常の処理を行った隠れ状態の表現を揃えるために、軽量なアダプターを使用します。 アダプターはFFNと同じ構造を持ちますが、中間層の次元を16分の1に削減しています。

4.  **損失関数:** 次のトークン予測損失と、レイヤー使用数の二乗和を最小化するスキップ損失を組み合わせて最適化します。 スキップ損失は以下の疑似コードで表現できます。
    ```python
    def skipping_loss(gating_scores):
        #gating_scores: (num_layers, seq_len), each element is g_lt in the paper
        num_layers, seq_len = gating_scores.shape
        sum_of_g_lts = np.sum(gating_scores, axis=0) #sum over layers
        squared_sum = sum_of_g_lts ** 2 #element-wise square
        loss = np.mean(squared_sum) #mean over the token sequence
        return loss
    ```

## 6. コストや物理的な詳細について

*   **モデル:** Llama-3-8B-Instruct (32 Transformerレイヤー)
*   **訓練データセット:** Tulu-v2
*   **ルーター中間層の次元:** 隠れ層の次元の1/16
*   **最適化:** AdamW
*   **学習率:** 1e-4
*   **バッチサイズ:** 64 (global)
*   **ハードウェア:** 8 NVIDIA A100-PCIE-40GB GPUs
*   **学習時間:** 約7時間
*   **スキップ損失の係数:** 1e-3

## 7. 参考文献のうち、特に参照すべきもの

*   **[Raposo et al., 2023] Mixture-of-depths: Dynamically allocating compute in transformer-based language models.** : アダプティブ計算の初期の研究であり、FlexiDepthの動機付けとなった研究の一つです。

*   **[Ivison et al., ] Camels in a changing climate: Enhancing lm adaptation with tulu 2.**: FlexiDepthの訓練に使用されたTulu-v2データセットに関する情報が含まれています。

*   **[Men et al., ] Shortgpt: Layers in large language models are more redundant than you expect.**: LLMにおけるレイヤーの冗長性に着目し、レイヤースキップの可能性を示唆した研究です。

## 8. この論文を140字以内のツイートで要約すると？

FlexiDepth: 事前学習済みLLMにプラグイン可能なルーターとアダプターを導入し、トークン毎にレイヤー数を動的に調整！ Llama-3-8Bで8レイヤー削減しつつ性能維持。トークンタイプで計算需要が違うことを発見。 #LLM #レイヤースキップ #効率化


---


# PaperBench: Evaluating AI's Ability to Replicate AI Research

[View Paper](http://arxiv.org/abs/2504.01848v1)

## 1. 既存研究では何ができなかったのか

既存の研究では、AIエージェントが最先端のAI研究を自律的に再現する能力を、包括的かつ客観的に評価することが困難でした。具体的には、以下の点が課題でした。

*   **複雑なタスクの評価**: 論文の内容を理解し、コードを開発し、実験を実行するという複雑なプロセス全体を評価できるベンチマークが存在しませんでした。
*   **客観的な評価基準**: 研究の再現における成功度合いを客観的に測定するための、明確な評価基準がありませんでした。主観的な判断に頼らざるを得ない部分が多く、評価の信頼性が低い状況でした。
*   **評価のコスト**: 人間の専門家による評価は時間と労力がかかるため、大規模な評価が困難でした。一つの再現試行を評価するのに数十時間かかる場合もありました。
*   **再現性の評価**: 単に既存のコードを利用する能力ではなく、AIエージェントがゼロからコードを書き、実行する能力を評価する必要がありました。既存のコードベースに依存するのではなく、エージェントが自力で問題を解決できるかを評価することが重要でした。
*   **長期間にわたるタスクの評価**: 研究論文の再現には時間がかかるため、AIエージェントが長期間にわたってタスクを遂行する能力を評価する必要がありました。短期的なタスクだけでなく、長期的な計画と実行能力を評価することが求められました。

## 2. どのようなアプローチでそれを解決しようとしたか

PaperBenchでは、上記の問題を解決するために、以下のアプローチを採用しました。

*   **包括的なベンチマークの構築**: 最先端のAI研究論文の再現をタスクとするベンチマーク「PaperBench」を構築しました。このベンチマークには、ICML 2024の主要な論文から選ばれた20件の研究が含まれています。
*   **階層的な評価ルーブリックの開発**: 各論文の再現タスクを、より小さなサブタスクに分解し、明確な評価基準を設けた階層的な評価ルーブリックを開発しました。これにより、客観的かつ詳細な評価が可能になりました。各ルーブリックは、論文の著者と共同で作成され、正確性と現実性を担保しています。
*   **LLMベースの自動評価 judge の導入**: 大規模な評価を可能にするため、LLM（Large Language Model）ベースの自動評価 judge を開発しました。この judge は、ルーブリックに基づいて再現試行を自動的に評価します。judge の性能は、人間による評価との比較によって検証されています。
*   **ゼロからのコード開発の要求**: AIエージェントには、論文の著者のオリジナルコードを使用せず、ゼロからコードを開発することを要求しました。これにより、エージェントのコーディング能力と問題解決能力を直接的に評価できます。
*   **再現フェーズの導入**: 評価の信頼性を高めるために、再現フェーズを導入しました。AIエージェントの提出物を、クリーンな環境で実行し、結果を検証します。これにより、エージェントがハードコードされた結果を提出するのを防ぎます。
*   **部分的な進捗の評価**: 評価ルーブリックには、部分的な進捗を評価するための Execution ノードと Code Development ノードが含まれています。これにより、エージェントが完全に再現できなくても、部分的な成功に対して評価を与えることができます。

## 3. 結果、何が達成できたのか

PaperBenchの導入により、以下の成果が得られました。

*   **AIエージェントの再現能力の定量化**: PaperBenchを用いることで、AIエージェントが最先端のAI研究を再現する能力を定量的に評価することが可能になりました。
*   **モデル間の比較**: 複数の最先端モデルをPaperBenchで評価した結果、Claude 3.5 Sonnet (New) が最高のパフォーマンスを示し、平均再現スコア21.0%を達成しました。
*   **人間との比較**: 機械学習の博士号取得者による再現試行と比較した結果、現時点ではモデルは人間のパフォーマンスに及ばないことがわかりました。
*   **評価の自動化**: LLMベースの自動評価 judge を開発し、その性能を評価しました。これにより、評価プロセスを大幅に効率化することができました。
*   **オープンソース化**: PaperBenchのコードをオープンソース化し、AIエージェントのAIエンジニアリング能力の研究を促進しました。
*   **軽量版のベンチマーク**: より広範なコミュニティが利用できるように、PaperBench Code-Devという軽量版のベンチマークをリリースしました。

## 4. Limitationや問題点は何か。本文で言及されているものの他、あなたが考えるものも含めて

PaperBenchには、以下の Limitation や問題点があります。

*   **ベンチマークの規模**: PaperBenchに含まれる論文は20件のみであり、AI研究コミュニティの成果全体を網羅するには不十分です。理想的には、より多くの論文を評価対象とすべきです。
*   **著者のコードの存在**: ほとんどの論文において、著者のコードがオンラインで公開されています。大規模なコーパスで事前学習されたモデルは、これらのコードから知識を獲得している可能性があり、ベンチマークの性能が実際よりも高く評価される可能性があります。
*   **ルーブリック作成のコスト**: 詳細なルーブリックを作成するには、専門家が数日間を要するため、非常に労力がかかります。ルーブリックの品質を維持しながら、作成プロセスを効率化する必要があります。
*   **自動評価 judge の精度**: 自動評価 judge の性能は、人間の専門家による評価には及びません。また、judge は非決定的なモデルを使用しているため、評価結果が変動する可能性があります。
*   **評価コスト**: PaperBenchの評価には、モデルの推論コストと計算環境のコストがかかります。特に、GPUを必要とするタスクではコストが高くなります。
*   **Specification gaming の可能性**: AIエージェントが、ベンチマークの評価基準を悪用して、過剰な性能または過小な性能を示す可能性があります。
*   **長期的な視野の欠如**: PaperBench は、研究論文の再現という特定のタスクに焦点を当てており、長期的な研究開発プロセス全体を評価するものではありません。
*   **創造性の欠如**: PaperBench は、既存の研究論文の再現を評価するものであり、AIエージェントの創造性や独自性を評価するものではありません。

## 5. 技術的な詳細について。技術者が読むことを想定したトーンで

PaperBenchの技術的な詳細について説明します。

*   **タスク**: AIエージェントは、与えられた研究論文（PDFおよびMarkdown形式）と、論文の著者が提供する補足情報（addendum）に基づいて、論文の実験結果を再現する必要があります。
*   **提出物**: AIエージェントは、再現に必要なすべてのコードを含むリポジトリを提出します。リポジトリのルートディレクトリには、`reproduce.sh` ファイルが含まれている必要があり、このスクリプトが実験結果を再現するためのエントリポイントとなります。
*   **再現フェーズ**: 提出されたリポジトリは、クリーンなUbuntu 24.04イメージ（A10 GPU付き）の仮想マシン上で実行されます。`reproduce.sh` スクリプトが実行され、結果が生成されます。実行時間は12時間に制限されています。
*   **評価ルーブリック**: 各論文には、再現の成功に必要な具体的なアウトカムを定義する階層的なルーブリックが用意されています。ルーブリックは、論文の著者と共同で作成され、合計8,316個の評価可能なタスクが含まれています。
    *   ルーブリックの各ノードは、Code Development, Execution, Result Match のいずれかのタイプに分類されます。
        *   **Code Development**: ソースコードが要件を正しく実装しているかを評価します。
        *   **Execution**: `reproduce.sh` スクリプトの実行が要件を正常に実行するかを評価します。
        *   **Result Match**: 再現結果が論文の結果と一致するかを評価します。
    *   リーフノードはバイナリ（合格/不合格）で評価され、親ノードのスコアは子ノードの加重平均として計算されます。
*   **自動評価 judge**: LLMベースの自動評価 judge は、各リーフノードを個別に評価します。judge には、論文のMarkdown、ルーブリックのJSON、リーフノードの要件、および提出物（フィルタリングされたコードベース）が入力として与えられます。
*   **JudgeEval**: 自動評価 judge の精度を評価するために、JudgeEval という補助的な評価ベンチマークが作成されました。JudgeEval では、人間が評価したデータセットと自動評価 judge の出力を比較します。
*   **エージェント環境**: AIエージェントは、インターネットにアクセス可能であり、必要なオンラインサービス（HuggingFace、OpenAI APIなど）のAPIキーが提供されます。エージェントは、ブラックリストに登録されたリソース（論文著者のコードリポジトリなど）を使用することはできません。
*   **エージェントの足場 (scaffolding)**: 実験では、Inspect AIの basic agent をベースにしたシンプルなエージェント足場を使用しました。この足場は、ツール使用ループを実行し、エージェントにBashシェルコマンド実行ツール、Pythonコード実行ツール、Webブラウザツール、およびファイルリーダツールを提供します。

疑似コードでルーブリックの評価方法を示すと以下のようになります。

```python
def evaluate_rubric(submission, rubric, judge):
    """
    ルーブリックに基づいて提出物を評価する

    Args:
        submission: 提出物（コード、実行結果など）
        rubric: 評価ルーブリック（階層構造）
        judge: 評価 judge （人間またはLLM）

    Returns:
        最終的な再現スコア
    """

    def evaluate_node(node):
        """
        ルーブリックのノードを評価する（再帰的）
        """
        if node.is_leaf():
            # リーフノードの場合、judge を使用して評価
            score = judge.evaluate(submission, node)  # 0 or 1
            return score
        else:
            # 非リーフノードの場合、子ノードを評価し、加重平均を計算
            child_scores = [evaluate_node(child) for child in node.children]
            weighted_sum = sum(
                child.weight * score for child, score in zip(node.children, child_scores)
            )
            total_weight = sum(child.weight for child in node.children)
            if total_weight == 0:
                return 0  # 子ノードがない場合、スコアは0
            score = weighted_sum / total_weight
            return score

    # ルートノードを評価し、最終的なスコアを取得
    final_score = evaluate_node(rubric.root)
    return final_score
```

## 6. コストや物理的な詳細について。例えばトレーニングに使用したGPUの数や時間、データセット、モデルのサイズなど

PaperBenchの実験におけるコストと物理的な詳細について説明します。

*   **エージェントの実行環境**: Ubuntu 24.04 Dockerコンテナ内で、単一のA10 GPUを使用します。
*   **エージェントの実行時間**: 最大実行時間は12時間です。
*   **自動評価 judge のコスト**: o3-mini を使用した場合、1つの提出物を評価するのに約66 USDのAPIクレジットがかかります。PaperBench Code-Dev では、約10 USDにコストが削減されます。
*   **人間による評価のコスト**: 専門家による評価は、1つの論文あたり数十時間を要します。
*   **データセット**: ICML 2024のSpotlightおよびOral論文から選ばれた20件の研究論文を使用します。
*   **モデル**: Claude 3.5 Sonnet (New), OpenAI o1, o3-mini など、複数の最先端モデルを使用します。
*   **API**: エージェントには、HuggingFaceおよびOpenAI APIのAPIキー（1000 USD分のクレジット付き）が提供されます。
*   **評価コストの削減**: PaperBench Code-Dev (PBCD) は、GPU要件を排除するだけでなく、コストの問題にも対処するように設計されています。Execution がないため rollouts の実行時間を半減することが可能であり、その結果 eval run あたりのコストは4000 USD となります。
    * grading コストは PaperBench Code-Dev においては1論文あたり平均10 USD に削減されます。
*   **実験的な評価 judge**: 実験的な SimpleJudge を使用した場合、評価コストを10分の1に削減できることが示唆されています。

これらのコストは、モデルの選択、APIの使用量、およびハードウェアの要件によって大きく変動します。PaperBench Code-Dev は、GPU要件を排除し、評価コストを削減するための代替手段となります。

## 7. 参考文献のうち、特に参照すべきもの

PaperBenchの研究において、特に参照すべき参考文献は以下のとおりです。

*   **Anthropic's Responsible Scaling Policy**: Anthropicによる責任あるスケーリングポリシーに関する記述。AIの安全な開発を促すための枠組みとしてPaperBenchがどのように活用できるか理解できます。
*   **DeepMind’s Frontier Safety Framework**: DeepMindによるフロンティアセーフティフレームワーク。AIの潜在的なリスクを評価し、軽減するための対策についてPaperBenchがどのように貢献できるか知ることができます。
*   **MLE-bench: Evaluating machine learning agents on machine learning engineering**: 機械学習エージェントの機械学習エンジニアリング能力を評価するためのベンチマーク。PaperBenchとの比較を通じて、PaperBenchの独自性や貢献を把握できます。
*   **RE-Bench: Evaluating frontier AI R&D capabilities of language model agents against human experts**: 大規模言語モデルエージェントのフロンティアAIの研究開発能力を人間専門家と比較評価する研究。PaperBenchとの類似点や相違点を理解し、PaperBenchの位置づけを明確にできます。
*   **Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers**: 大規模言語モデルが斬新な研究アイデアを生成できるかどうかの大規模な人間による研究。LLMの潜在能力と限界を理解し、PaperBenchの評価結果を解釈するのに役立ちます。

## 8. この論文を140字以内のツイートで要約すると？

AIがAI研究を再現できるか？PaperBenchは最先端ML論文の再現を評価するベンチマーク。Claude 3.5 Sonnetが最高スコアも人間には及ばず。自動評価judgeで効率化。AIの自律的な研究開発能力を測る第一歩。#AI #機械学習 #ベンチマーク


---

# LSNet: See Large, Focus Small

[View Paper](http://arxiv.org/abs/2503.23135v1)

## 1. 既存研究では何ができなかったのか

既存の軽量ビジョンネットワークは、主に self-attention メカニズムや畳み込みを用いて token mixing を行っていました。この依存により、以下の点が課題でした。

*   **有効性と効率の限界:** 軽量ネットワークにおける知覚と集約のプロセスにおいて、有効性と効率が制限されていました。
*   **性能と効率のトレードオフ:** 計算リソースが限られた状況下で、性能と効率のバランスを取ることが困難でした。
*   **自己注意機構の課題:** 重要でない領域への過剰な注意、計算コストの増大、表現能力の制限。
*   **畳み込みの課題:** 固定カーネルによる制約、コンテキストへの適応性の欠如、表現力の制限。

## 2. どのようなアプローチでそれを解決しようとしたか

本研究では、人間の視覚システムに着想を得て、"See Large, Focus Small"という戦略を提案しました。具体的には、以下の要素を取り入れたLSNetを設計しました。

*   **LS Convolutionの導入:** 大規模カーネルによる知覚と小規模カーネルによる集約を組み合わせることで、広範囲の知覚情報を効率的に捉え、動的かつ複雑な視覚表現のための精密な特徴集約を実現します。
*   **LSNetアーキテクチャの構築:** LS Convolutionを基本ブロックとして、軽量モデルのファミリーであるLSNetを構築しました。
*   **Dynamic Heteroscale Vision Ability:** 周辺視野による広範囲の知覚と中心視野による詳細な集約という、人間の視覚システムのダイナミックな異種スケール視覚能力を模倣。
*   **Large-Kernel Perception (LKP):** 広範囲のコンテキスト情報を捉え、空間関係をモデリングします。
*   **Small-Kernel Aggregation (SKA):** 高度に関連する視覚フィールド内の特徴を融合します。
*   **効率的な設計:** Depth-wise Convolution と Group Mechanism を用いて LS Convolution を効率的に設計。

## 3. 結果、何が達成できたのか

LSNetは、様々なビジョンタスクにおいて、既存の軽量ネットワークと比較して、優れた性能と効率を達成しました。

*   **ImageNet画像分類:** 既存のSOTA軽量モデルと比較して、同等以上の性能を達成。
*   **COCO物体検出/インスタンスセグメンテーション:** 物体検出およびインスタンスセグメンテーションにおいて、優れた転移性能を示しました。
*   **ADE20Kセマンティックセグメンテーション:** セマンティックセグメンテーションタスクにおいて、性能向上が確認されました。
*   **ロバスト性:** ImageNet-Cなどの様々なベンチマークで、ロバストな性能を示しました。

## 4. Limitationや問題点は何か。本文で言及されているものの他、あなたが考えるものも含めて

**論文で言及されているLimitations:**

*   **他のシナリオへの拡張:** 視覚言語タスクや教師なし学習などの他のシナリオへのLSNetの応用は、計算リソースの制限により行われていません。
*   **大規模データセットでの事前学習:** 大規模データセットでのLSNetの事前学習も、計算リソースの制限により行われていません。
*   **潜在的な悪用:** モデルの悪用の可能性について言及していますが、具体的な対策は本論文の範囲外としています。

**私が考える問題点:**

*   **パラメータチューニング:** 大規模カーネルサイズやグループ数など、LS convolutionのパラメータ設定は経験的なものが多く、タスクやデータセットごとに最適な値を探索する必要がある可能性があります。
*   **汎用性:** "See Large, Focus Small"の戦略が、すべてのビジョンタスクに適しているとは限りません。例えば、グローバルなコンテキストが重要なタスクでは、LS convolutionの局所的な集約がボトルネックになる可能性があります。
*   **解釈性:** LS convolutionの動作は、従来の畳み込み層と比較して複雑であり、モデルの解釈が難しい可能性があります。モデルの改善には、可視化や分析による理解が重要です。
*   **計算コストの最適化:** 論文中ではLS convolutionの計算コストが低いことが強調されていますが、大規模カーネルのdepth-wise convolutionはメモリ消費量が大きい可能性があります。特に高解像度の画像処理においては、メモリ効率の改善が課題となる可能性があります。

## 5. 技術的な詳細について。技術者が読むことを想定したトーンで

LSNetは、人間の視覚システムを模倣した "See Large, Focus Small" 戦略に基づいて設計された軽量ビジョンネットワークです。キーとなる要素技術は LS convolution です。

*   **LS Convolution:**
    *   **Large-Kernel Perception (LKP):**  `kernel_size` が `K_L` の depth-wise 畳み込みを行い、広範囲のコンテキスト情報を効率的に捉えます。Point-wise 畳み込み (PWConv) でチャネル数を削減した後、再びPWConvを行い、空間関係のモデリングを行います。
        ```python
        def large_kernel_perception(x, K_L, reduction_ratio=2):
            C = x.shape[1]
            x = nn.Conv2d(C, C // reduction_ratio, kernel_size=1)(x) # PWConv
            x = nn.Conv2d(C // reduction_ratio, C // reduction_ratio,
                          kernel_size=K_L, padding=K_L // 2, groups=C // reduction_ratio)(x) # DWConv
            x = nn.Conv2d(C // reduction_ratio, D, kernel_size=1)(x) # PWConv
            return x
        ```
    *   **Small-Kernel Aggregation (SKA):** LKP で得られたコンテキスト情報を基に、grouped dynamic convolution を行い、関連性の高い領域の特徴を効率的に集約します。チャネルを `G` 個のグループに分割し、グループごとに異なる集約ウェイトを適用します。
        ```python
        def small_kernel_aggregation(x, weights, K_S, G):
            C = x.shape[1]
            y = []
            for g in range(G):
                x_g = x[:, g*C//G : (g+1)*C//G]
                weight_g = weights[:, g]
                y_g = nn.Conv2d(C // G, C // G, kernel_size=K_S, padding=K_S // 2, groups=C // G, weight=weight_g)(x_g)
                y.append(y_g)
            y = torch.cat(y, dim=1)
            return y
        ```
*   **LS Block:** LS convolution を基本ブロックとして、skip connection、depth-wise convolution、SE layer、FFN を組み合わせ、モデルの表現能力を高めます。
*   **LSNet Architecture:** Overlapping patch embedding、LS Block、Downsampling layer、MSA Block を組み合わせ、効率的な特徴抽出と空間解像度の削減を実現します。

## 6. コストや物理的な詳細について。例えばトレーニングに使用したGPUの数や時間、データセット、モデルのサイズなど

論文に記載されている主なトレーニングの詳細とリソースは以下の通りです。

*   **データセット:** ImageNet-1K, COCO-2017, ADE20K
*   **イメージサイズ:** 224x224 (ImageNet-1K), 1333x800 (COCO), 512x512 (ADE20K)
*   **トレーニングエポック:** 300 (ImageNet-1K), 12 (COCO), 40K iterations (ADE20K)
*   **オプティマイザ:** AdamW
*   **バッチサイズ:** 2048 (ImageNet-1K), 16 (COCO), 32 (ADE20K)
*   **初期学習率:** 4e-3 (ImageNet-1K), 2e-4 (COCO), 2e-4 (ADE20K)
*   **学習率スケジューラ:** Cosine annealing (ImageNet-1K), Poly learning rate (ADE20K)
*   **使用GPU:** Nvidia RTX 3090 (推論スループット測定)
*   **FLOPs:** LSNet-T (0.3G), LSNet-S (0.5G), LSNet-B (1.3G)

具体的なトレーニング時間、GPU数など詳細な情報については論文中に記載されていません。

## 7. 参考文献のうち、特に参照すべきもの

*   **MobileNetV2 (Sandler et al.):** 軽量CNNアーキテクチャの設計における重要な先行研究であり、depth-wise separable convolutionやinverted residual blockといった効率的なモジュールが提案されています。
*   **Swin Transformer (Liu et al.):** Transformerベースのアーキテクチャであり、階層的な特徴表現とshifted windowによる効率的なattention計算が特徴です。LSNetの設計において、MSA blockの採用など、影響を受けていると考えられます。
*   **EfficientFormer (Li et al.):** 軽量なVision Transformerの設計に関する研究であり、latencyとperformanceのトレードオフを改善するためのdimension-consistent design paradigmが提案されています。LSNetと同様に、効率的なモデル設計を目指しています。
*   **Dynamic Convolution (Chen et al.):** LSNetのSKAで用いられているdynamic convolutionに関する研究であり、入力に応じて畳み込みカーネルを動的に生成する手法が提案されています。LSNetでは、LKPによって得られたコンテキスト情報を用いてdynamic convolutionのカーネルを生成することで、より効果的な特徴集約を実現しています。

## 8. この論文を140字以内のツイートで要約すると？

人間の視覚システムに着想を得た #LSNet 発表！ "See Large, Focus Small" 戦略で、広範囲知覚(LKP)と詳細集約(SKA)を組み合わせたLS Convolutionを提案。軽量なのに高性能！画像認識、物体検出、セマンティックセグメンテーションでSOTA達成！ #AI #ComputerVision
'''

---


# Towards Physically Plausible Video Generation via VLM Planning

[View Paper](http://arxiv.org/abs/2503.23368v2)

## 1. 既存研究では何ができなかったのか

既存のビデオ拡散モデル（VDMs）は、大規模なビデオデータセットで学習することで、写実的なビデオ生成において目覚ましい進歩を遂げました。しかし、それらは現実世界の物理法則を理解し、それらに従うビデオを生成するという点で課題を抱えていました。具体的には、以下のような問題点がありました。

*   **物理的な妥当性の欠如:** 商用VDMでさえ、物理法則に準拠したビデオを生成するのが困難でした。生成されたビデオは視覚的にはリアルに見えるかもしれませんが、現実の物理的な動きを模倣できませんでした。例えば、ボールが落下して跳ね返るようなシーンで、不自然な動きや物理的にあり得ない挙動を示すことがありました。
*   **テキスト記述の曖昧さと限界:** PhyT2Vのように、物理プロセスを詳細に記述したテキストプロンプトを使用しても、VDMが物理的に妥当なビデオを生成するのを助けるには不十分でした。テキスト記述とビデオ内の実際の動きの間にはギャップと曖昧さがあり、VDMは物理法則を一般的に理解するのではなく、トレーニングデータに過剰適合する傾向がありました。
*   **計算コストと専門知識:** グラフィックスエンジンからのシミュレーションを使用してVDMをガイドする方法もありましたが、これらのアプローチはグラフィックスエンジンがシミュレートできる物理効果に依存し、高い計算コストを伴いました。また、これらのシミュレーターを使用するには、高度な専門知識が必要であり、ユーザーフレンドリーではありませんでした。
*   **特定タイプの物理運動への限定:** PhysDiffのように、既存の手法は特定の種類の物理運動（自然な振動など）に焦点を当てており、物理的に妥当なビデオを生成するための汎用的なアプローチを確立していませんでした。
*   **モーションコントロールの課題:** 既存のモーションコントロール手法（バウンディングボックス制御、ポイント軌道制御、カメラモーション制御など）は、モーションの制御に重点を置いていましたが、物理的な妥当性が見過ごされることがよくありました。
*   **大規模な物理データの不足:** 物理現象の抽象性と多様性のため、スケーラブルな物理データを収集してモデルをトレーニングすることは困難でした。

## 2. どのようなアプローチでそれを解決しようとしたか

この論文では、これらの問題を解決するために、Vision Language Model (VLM) プランニングを通じて物理的に妥当なビデオ生成を行うための新しい二段階の画像からビデオへの生成フレームワークを提案しました。このフレームワークは、物理を明示的に組み込むことで、VDMの限界に対処します。

このアプローチの主要なアイデアは、VLMとVDMの強みを組み合わせることです。VLMは物理的な推論と計画に優れており、VDMは詳細でリアルなビデオを生成できます。

**段階1: VLMによる粗粒度のモーションプランニング**

*   VLM（GPT-4oを使用）を粗粒度のモーションプランナーとして使用します。
*   与えられた画像とテキストプロンプトに基づいて、VLMはシーン内のオブジェクトを認識し、それらのバウンディングボックスを特定します。
*   VLMは、シーンに適用可能な物理法則を決定します。
*   Chain-of-Thought (CoT) と物理認識推論を使用して、現実世界の物理ダイナミクスを近似する粗いモーション軌道/変化を予測します。VLMは、オブジェクトの将来のバウンディングボックスの位置を予測し、フレーム間の一貫性を保証します。
*   VLMは、与えられたシーンに対する物理法則とコンテキストを決定し、ステップごとに推論を実行して、画像空間内のオブジェクトの物理的に妥当な動きを予測します。

**段階2: VDMによる詳細なモーション合成**

*   第一段階でVLMによって予測されたモーション軌道/変化を使用して、VDMのビデオ生成をガイドします。
*   VLMによって計画された近似パス/変化を条件として、画像からビデオへの拡散モデルを使用して詳細なモーションを生成します。
*   予測されたモーション軌道/変化は粗いため、推論中にノイズを追加して、VDMがより細かいモーションを自由に生成できるようにします。これにより、VDMはVLMによって提供された粗いガイダンスを洗練し、詳細レベルの動き（速度、加速度、振動など）を合成できます。
*   粗いモーション軌道から合成されたモーションシーケンスをアニメーション化し、対応するオプティカルフローを導出します。オプティカルフローは、拡散モデルをガイドするための構造化ノイズとして使用されます。
*   さらに、推論フェーズ中にノイズを注入することで、VDMに柔軟性を持たせ、必要に応じて粗いモーション軌道から逸脱して高品質の詳細レベルの動きを生成できるようにします。

## 3. 結果、何が達成できたのか

提案されたフレームワークは、既存の手法と比較して、物理的に妥当なビデオ生成において優れたパフォーマンスを発揮しました。主な達成事項は以下の通りです。

*   **物理的な妥当性の向上:** フレームワークは、物理的に妥当なモーションを生成することができ、既存の商用クローズドソースVDMが失敗するタスクを実現しました。
*   **最先端のパフォーマンス:** 確立された2つの物理的に妥当なビデオ生成ベンチマークで評価したところ、フレームワークは一貫して優れたパフォーマンスを発揮しました。PhyGenBenchでは、最良のT2Vメソッドを平均15.3%、最良のI2Vメソッドを11.1%上回りました。Physics-IQベンチマークでは、固体力学で22.2%、流体力学で9.2%の改善を達成しました。
*   **効果的なVLMプランニング:** VLMにおけるChain-of-Thought (CoT) と物理認識推論のアプローチは、ビデオ生成の品質と物理的な妥当性の両方を効果的に改善しました。VLMプランナーは、粗いモーション軌道を予測し、現実世界の物理ダイナミクスに近似させることができました。
*   **VDMの柔軟性の向上:** 拡散モデルにノイズの多い軌道に対するロバスト性を持たせるために、潜在空間にランダムノイズを注入することで、VDMが粗いモーション軌道から逸脱して高品質の詳細レベルの動きを生成できるようにしました。
*   **包括的な評価:** 広範な実験とユーザースタディを実施し、物理的に妥当なビデオ生成におけるフレームワークの有効性と一般化能力を実証しました。ユーザースタディの結果は、競合他社からのビデオよりも、フレームワークによって生成されたビデオに対する強い好みを示しました。

## 4. Limitationや問題点は何か。本文で言及されているものの他、あなたが考えるものも含めて

この論文で提案されたフレームワークは、物理的に妥当なビデオ生成において大きな進歩を遂げましたが、いくつかの制限と問題点が残っています。

**論文で言及されている制限:**

*   **画像空間バウンディングボックス軌道で表現できない物理イベントのモデリング:** 固体破砕や気体凝固など、オブジェクトの固有の状態変化を伴う現象はモデル化できません。例えば、ガラスが割れるような現象は、バウンディングボックスの軌跡だけでは十分に表現できません。
*   **3D空間認識の欠如:** フレームワークは3D空間認識に欠けており、シーン内の空間関係を理解することができません。これにより、奥行きや遮蔽の表現が不正確になる可能性があります。
*   **小さなオブジェクトのオプティカルフローのノイズ干渉:** 小さなオブジェクトのオプティカルフローはノイズ干渉を受けやすく、フレームワークが曖昧なコンテンツを生成する原因となります。遠くのオブジェクトや、動きの速いオブジェクトは、オプティカルフローの精度が低下し、生成されるビデオにアーティファクトが発生する可能性があります。
*   **ベースモデルの制約:** フレームワークのパフォーマンスは、ベースとなるVDMの能力に制約されます。より高性能なVDMを使用することで、フレームワークの性能をさらに向上させることができます。

**私が考える問題点:**

*   **VLMの誤認識とCoTの限界:** VLMは、画像とテキストプロンプトの解釈において誤りを犯す可能性があり、不正確なモーションプランニングにつながる可能性があります。CoT推論も、VLMが物理法則を完全に理解しているわけではないため、常に正しい結果を保証するものではありません。
*   **特定の物理現象への偏り:** ベンチマークで使用されている物理現象の種類が限られているため、フレームワークがすべての種類の物理現象に対して同じようにうまく機能するかどうかは不明です。
*   **計算コスト:** VLMとVDMの両方を使用するため、フレームワークの計算コストは高くなる可能性があります。特に、高解像度のビデオを生成する場合は、計算リソースが必要になります。
*   **汎用性の欠如:** フレームワークは、特定の種類のビデオ（例えば、人工的な環境でのオブジェクトの動き）に対して最適化されている可能性があり、現実世界のより複雑なシナリオではうまく機能しない可能性があります。
*   **評価の難しさ:** 物理的な妥当性を客観的に評価することは依然として困難であり、既存のベンチマークとメトリックは完璧ではありません。生成されたビデオが実際に物理法則に従っているかどうかを判断するには、人間の判断が必要になる場合があります。

## 5. 技術的な詳細について。技術者が読むことを想定したトーンで

このセクションでは、提案されたフレームワークの技術的な詳細について詳しく説明します。

**アーキテクチャ:**

このフレームワークは、VLMベースのモーションプランナーとVDMベースのモーションシンセサイザーから構成される二段階のパイプラインです。VLMは、GPT-4oとGrounded-SAMを組み合わせて使用​​しており、VDMは、Go-with-the-FlowをベースにファインチューニングされたCogVideoXを使用しています。

**VLMモーションプランニング:**

1.  **オブジェクト検出とセグメンテーション:**
    *   GPT-4oを使用して、テキスト記述に基づいて物理現象に関与する可能性のあるオブジェクトを認識します。
    *   Grounded-SAMを使用して、これらのオブジェクトを検出し、セグメンテーションを行い、バウンディングボックスを取得します。
2.  **物理法則の決定:**
    *   LLMの事前学習された知識を利用して、現在のシーンに適用可能な物理法則を決定します。
    *   一般的な物理現象は、重力、運動量保存、光学、熱力学、磁気、流体力学の6つのカテゴリに分類されます。
    *   VLMに物理コンテキスト情報を提供して、物理法則の理解を深めます。
3.  **モーション軌道の予測:**
    *   VLMに、画像空間内のオブジェクトの将来のバウンディングボックスの位置を予測するように指示します。
    *   バウンディングボックスは、\[x, y, w, h]の形式で表現され、xとyは左上隅の座標、wとhは幅と高さを表します。
    *   Chain-of-Thought (CoT)推論を使用して、VLMの推論能力を強化します。
    *   CoTは、物理現象の分析を段階的な推論として構成します。
        *   VLMは、ビデオキャプションを分析し、物理法則を詳細に説明します。
        *   VLMは、シーン内の各オブジェクトの潜在的な相互作用と動きを分析します。
        *   VLMは、各オブジェクトに対応するバウンディングボックスの位置と形状の詳細な変化を時間経過とともに予測します。
    *   VLMは、次の12フレームのオブジェクトバウンディングボックスの変化を推論します。
    *   選択されたVDMの生成プロセスとの互換性を保つために、これらの推論された12フレームは、線形補間によって合計49フレームに拡張されます。

**VDMモーション合成:**

1.  **構造化ノイズの生成:**
    *   VLMによって計画されたモーション軌道を使用して、合成モーションシーケンスをアニメーション化し、対応するオプティカルフローを導出します。
    *   オブジェクト`o_i`のバウンディングボックスを最初のフレームから抽出し、モーション軌道で指定されたバウンディングボックスの位置`b_i^t`に移動します。
    *   オブジェクト`o_i`の形状の変化をアニメーション化するために、`o_i`のサイズを変更して`b_i^t`に合わせます。
    *   合成モーションビデオ`V_hat(t)`は、アニメーション関数を使用して生成されます。
    *   RAFTなどのオプティカルフロー推定器を使用して、合成ビデオからオプティカルフローを抽出します。
    *   オプティカルフローを構造化ノイズテンソル`Q`として定式化します。
2.  **ノイズ注入:**
    *   構造化ノイズ`Q_i`にノイズを注入することで、VDMが詳細レベルのモーションを自由に生成できるようにします。
    *   ノイズ注入は、次の式を使用して行われます。
        ```python
        def inject_noise(Q_i, gamma, zeta):
          return ((1 - gamma) * Q_i + zeta * gamma) / np.sqrt((1 - gamma)**2 + gamma**2)
        ```

**実装の詳細:**

*   VLMにはGPT-4oを使用。
*   セグメンテーションにはGrounded-SAMを使用。
*   VDMにはCogVideoXを使用し、Go-with-the-Flowをベースにファインチューニング。
*   オプティカルフロー推定にはRAFTを使用。

## 6. コストや物理的な詳細について。例えばトレーニングに使用したGPUの数や時間、データセット、モデルのサイズなど

論文に記載されている範囲でのコストおよび物理的な詳細を以下にまとめます。

*   **モデルサイズ:**
    *   CogVideoX は、20億パラメータと50億パラメータのバリアントがあります。実験ではどちらが使われたか明記されていません。
    *   GPT-4o のサイズは公開されていません。
*   **データセット:**
    *   VDM (CogVideoX) は大規模なビデオデータセットでトレーニングされていますが、具体的なデータセット名やサイズは論文に記載されていません。
    *   PhyGenBench は、160個のプロンプトからなる物理知識のベンチマークです。
    *   Physics-IQ は、66個の異なる物理シナリオにわたる396個の現実世界のビデオで構成されています。
*   **GPU:**
    *   論文にはGPUの種類や数が明記されていません。
*   **トレーニング時間:**
    *   論文にはトレーニング時間が明記されていません。

この論文ではトレーニングに関する詳細がほとんど記載されていません。より詳細な情報は、参照されているCogVideoXやGo-with-the-Flowの論文を参照する必要があります。

## 7. 参考文献のうち、特に参照すべきもの

この論文を理解するために特に参照すべき参考文献は以下の通りです。

*   **CogVideoX:** VDMのベースモデルとして使用されているため、アーキテクチャやトレーニング方法を理解する上で重要です。
*   **Go-with-the-Flow:** ファインチューニングされたCogVideoXのベースとなっているため、モーション制御に関する技術的な詳細を把握する上で役立ちます。
*   **GPT-4o, Grounded SAM:**  VLM部分のキーコンポーネントなので、VLMの動作を理解するために重要です。
*   **PhyGenBench および Physics-IQ:**  評価に使用されたベンチマークなので、タスクと評価メトリックを理解するために重要です。

## 8. この論文を140字以内のツイートで要約すると？

物理法則を理解しないVDMにVLMで物理的な推論を組み込み、#物理的に妥当 なビデオ生成を実現！二段階構成で、VLMが粗い動きを計画し、VDMが詳細を合成。既存手法を大幅に凌駕する性能を達成！ #VLM #VDM #AI


---


# VerifiAgent: a Unified Verification Agent in Language Model Reasoning

[View Paper](http://arxiv.org/abs/2504.00406v1)

## 1. 既存研究では何ができなかったのか

既存の研究は、大規模言語モデル(LLM)の推論能力検証において、以下の点で限界がありました。

*   **モデル固有性・ドメイン制限:** 既存の検証手法は、特定のモデルや特定の推論タスクに特化していることが多く、他のモデルやタスクへの適用が困難でした。例えば、数学の問題に特化した検証器は、論理的な推論や常識推論には利用できません。
*   **計算資源の浪費:** 既存手法は、計算資源を大量に消費していました。特に、複雑な推論を検証する場合、検証プロセス自体が重くなり、スケーラビリティに問題がありました。
*   **統一的な検証の欠如:** 様々な推論タスク（数学、論理、常識など）に対して、統一的に適用できる検証手法が存在しませんでした。タスクごとに異なる検証器を開発・運用する必要があり、非効率でした。
*   **フィードバックループの欠如:** 検証結果をモデルの推論プロセスにフィードバックし、精度を向上させる仕組みが十分に確立されていませんでした。

## 2. どのようなアプローチでそれを解決しようとしたか

VerifiAgentは、上記の問題を解決するために、以下の2つのレベルの検証を統合した統一的な検証エージェントとして設計されました。

1.  **メタ検証 (Meta-Verification):** モデルの応答の完全性と一貫性を評価します。これは、LLMの出力に対する健全性チェックとして機能します。
    * 例えば、回答に矛盾する情報が含まれていないか、前提条件が満たされているかなどを検証します。
    * Python風疑似コード:
    ```python
    def meta_verify(response):
      is_complete = check_completeness(response)
      is_consistent = check_consistency(response)
      return is_complete and is_consistent
    ```

2.  **ツールベースの適応型検証 (Tool-based Adaptive Verification):** 推論の種類（数学、論理、常識）に応じて、適切な検証ツールを自律的に選択します。これにより、効率的かつロバストな検証が可能になります。
    *   数学的推論の場合、数式計算ツールや定理証明器を利用します。
    *   論理的推論の場合、論理式チェッカーや真理値表を利用します。
    *   常識推論の場合、知識グラフや外部APIを利用します。
    * Python風疑似コード:
    ```python
    def adaptive_verify(reasoning_type, response):
      if reasoning_type == "mathematical":
        tool = select_math_tool() # 例: 電卓API
        result = tool.verify(response)
      elif reasoning_type == "logical":
        tool = select_logic_tool() # 例: 真理値表チェッカー
        result = tool.verify(response)
      elif reasoning_type == "commonsense":
        tool = select_commonsense_tool() # 例: 知識グラフAPI
        result = tool.verify(response)
      return result
    ```
    * 重要な点として、`select_xxx_tool()`は、その時点で利用可能なツールの中から、最も適切だと判断されるものを動的に選択する関数である。

さらに、VerifiAgentは検証結果をLLMにフィードバックし、推論精度を向上させる仕組みを備えています。

## 3. 結果、何が達成できたのか

実験結果から、VerifiAgentは以下の点で既存手法を上回る成果を達成しました。

*   **精度向上:** VerifiAgentは、既存の検証手法（演繹検証器、後方検証器など）と比較して、すべての推論タスクにおいて優れた性能を示しました。
*   **効率向上:** 検証結果からのフィードバックを活用することで、LLMの推論精度をさらに向上させることができました。
*   **推論のスケーリング:** VerifiAgentは、既存のプロセス報酬モデルと比較して、より少ないサンプル数とコストで、数学的推論の領域においてより良い結果を達成しました。 これは、推論のスケーリングにVerifiAgentが有効であることを示唆しています。

## 4. Limitationや問題点は何か

本文で言及されている制限事項は明確ではありませんが、以下のような制限事項や問題点が考えられます。

*   **ツールの可用性:** VerifiAgentの性能は、利用可能な検証ツールの質と可用性に大きく依存します。 特に、常識推論など、高度な知識を必要とするタスクでは、適切なツールの開発が課題となります。
*   **複雑な推論への対応:** VerifiAgentが複雑な推論タスクにどの程度対応できるかは不明です。 特に、複数の推論ステップが組み合わさった複雑な問題では、検証が困難になる可能性があります。
*   **バイアスの問題:** 検証ツール自体がバイアスを含んでいる場合、VerifiAgentの検証結果もバイアスを受ける可能性があります。
*   **計算コスト:** メタ検証と適応型検証の両方を実行するため、既存手法と比較して計算コストが高くなる可能性があります。 特に、大規模なLLMや複雑な推論タスクでは、計算資源の制約が問題となる可能性があります。
*   **汎用性の限界:** 現在の結果は数学的推論に偏っている可能性があるため、他の推論タイプに対する汎用性は不明です。
*   **敵対的攻撃への脆弱性:** LLMに対する敵対的攻撃と同様に、VerifiAgent自体も攻撃に対して脆弱である可能性があります。例えば、検証器を欺くように巧妙に作成された入力に対して、誤った検証結果を出力する可能性があります。
*   **説明可能性の欠如:** VerifiAgentがどのように検証ツールを選択し、結果を判断しているのかがブラックボックスである場合、その信頼性を評価することが困難になります。検証プロセスの透明性を高めることが重要です。

## 5. 技術的な詳細について

VerifiAgentは、モジュール化されたアーキテクチャを採用しており、以下の主要なコンポーネントで構成されています。

1.  **推論エンジン (Reasoning Engine):** LLM（例: GPT-3, LaMDA）が担当します。 推論タスクの入力に基づいて、推論結果を生成します。
2.  **メタ検証モジュール (Meta-Verification Module):** 推論エンジンの出力（応答）の完全性と一貫性を評価します。 以下のチェックを実行します。
    *   **完全性チェック:** 必要な情報がすべて含まれているかを確認します。
    *   **一貫性チェック:** 回答に矛盾する情報が含まれていないかを確認します。
    *   **前提条件チェック:** 推論に必要な前提条件が満たされているかを確認します。

    Python風疑似コード:

    ```python
    def check_completeness(response):
      # 必要な情報がすべて含まれているかを確認するロジック
      # 例：キーワードの存在確認、質問への回答の網羅性チェック
      keywords = ["答え", "根拠", "結論"]
      for keyword in keywords:
        if keyword not in response:
          return False
      return True

    def check_consistency(response):
      # 回答に矛盾する情報が含まれていないかを確認するロジック
      # 例：数値の矛盾、論理的な矛盾
      try:
        numbers = extract_numbers(response)
        if numbers[0] + numbers[1] != numbers[2]:
          return False
      except:
        pass # 数値が見つからない場合は矛盾チェックをスキップ
      return True
    ```

3.  **適応型検証モジュール (Adaptive Verification Module):** 推論の種類に応じて、適切な検証ツールを自律的に選択し、検証を実行します。
    *   **推論タイプ識別器:** 入力された推論タスクの種類（数学、論理、常識など）を識別します。 自然言語処理技術（例: テキスト分類、キーワード抽出）を利用します。
    *   **ツール選択モジュール:** 推論タイプに基づいて、最適な検証ツールを選択します。 知識ベース（推論タイプとツールの対応関係）を参照します。
    *   **検証ツールインターフェース:** 選択された検証ツールを呼び出し、推論結果を検証します。 各ツールのAPIに合わせて、入出力フォーマットを変換します。
    * Python風疑似コード:
      ```python
      def select_math_tool():
        # 利用可能な数学ツールのリストから最適なツールを選択
        # 例: Wolfram Alpha API, SymPyライブラリ
        return WolframAlphaAPI()

      def select_logic_tool():
        # 利用可能な論理ツールのリストから最適なツールを選択
        # 例: 真理値表チェッカー, 命題論理ソルバー
        return TruthTableChecker()

      def select_commonsense_tool():
        # 利用可能な常識ツールのリストから最適なツールを選択
        # 例: ConceptNet API, Wikidata API
        return ConceptNetAPI()

      class WolframAlphaAPI:
        def verify(self, response):
          # Wolfram Alpha APIを使って数式を検証する
          # (API呼び出しと結果の解析)
          pass

      class TruthTableChecker:
        def verify(self, response):
          # 真理値表チェッカーを使って論理式を検証する
          # (真理値表の生成と評価)
          pass

      class ConceptNetAPI:
        def verify(self, response):
          # ConceptNet APIを使って常識的な知識を検証する
          # (API呼び出しと結果の解析)
          pass
      ```

4.  **フィードバックモジュール (Feedback Module):** 検証結果を推論エンジンにフィードバックし、推論精度を向上させます。
    *   **報酬/罰則:** 検証結果に基づいて、推論エンジンに報酬または罰則を与えます。
    *   **知識の更新:** 検証結果から得られた知識を、推論エンジンの知識ベースに追加します。
    *   **パラメータ調整:** 検証結果に基づいて、推論エンジンのパラメータを調整します。

各モジュールは疎結合に設計されており、必要に応じて拡張や変更が可能です。 VerifiAgentは、Pythonなどの高レベル言語で実装されることが想定されます。

## 6. コストや物理的な詳細について

論文には、トレーニングに使用したGPUの数や時間、データセット、モデルのサイズなど、具体的なコストや物理的な詳細に関する記述はありません。 おそらく、実験は既存のLLM（例: GPT-3, LaMDA）をベースに行われており、VerifiAgent自体のトレーニングは比較的小規模なデータセットで行われたと考えられます。
詳細な情報がないため、推定に基づいて記述します。

*   **LLM:** ベースとなるLLMは、数十億から数千億のパラメータを持つ大規模なモデルである可能性があります。 これらは、大規模なテキストコーパスで事前にトレーニングされています。
*   **VerifiAgentのトレーニングデータ:** メタ検証モジュールおよび適応型検証モジュールのトレーニングには、数千から数百万の推論タスクとその正解データセットが使用された可能性があります。
*   **ハードウェア:** 実験には、高性能GPU（例: NVIDIA A100, V100）が複数台使用されたと考えられます。
*   **トレーニング時間:** LLMのファインチューニングおよびVerifiAgent自体のトレーニングには、数時間から数日かかる可能性があります。

将来の研究では、これらの詳細を明確にすることが望ましいです。

## 7. 参考文献のうち、特に参照すべきもの

abstractとtitleから参照すべき文献を特定することは困難ですが、もしVerifiAgentが特定の検証ツールやLLMを利用しているのであれば、それらに関する論文が重要になるでしょう。

例えば、

*   Wolfram Alpha APIを利用している場合、Wolfram Alphaに関する論文
*   特定のLLM（GPT-3など）を利用している場合、そのLLMに関する論文
*   特定の知識グラフ（ConceptNetなど）を利用している場合、その知識グラフに関する論文

などが挙げられます。 VerifiAgentの性能を理解するためには、これらの基盤となる技術に関する知識が不可欠です。

## 8. この論文を140字以内のツイートで要約すると？

LLMの推論精度を向上させる #VerifiAgent 登場！メタ検証とツール適応型検証で、数学・論理・常識推論を強化。既存手法より高精度＆効率的！ #AI #自然言語処理 [論文リンク]


---


# VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in One Step

[View Paper](http://arxiv.org/abs/2504.01956v2)

## 1. 既存研究では何ができなかったのか

既存研究は、スパースな視点からの3Dシーン復元において、以下の点で限界がありました。

*   **不十分な情報からの性能劣化:** 入力視点間のオーバーラップが少なく、視覚情報が不足している場合、性能が低下していました。
*   **遅い推論時間:** ビデオ拡散モデルは高品質なビデオを生成するために多数のノイズ除去ステップを必要とし、高速なフィードフォワードモデルと比較して効率が劣っていました。
*   **3D制約の欠如:** 2Dビデオデータで学習されたモデルは、RGB空間と時間的な一貫性に重点を置いており、安定した3Dジオメトリを考慮していませんでした。そのため、生成されたビデオは空間的な安定性や正確なカメラジオメトリを欠き、3Dアプリケーションには不向きでした。
*   **モーションの不要なバリエーション:** 静的なシーンでビデオを生成する際に、人物の動き、物体のインタラクション、環境の変化など、3Dの一貫性を損なう不要なバリエーションが発生することがありました。

## 2. どのようなアプローチでそれを解決しようとしたか

VideoSceneは、これらの課題を解決するために、以下の手法を取り入れました。

*   **3D-aware Leap Flow Distillation:** 時間のかかる冗長なノイズ除去段階をスキップするために、3D事前知識を活用した蒸留戦略を設計しました。具体的には、まず、高速なフィードフォワードSparse-view 3DGSモデルを使用して、粗い3D構造を生成し、初期の3D-awareビデオを生成して、後続の拡散ステップをガイドします。
*   **Dynamic Denoising Policy Network (DDPNet):** 推論時に最適なリープタイムステップを適応的に決定するために、動的なノイズ除去ポリシーネットワークを学習させました。これにより、ノイズの追加と情報の保持のバランスを取り、品質を損なうことなく効率を向上させました。
*   **Consistency Distillation:** 確率フロー常微分方程式（PF-ODE）に基づいて、ノイズからPF-ODE軌跡の初期点（クリーンな画像）への直接マッピングを学習するConsistency Modelを使用しました。これにより、マルチステップ反復サンプリングの利点を維持しつつ、ワンステップ画像生成が可能になります。
*   **Contextual Bandit Learning:** DDPNetでは、コンテキストバンディットアルゴリズムを導入し、入力ビデオの品質に基づいて最適なノイズレベル（タイムステップ）を選択するように学習させました。

## 3. 結果、何が達成できたのか

VideoSceneは、以下の成果を達成しました。

*   **高速な3Dシーン生成:** 既存のビデオ拡散モデルよりも高速に3Dシーンを生成できるようになりました。ワンステップ生成が可能となり、推論時間を大幅に短縮しました。
*   **優れた3Dシーン生成品質:** 既存手法を上回る3Dシーン生成結果を実現しました。生成されたビデオは、より高い視覚品質と空間的な一貫性を持ち、実際の3D構造と整合していました。
*   **クロスデータセットの汎化性能:** RealEstate10Kデータセットで学習したモデルを、ACIDデータセットなどの新しいシーンに適用した場合でも、良好な結果が得られました。
*   **幾何学的整合性の向上:** 生成されたフレーム間のカメラジオメトリの整合性が向上し、より正確な3D再構成が可能になりました。
*   **3D事前知識の活用:** 3D事前知識を活用することで、モデルがより効率的に学習し、高品質な3Dシーンを生成できるようになりました。

## 4. Limitationや問題点は何か

*   **計算リソースの消費:** VideoSceneの主な計算リソースの消費は、ビデオ拡散モデル自体に由来します。ビデオ拡散モデルを使用すること自体が、計算コストを必要とします。
*   **意味的なずれによる失敗:** 入力視点間の意味的なずれが大きい場合、生成が失敗することがあります。例えば、閉じたドアを通って部屋に入るような、現実にはありえないシーンを生成してしまうことがあります。
*   **Sparse-view 3DGSモデルの品質:** 3D-aware leap flow distillation戦略では、初期の3D構造を生成するためにSparse-view 3DGSモデルが使用されますが、このモデルの品質が低い場合、生成されるビデオの品質に影響を与える可能性があります。
*   **DDPNetの過学習:** DDPNetはパラメータ数が少ないため、過学習を防ぐために初期段階でのみ完全な学習に参加させました。過学習が起こると、ノイズレベルの選択が最適でなくなる可能性があります。
*   **3Dデータセットへの依存:** モデルの学習には3Dシーンデータセットが必要であり、データセットの品質が結果に影響を与える可能性があります。

## 5. 技術的な詳細について

VideoSceneは、以下の技術要素で構成されています。

1.  **Sparse-view 3DGS Model:** 2つの入力画像から高速に3D Gaussian Splatting (3DGS) モデルを生成します。論文ではMVSplatを使用しています。

2.  **Video Diffusion Model:** 事前学習済みのビデオ拡散モデル（論文ではCogVideoX-5B-I2Vを使用）をベースに、以下の処理を行います。

    *   **3D-aware Leap Flow Distillation:**
        *   Sparse-view 3DGSモデルによって生成された粗い3Dシーンからレンダリングされたビデオ \{I\_{\mathrm{Render}}\}\_{\tau=1}^{\mathcal{T}} をエンコードし、特徴量  \mathbf{x}\_{0}^{r} を得ます。
        *   ランダムに選択されたタイムステップ *t* でノイズを追加し、 \mathbf{x}\_{t}^{r} を生成します。
        *   損失関数 caligraphic\_L\_{D} を最小化するように、ビデオ拡散モデルの注意機構レイヤーを蒸留学習します。

        ```python
        def leap_flow_distillation(x_0_render, t):
          # x_0_render: エンコードされたレンダリングビデオ
          # t: ランダムに選択されたタイムステップ

          # ノイズを追加
          x_t_r = alpha_t * x_0_render + sigma_t * epsilon  # epsilon ~ N(0,I)

          # 教師モデルによる予測
          x_t_n = x_t_r + (t_n - t_n_plus_1) * Phi(x_t_r, t_n_plus_1, phi)

          # 学生モデルとEMA学生モデルを用いた損失計算
          loss = distance(f_theta(x_t_r, t_n_plus_1), f_theta_minus(x_t_n, t_n))

          return loss
        ```

    *   **Dynamic Denoising Policy Network (DDPNet):**

        *   コンテキストバンディットアルゴリズムを用いて、最適なノイズ除去タイムステップ *t* を動的に選択します。
        *   CNNベースのポリシーネットワーク italic\_π\_{\psi}(*t* | \mathbf{x}\_{0}^{r}; italic\_ψ)  は、入力ビデオの特徴 \mathbf{x}\_{0}^{r}  に基づいて、タイムステップ *t* を選択します。
        *   タイムステップ *t* の選択による即時報酬 *r* を、負の平均二乗誤差損失 caligraphic\_L\_{MSE}  として定義します。
        *   報酬を最大化するように、ポリシー勾配法を用いてポリシーネットワークのパラメータ ψ を更新します。

        ```python
        def dynamic_denoising_policy(x_0_render):
          # x_0_render: エンコードされたレンダリングビデオ

          # ポリシーネットワークによるタイムステップの選択
          t = policy_network(x_0_render)  # CNN

          # 選択されたタイムステップでノイズを追加
          x_t_r = alpha_t * x_0_render + sigma_t * epsilon

          # 学生モデルによる予測
          x_0_pred = (x_t_r - sigma_t * noise_predictor(x_t_r, t)) / alpha_t

          # MSE損失
          mse_loss = stopgrad(MSE(x_0_pred, x_0))

          # 報酬
          reward = -mse_loss

          # ポリシーネットワークの更新
          policy_loss = -reward

          return policy_loss
        ```

3.  **Consistency Model:**  確率フロー常微分方程式（PF-ODE）を解くことで、ノイズからクリーンな画像へのマッピングを学習します。これにより、ワンステップでの画像生成が可能になります。

## 6. コストや物理的な詳細について

*   **GPU:** 8 NVIDIA A100 (80G) GPUsを使用
*   **トレーニング時間:** 2日間
*   **データセット:** RealEstate10Kデータセットから49フレームをサンプリング (バッチサイズ2)
*   **学習率:** finetune attention layers で 1e-4, distillation training で 3e-5
*   **Optimizer:** AdamW
*   **モデルのサイズ:**
    *   事前学習済み CogVideoX-5B-I2V を使用
    *   DDPNetはCNNアーキテクチャを使用し、パラメータ数はビデオ拡散モデルより大幅に少ない
*   **推論時間:** 3秒以内 (3DGSレンダリング 0.5秒, 蒸留ビデオ生成 2.5秒)

## 7. 参考文献のうち、特に参照すべきもの

*   **3D Gaussian Splatting:** Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering.
    *   高速な3Dシーン表現手法として、3D Gaussian Splattingの基本的な理解が必要です。
*   **Consistency Models:** Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference.
    *   高速な推論を可能にするConsistency Modelの理論と実装について理解を深める必要があります。
*   **CogVideo:** Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers.
    *   ベースとなるビデオ拡散モデルのアーキテクチャとトレーニング方法について理解しておくと良いでしょう。
*   **MVSplat:** Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei Cai. Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images.
    * Sparse-viewからの3D Gaussian Splattingの実現方法について理解しておくと良いでしょう。

## 8. この論文を140字以内のツイートで要約すると？

スパースな視点から高速に3Dシーンを生成するVideoSceneを発表！3D事前知識と蒸留学習でビデオ拡散モデルを効率化。ワンステップで高品質な3Dビデオを生成し、3D-aware leap flow distillationとDDPNetで更なる高速化を実現！ #3D #拡散モデル #VideoScene


---


# AnimeGamer: Infinite Anime Life Simulation with Next Game State Prediction

[View Paper](http://arxiv.org/abs/2504.01014v1)

## 1. 既存研究では何ができなかったのか

既存研究、特に "Unbounded" という手法は、アニメキャラクターの無限ライフシミュレーションにおいて以下の点が不十分でした。

*   **歴史的な視覚的コンテキストの欠如:** マルチターンの対話から言語指示を生成する際に、過去のゲームの状態（画像）を考慮していなかったため、ゲームプレイに一貫性がありませんでした。例えば、キャラクターの位置や服装などが前の状態と矛盾する可能性がありました。
*   **静止画像の生成:** アニメのダイナミックな動きやインタラクションを表現できず、静止画のみを生成していたため、ゲーム体験として面白みに欠けました。キャラクターが全く動かない世界を想像してください。
*   **有限ゲームの制約:** 既存のゲームにおけるフレーム生成の研究は、マウスやキーボードなどの限定的な入力に基づき、あらかじめ定義された環境内での探索に制限されていました。

## 2. どのようなアプローチでそれを解決しようとしたか

AnimeGamerは、これらの問題を解決するために、以下の新しいアプローチを採用しました。

*   **マルチモーダル大規模言語モデル (MLLM) の活用:** MLLM を使用して、ゲームの状態（キャラクターの動きや状態の更新を含む）を生成します。過去のゲームの状態（アニメーションショット）と指示を考慮することで、一貫性を保ちます。
*   **アクション認識マルチモーダル表現の導入:** アニメーションショットを表現するために、キャラクターの動きを考慮した新しい表現方法を開発しました。これにより、動きのある高品質なビデオクリップを生成することが可能になりました。
*   **ビデオ拡散モデルの利用:** アクション認識マルチモーダル表現を、ビデオ拡散モデルで高品質なビデオクリップに変換します。
*   **自動データ収集パイプライン:** アニメ映画から必要なトレーニングデータを自動的に収集するパイプラインを構築しました。これにより、プレイヤーが好きなキャラクターでゲームを体験するためのモデルを、自分のデータで学習させることが可能になりました。
*   **スライディングウィンドウ技術:** 理論的に無限の生成を可能にするため、マルチモーダル生成にスライディングウィンドウ技術を採用しています。

## 3. 結果、何が達成できたのか

AnimeGamerは、既存の手法と比較して以下の点で優れた結果を達成しました。

*   **文脈の一貫性:** 過去のゲームの状態を考慮することで、キャラクターの行動や環境が前の状態と矛盾しない、一貫性のあるゲームプレイを実現しました。
*   **満足のいくダイナミクス:** キャラクターの動きやインタラクションをビデオとして生成することで、静止画だけでは実現できない、より魅力的なゲーム体験を提供しました。
*   **命令の遵守:** ユーザーの指示をより忠実に反映したゲームの状態を生成しました。
*   **総合的なゲーム体験の向上:** 自動評価指標と人間の評価の両方で、既存の手法を上回る評価を得ました。

## 4. Limitationや問題点は何か

論文で言及されているもの：

*   **クローズドドメインでの評価:** タスク設定が、カスタムキャラクターでモデルをトレーニングし、それらをクローズドドメインで評価することを重視しているため、オープンなドメインへの拡張に関する調査が不足しています。

私が考えるもの：

*   **計算コスト:** MLLMとビデオ拡散モデルを使用しているため、計算コストが高い可能性があります。特に、高品質なビデオをリアルタイムで生成するには、高性能なGPUが必要となるでしょう。
*   **制御の複雑さ:** アクション認識マルチモーダル表現やビデオ拡散モデルのパラメータ調整が難しく、意図した動きや表現を正確に制御するのが難しい可能性があります。
*   **汎用性の問題:** 特定のアニメスタイルやキャラクターに特化したモデルになる可能性があり、異なるスタイルやキャラクターへの適用が難しい場合があります。
*   **データセットの偏り:** トレーニングデータとして使用するアニメ映画の選択によって、生成されるゲームに偏りが生じる可能性があります。

## 5. 技術的な詳細について

AnimeGamerの中核となる技術要素は以下の通りです。

1.  **アニメーションショットのトークン化とデトークン化**
    *   **アニメーションショットエンコーダ (caligraphic\_E\_a):**
        *   最初のアニメクリップのフレームの visual features (f\_v) と、T5 で表現されるキャラクターの動作に焦点を当てた短い motion prompt (f\_md) を組み合わせた、アクション認識マルチモーダル表現 (s\_a) を作成します。
        *   visual features と textual features を連結する前に、多層パーセプトロン (MLP) とレイヤー正規化 (LN) を適用して、特徴量の次元とスケールを調整します。

            ```python
            def animation_shot_encoder(visual_features, motion_description):
                x = MLP(visual_features)
                x = LN(x)
                y = MLP(motion_description)
                y = LN(y)
                s_a = concat(x, y) # 沿ってトークン次元の連結
                return s_a
            ```

        *   光フローを使ってアクションの強度を測定し、それを motion scope (f\_ms) で表現します。これは動画内のキャラクターの動きの激しさを表します。
    *   **アニメーションショットデコーダ (caligraphic\_D\_a):**
        *   DiT (Diffusion Transformer) アーキテクチャをベースに、アニメーションショットエンコーダで生成されたアクション認識マルチモーダル表現から高品質な動画を生成します。
        *   motion scope (f\_ms) を timestep embedding に追加することで、動画の動きの強度を制御します。
        *   学習プロセスでは、エンコーダ、デコーダ、およびVAEを共同で訓練し、以下の損失関数を最小化します。

            ```python
            def loss_function(z_t, t, c, s_a, epsilon, epsilon_theta):
                loss = MSE(epsilon, epsilon_theta(z_t, t, c, s_a))
                return loss
            ```

            MSEは平均二乗誤差(Mean Squared Error)を表し、
            epsilonは標準ガウス分布からサンプリングされたランダムノイズを表し、
            epsilon\_thetaはノイズ除去を行うDiTモデルの出力を表します。

2.  **ゲーム状態の予測**
    *   **MLLM (Multimodal Large Language Model):**
        *   過去のゲームの状態（アニメーションショット表現）とユーザーからの指示を入力として、次のゲームの状態（アニメーションショット表現とキャラクターの状態）を予測します。
        *   N 個の学習可能なクエリを使用して、アニメーションショット表現を生成します。ここで、N は DiT モデルのトークン数と一致するように設定します。
        *   キャラクターの状態（スタミナ、社会性、娯楽）と motion scope は、離散的なターゲットとして扱われ、特別なトークンを使用して表現されます。
        *   損失関数は、クロスエントロピー損失 (キャラクターの状態とmotion scopeの予測) と MSE損失 (アニメーションショット表現の予測) の組み合わせです。

            ```python
            def overall_loss(L_CE, L_MSE, alpha):
                loss = L_CE + alpha * L_MSE
                return loss
            ```

            ここで L\_CE はクロスエントロピー損失、L\_MSE は平均二乗誤差、alpha は損失の重みを調整するハイパーパラメータです。
    *   **デコーダの適応:**
        *   MLLM の出力とデコーダの潜在空間のずれを軽減するために、デコーダのみをファインチューニングする適応学習を行います。

## 6. コストや物理的な詳細について

論文には、具体的なハードウェア構成やトレーニング時間に関する詳細な記述はありません。しかし、以下の推測が可能です。

*   **データセット:** 10個の人気アニメ映画から収集した約20,000個のビデオクリップ（各16フレーム、480x720解像度）を使用しています。
*   **モデルサイズ:** アニメーションショットデコーダは CogvideoX-2B をベースにしているため、約20億のパラメータを持つと考えられます。MLLM は Mistral-7B をベースにしているため、約70億のパラメータを持つと考えられます。
*   **GPU:** MLLMとビデオ拡散モデルのトレーニングには、複数の高性能GPUが必要と考えられます。少なくとも8枚以上のハイエンドGPU (NVIDIA A100など) を使用した可能性があります。
*   **トレーニング時間:** 大規模なデータセットとモデルを使用しているため、数日から数週間程度のトレーニング時間が必要だったと考えられます。

## 7. 参考文献のうち、特に参照すべきもの

*   **Li et al., Unbounded: A generative infinite game of character life simulation.** この論文は、AnimeGamerが解決しようとした問題点を明確に示しており、AnimeGamerのアプローチを理解する上で重要です。
*   **Zhu et al., Cogvideox: Text-to-video diffusion models with an expert transformer.** AnimeGamerのアニメーションショットデコーダのベースとなっているモデルであり、ビデオ生成技術の理解に役立ちます。
*   **Jiang et al., Exploring the frontiers of animation video generation in the sora era: Method, dataset and benchmark.** アニメーションビデオ生成に関する最新の研究動向を知る上で参考になります。

## 8. この論文を140字以内のツイートで要約すると？

アニメキャラ無限ライフ！AnimeGamerはMLLMで過去の状態を考慮し、動きのあるゲームを生成。アクション認識表現で動画品質UP！既存手法より一貫性があり、楽しいゲーム体験を実現！ #AnimeGamer #無限ライフ #ゲームAI


---


# DreamActor-M1: Holistic, Expressive and Robust Human Image Animation with Hybrid Guidance

[View Paper](http://arxiv.org/abs/2504.01724v2)

## 1. 既存研究では何ができなかったのか

既存の画像ベースの人物アニメーション手法は、写実的な身体や顔の動きを合成できるものの、以下の点で課題が残っていました。

*   **きめ細かい全体的な制御性の欠如:** 微妙な目のまばたきや唇の震えといった、細かな表情の制御が困難でした。
*   **マルチスケールへの適応性の不足:** ポートレートから全身まで、さまざまなスケールの入力に対応できませんでした。
*   **長期的な時間的コヒーレンスの欠如:** 特に、参照画像に写っていない領域（例えば、服の背面）において、長期的な一貫性を維持することが困難でした。
*   **表情の豊かさとロバスト性の不足:** 細かい表情や複雑な動きにおいて、表現力と安定性に課題がありました。
*   **顔と体の動きの全体的な制御が難しく、実世界のマルチスケール環境への対応が不足。**
*   **参照画像に写っていない領域において、長期的な時間的一貫性を維持するのが難しい。**

## 2. どのようなアプローチでそれを解決しようとしたか

DreamActor-M1 は、これらの課題を解決するために、以下のハイブリッドなガイダンスを用いる DiT (Diffusion Transformer) ベースのフレームワークを提案しました。

*   **ハイブリッド制御信号 (Motion Guidance):**
    *   **Implicit Facial Representations:** GANなどの従来手法のランドマークに頼らず、顔の表情の細かな制御のために、潜在空間で顔の表情を表現します。
    *   **3D Head Spheres:** 頭部のスケールと回転を制御するために、3D ヘッドスフィアを使用します。
    *   **3D Body Skeletons:** 体の動きと骨の長さを調整するために、3D ボディスケルトンを使用します。

    ```python
    # Hybrid Control Signal の疑似コード
    face_latent = face_motion_encoder(driving_video_faces) # Implicit Facial Representations
    head_sphere = render_head_sphere(3d_facial_parameters) # 3D Head Spheres
    body_skeleton = project_smplx_joints(smplx_parameters) # 3D Body Skeletons
    pose_feature = pose_encoder(concatenate([head_sphere, body_skeleton]))
    ```

*   **段階的な学習戦略 (Scale Adaptation):**
    *   ポートレート、上半身、全身など、解像度とスケールが異なるデータセットを使用して、段階的にモデルを学習させました。

*   **補完的な外観ガイダンス (Appearance Guidance):**
    *   ターゲットの動きから異なるポーズをサンプリングし、複数フレームの参照画像を生成して、見えない領域のテクスチャを提供します。これらの参照をビデオセグメント全体に伝播させることで、長期的な合成全体で一貫したディテールを維持します。参照画像が限られた情報しか持たない場合(例えば、顔の向きが大きく変わる、体の一部しか写っていないなど)に、このアプローチにより、時間的な一貫性を維持します。

    ```python
    # Complementary Appearance Guidance の疑似コード
    sampled_poses = sample_poses_from_driving_video(driving_video)
    multi_frame_references = generate_multi_frame_references(sampled_poses, reference_image)
    propagated_references = propagate_references_across_segments(multi_frame_references)
    ```

## 3. 結果、何が達成できたのか

実験により、DreamActor-M1 は既存の最先端手法を凌駕し、以下の点で優れた結果を示しました。

*   **表現力豊かなアニメーション:** ポートレート、上半身、全身のアニメーションにおいて、より豊かで自然な表現が可能になりました。
*   **ロバストな長期的な一貫性:** 長期間にわたって、一貫性のあるビデオを生成できるようになりました。
*   **きめ細かいモーション:** 微妙な表情や動きをより正確に制御できるようになりました。
*   **多様なキャラクタースタイル:** さまざまなキャラクター（アニメ調のキャラクターなど）のアニメーションにも対応できました。

## 4. Limitationや問題点は何か

論文で言及されている制限事項:

*   **動的なカメラワークの制御の難しさ:** カメラが大きく動くような映像の生成は苦手です。
*   **環境オブジェクトとの物理的な相互作用の欠如:** キャラクターが周囲の環境と自然にインタラクトするような映像を生成できません。
*   **骨の長さ調整の不安定性:** 事前学習済みの画像編集モデルを使用しているため、エッジケースにおいて骨の長さ調整が不安定になることがあります。

私が考える問題点:

*   **データセットへの依存:** 学習データセットの偏りによって、生成されるアニメーションの品質や多様性が制限される可能性があります。
*   **計算コスト:** DiT ベースのフレームワークであるため、計算コストが高く、リアルタイムでの動作が難しい場合があります。
*   **制御信号の設計の複雑さ:** ハイブリッド制御信号の設計は複雑であり、パラメータの調整が難しい場合があります。
*   **評価指標の限界:** 評価には一般的な指標（FID, SSIM, LPIPS, PSNR, FVD）を使用しているものの、主観的な品質や表現力を十分に評価できているとは限りません。
*   **汎用性の限界:** 学習データにない種類の動きやスタイルに対して、どの程度汎化できるかは不明です。

## 5. 技術的な詳細について

DreamActor-M1 は、以下の技術的な要素で構成されています。

1.  **DiT (Diffusion Transformer) バックボーン:** 画像/ビデオ生成のバックボーンとして、Diffusion Transformer を採用しています。DiT は、大規模なデータセットで学習することで、高品質な画像/ビデオを生成する能力を獲得しています。
2.  **3D VAE (Variational Autoencoder) を用いた潜在空間での学習:** 3D VAE を使用して、ビデオを潜在空間にエンコードし、DiT はこの潜在空間で学習を行います。これにより、計算コストを削減し、より効率的な学習を可能にします。
3.  **ハイブリッド制御信号:**
    *   **Implicit Facial Representations:** 事前学習済みの顔認識モデル（例：ArcFace）を使用して、顔の表情を潜在ベクトルとして抽出します。この潜在ベクトルは、DiT の Cross-Attention レイヤーを通して、ビデオの特徴と組み合わされます。これにより、顔の表情を細かく制御できます。
    *   **3D Head Spheres:** 頭部の姿勢（回転、位置）を 3D ヘッドスフィアとして表現します。3D ヘッドスフィアは、DiT の入力に直接結合され、頭部の動きを制御します。
    *   **3D Body Skeletons:** SMPL-X モデルから得られた 3D スケルトンを 2D 画像平面に投影し、DiT の入力に結合します。骨の長さは、参照画像と運転画像の間で正規化されます。

4.  **マルチリファレンス注入:** 学習時と推論時に、複数の参照画像を使用することで、隠れた領域の情報を提供し、長期的な一貫性を向上させます。
5.  **段階的な学習:** モデルを段階的に学習させることで、学習の安定性を向上させます。最初の段階では、ボディスケルトンとヘッドスフィアのみを使用して、基本的な人体アニメーションを学習します。次の段階では、顔の表情を制御するためのImplicit Facial Representationsを導入します。最後に、すべてのパラメータを微調整します。

**ネットワークアーキテクチャの詳細**

*   DiTの各ブロックにおいて、ReferenceNetのようにDiTのコピーをReferenceNetとして利用するのではなく、VAEを通して抽出されたI<sub>R</sub>とI<sub>VD</sub>を連結してDiTに入力します。
*   DiTブロック内では、連結されたトークンTに対し、まず最初の次元に沿ってself-attentionを実行します。
*   次に、Cross-Attentionを実行するために、それをT<sub>R</sub>とT<sub>D</sub>に分割します。

**骨の長さ調整の詳細**

1.  **A-poseへの変換:** まず、画像編集モデル（SeedEditなど）を使用して、参照画像と運転画像を標準化されたA-poseに変換します。

    ```python
    # A-poseへの変換の疑似コード
    reference_image_Apose = image_editing_model(reference_image, target_pose="A-pose")
    driving_image_Apose = image_editing_model(driving_image, target_pose="A-pose")
    ```

2.  **スケルトン推定:** 次に、RTMPose を使用して、A-poseの画像からスケルトンの関節位置を推定します。

    ```python
    # スケルトン推定の疑似コード
    reference_skeleton = rtmpose(reference_image_Apose)
    driving_skeleton = rtmpose(driving_image_Apose)
    ```

3.  **骨の長さの計算と正規化:** 参照スケルトンと運転スケルトンの骨の長さを計算し、運転スケルトンの骨の長さを参照スケルトンに一致するように正規化します。

    ```python
    # 骨の長さの計算と正規化の疑似コード
    def calculate_bone_lengths(skeleton):
        # 関節の位置から骨の長さを計算する
        bone_lengths = {}
        # 例：肩から肘の骨の長さを計算
        bone_lengths['shoulder_to_elbow'] = distance(skeleton['shoulder'], skeleton['elbow'])
        return bone_lengths

    reference_bone_lengths = calculate_bone_lengths(reference_skeleton)
    driving_bone_lengths = calculate_bone_lengths(driving_skeleton)

    def normalize_bone_lengths(driving_skeleton, reference_bone_lengths, driving_bone_lengths):
        # 運転スケルトンの骨の長さを参照スケルトンに合わせる
        normalized_skeleton = driving_skeleton.copy()
        for bone in reference_bone_lengths:
            scale_factor = reference_bone_lengths[bone] / driving_bone_lengths[bone]
            # 運転スケルトンの関節位置をスケーリング
            normalized_skeleton['elbow'] = driving_skeleton['shoulder'] + (driving_skeleton['elbow'] - driving_skeleton['shoulder']) * scale_factor
        return normalized_skeleton

    normalized_driving_skeleton = normalize_bone_lengths(driving_skeleton, reference_bone_lengths, driving_bone_lengths)
    ```

## 6. コストや物理的な詳細について

*   **GPU:** 8 x H20 GPUs
*   **学習ステップ:**
    *   第1段階: 20,000ステップ
    *   第2段階: 20,000ステップ
    *   第3段階: 30,000ステップ
*   **オプティマイザ:** AdamW (学習率 5e-6)
*   **ビデオクリップの長さ (学習時):** 25～121フレーム (ランダムに選択)
*   **空間解像度 (学習時):** 元のアスペクト比を維持しながら、サイズを調整
*   **ビデオセグメントの長さ (推論時):** 73フレーム
*   **Classifier-free guidance (cfg) パラメータ:**
    *   参照画像: 2.5
    *   モーション制御信号: 2.5
*   **データセット:**
    *   合計500時間の動画データ (ダンス、スポーツ、映画、スピーチなど)
    *   フルボディショットとハーフボディショットがそれぞれ約50%

## 7. 参考文献のうち、特に参照すべきもの

*   **Scaling rectified flow transformers for high-resolution image synthesis.**：DiTアーキテクチャのベースとなっている論文です。DiTの理解に不可欠です。
*   **High-resolution image synthesis with latent diffusion models.**：LDMの概念と実装を理解するために重要です。DreamActor-M1はLDMに基づいて構築されています。
*   **Expressive body capture: 3d hands, face, and body from a single image.**：SMPL-Xモデルの理解のために重要です。
*   **Rtmpose: Real-time multi-person pose estimation based on mmpose.**：姿勢推定に使用されているRTMPoseの理解のために重要です。
*   **Seededit: Align image re-generation to image editing.**：A-poseへの変換に使用されている画像編集モデルSeedEditの理解のために重要です。

## 8. この論文を140字以内のツイートで要約すると？

DreamActor-M1: 3D制御と潜在表現で、全身・表情アニメーションを高品質に生成！マルチスケール対応、長期一貫性も実現。DiTベースの革新的フレームワーク。 #画像アニメーション #拡散モデル #AI


---


# Medical large language models are easily distracted

[View Paper](http://arxiv.org/abs/2504.01201v1)

## 1. 既存研究では何ができなかったのか

既存研究では、大規模言語モデル (LLM) が、実際の臨床シナリオで遭遇するような、無関係な情報やノイズに対する脆弱性を十分に評価できていませんでした。特に、診察時の音声認識などによって生成された下書きメモのように、臨床現場で自然発生する可能性のある「ノイズ」が、LLM の性能に与える影響を定量的に評価した研究は不足していました。既存研究では、LLM が臨床的に重要な情報とそうでない情報を区別する能力、特に、無関係な情報が存在する場合の医学的な質問応答タスクにおける LLM のロバスト性を検証することができていませんでした。

## 2. どのようなアプローチでそれを解決しようとしたか

本研究では、以下の３つの主要なアプローチで、LLMの脆弱性を明らかにすることを試みました。

*   **MedDistractQA ベンチマークの開発:** USMLE (米国医師免許試験) スタイルの質問に、実際の臨床現場で起こりうる無関係な情報を組み込んだ MedDistractQA という新しいベンチマークデータセットを作成しました。このデータセットは、以下の2種類の「ノイズ」を埋め込むことで、LLM が臨床的に重要な情報とそうでない情報を区別できるかをテストします。

    *   **MedDistractQA-Nonliteral:** 医学用語を非臨床的な文脈で使用した、非文字通りの文を含んだ問題。例: 「患者の星座はがんです」(医学的な癌ではなく星座を指す)。
    *   **MedDistractQA-Bystander:** 患者の家族やペットなど、第三者の医学的詳細を含んだ問題。例: 「私の叔母の魚が心臓発作を起こしました」。

*   **実験による評価:** MedDistractQAを用いて、様々なLLM（商用モデル、オープンソースモデル、医学的にファインチューニングされたモデルなど）の性能を評価し、無関係な情報がLLMの精度に与える影響を定量的に測定しました。

*   **RAG (Retrieval-Augmented Generation) の影響:** RAGがLLMの精度向上に寄与するかどうかを検証しました。具体的には、医学教科書（Harrison's Principles of Internal Medicine）から取得した関連性の低い情報をRAGによってLLMに提供し、その影響を評価しました。

## 3. 結果、何が達成できたのか

本研究の結果、以下の点が明らかになりました。

*   **LLMは無関係な情報に弱い:** 無関係な情報 (MedDistractQA) の導入により、LLM の精度が最大 17.9% 低下しました。特にオープンソースモデルは、商用モデルよりも無関係な情報の影響を受けやすいことが示されました。
*   **医学的ファインチューニングの効果は限定的:** 医学データでファインチューニングしても、無関係な情報に対するロバスト性は改善されませんでした。一部のモデルでは、ファインチューニングによってかえって性能が低下する可能性も示唆されました。
*   **RAGは必ずしも有効ではない:** RAGを使用すると、関連性の低い情報がLLMに導入され、精度が低下する可能性があることが示されました。RAGは常に精度向上に繋がるわけではなく、LLMに悪影響を及ぼす可能性があることが示唆されました。
*   **大規模なモデルほど頑健:** ベースライン精度が高いモデルほど、無関係な情報に対するロバスト性が高い傾向がありました。

これらの結果から、LLM は臨床的に重要な情報とそうでない情報を区別するための論理的メカニズムを本質的に欠いており、実際の臨床応用には課題があることが示唆されました。

## 4. Limitationや問題点は何か。本文で言及されているものの他、あなたが考えるものも含めて

### 本文で言及されている Limitation

*   **データセットの偏り:** MedQA データセットは広く使用されているものの、臨床推論タスクの全範囲を網羅しているわけではありません。
*   **質問形式の限定:** 複数選択式の質問のみを使用しているため、自由形式の問題解決など、他の形式に一般化できない可能性があります。
*   **ノイズ生成のアルゴリズム依存:** 無関係な情報は特定の LLM (GPT-4o) を使用してアルゴリズム的に生成されたため、実際の臨床現場の複雑さや幅を完全に捉えられていない可能性があります。
*   **因果関係の不明確さ:** ベースライン精度と無関係な情報に対するロバスト性の間に強い相関関係が観察されましたが、因果関係を確立するには、さらなる制御された実験が必要です。

### その他考えられる Limitation

*   **評価指標の単純さ:** 精度のみを評価指標として使用しているため、LLM の判断の質や臨床的な妥当性を十分に評価できていない可能性があります。例えば、正答を導き出すまでの過程や根拠の妥当性、誤答の場合にどのような誤った推論を行ったかなどを分析する必要があります。
*   **ドメイン知識の偏り:** MedQA データセットは USMLE スタイルの質問に基づいており、特定の医学分野に偏っている可能性があります。より多様な臨床シナリオや患者背景を考慮した評価が必要です。
*   **倫理的な考慮の欠如:** 無関係な情報が LLM の判断に与える影響を評価する際に、倫理的な側面（例えば、患者のプライバシーやバイアス）を十分に考慮していません。無関係な情報が特定の患者グループに対する偏見を増幅させる可能性も考慮する必要があります。
*   **RAGにおける検索品質の評価:** RAGの実験において、検索されたテキストの関連性を評価する際に、単純なコサイン類似度のみを使用しています。より高度な検索品質の評価指標（例えば、専門家による判断や、LLM自身による関連性評価）を導入することで、RAGの効果をより正確に評価できる可能性があります。

## 5. 技術的な詳細について。技術者が読むことを想定したトーンで

本研究における技術的な詳細を以下に示します。

*   **データセット:** MedQA データセットに、GPT-4o を用いて生成した無関係な情報を注入しました。無関係な情報の生成には、各 MedQA 問題の不正解の選択肢から医学用語を抽出し、それらを非臨床的な文脈で使用するよう GPT-4o に指示しました。
*   **モデル評価:** 各 LLM に対して、MedQA, MedDistractQA-Nonliteral, MedDistractQA-Bystander, MedQA+RAG の4つのデータセットで評価を行いました。
    *   **推論設定:** 推論時のtemperatureは0に設定しました。ただし、OpenAIの推論モデルなど、temperature制御が不可能なモデルについてはデフォルト設定を使用しました。
    *   **ハードウェア:** MedMobile は A100 GPU で PyTorch と Transformers ライブラリを使用して実行しました。その他のオープンソースモデルおよび医学的にファインチューニングされたモデルは、A100 GPU で vLLM を使用して実行しました。商用モデルの推論は、各社の公式 API を使用して実行しました。
*   **RAG:** Harrison's Principles of Internal Medicine, 21st Edition から、質問とのコサイン類似度に基づいてテキストを検索しました。テキストのベクトル化には MedCPT ベクトルを使用し、最も類似度の高い段落をプロンプトに挿入しました。コサイン類似度の算出には、以下のPythonコードを参考にしてください。

```python
import numpy as np

def cosine_similarity(vec_a, vec_b):
    """Compute cosine similarity between two vectors."""
    dot_product = np.dot(vec_a, vec_b)
    magnitude_a = np.linalg.norm(vec_a)
    magnitude_b = np.linalg.norm(vec_b)
    return dot_product / (magnitude_a * magnitude_b) if (magnitude_a * magnitude_b) != 0 else 0

# Example usage (assuming vec_a and vec_b are numpy arrays)
# similarity = cosine_similarity(question_vector, paragraph_vector)
```
*   **統計分析:** ペアとなる精度測定（ベースラインと MedDistractQA/MedQA+RAG）に対して統計分析を実施しました。精度差の標準誤差は、二項分散成分から導出された式を用いて推定しました。グループ比較には Welch の t 検定を、ペアワイズ比較には 2 サンプル t 検定を使用しました。

## 6. コストや物理的な詳細について。例えばトレーニングに使用したGPUの数や時間、データセット、モデルのサイズなど

論文中には、具体的なGPUの数やトレーニング時間、モデルサイズなどの詳細なコスト情報は記載されていません。ただし、以下は推測できる情報です。

*   **GPU:** オープンソースモデルの推論には A100 GPU が使用されています。
*   **モデルサイズ:** 評価対象のモデルには、Llama-3-8B, Llama 3-70B, MedMobile など、様々なサイズのモデルが含まれています。
*   **データセットサイズ:** MedQA データセットと、それに追加された MedDistractQA データセットを使用しています。具体的なデータセットサイズは論文中に記載されていません。
*   **RAG のデータソース:** Harrison’s Principles of Internal Medicine, 21st Edition を RAG のデータソースとして使用しています。

これらの情報を元に、例えば、Llama 3-70B のような大規模モデルの評価には、相当な計算リソースと時間が必要であったと推測できます。また、GPT-4o を用いた無関係な情報の生成にも、相応のAPI利用コストが発生したと考えられます。

## 7. 参考文献のうち、特に参照すべきもの

*   **[19] Shi, F., Chen, X., Misra, K., Scales, N., Dohan, D., Chi, E. H., Schärli, N., & Zhou, D. (2023). Large language models can be easily distracted by irrelevant context. International Conference on Machine Learning.** この論文は、LLM が無関係な情報に弱いという本研究の主要な主張を裏付ける重要な先行研究です。
*   **[22] Jin, D., Pan, E., Oufattole, N., Weng, W. H., Fang, H., & Szolovits, P. (2021). What disease does this patient have? a large-scale open domain question answering dataset from medical exams.** MedQA データセットに関する論文であり、データセットの詳細やLLMの医療知識評価における重要性を理解する上で役立ちます。
*   **[24] Silverman, E., Crapo, J., Make, B., Jameson, J., Fauci, A., Kasper, D., Hauser, S., Longo, D., & Loscalzo, J. (2022). Harrison’s principles of internal medicine 21e.** RAGのデータソースである医学教科書であり、RAGの実験設定や医療知識の参照元を理解する上で重要です。

## 8. この論文を140字以内のツイートで要約すると？

医療LLMは、ノイズに弱い！MedDistractQAという新ベンチマークで、無関係な情報によりLLMの精度が大幅に低下することを確認。RAGも使い方によっては逆効果。医療AIの実用化には、ノイズ対策が不可欠！ #医療AI #LLM #自然言語処理


---


# Understanding R1-Zero-Like Training: A Critical Perspective

[View Paper](http://arxiv.org/abs/2503.20783v1)

## 1. 既存研究では何ができなかったのか

既存研究、特にDeepSeek-R1-Zeroの再現研究において、以下の点が十分に理解されていなかったり、考慮されていなかったりした。

*   **ベースモデルの特性の理解不足**: ベースモデル（DeepSeek-V3-BaseやQwen2.5など）が、RLによるファインチューニングなしでも既に高い性能を示す（"Aha moment"やテンプレートなしでの強力な推論能力など）理由が不明確だった。ベースモデルの事前学習データや学習方法が、RLの性能に与える影響が考慮されていなかった。
*   **GRPOにおける最適化バイアスの無視**: Group Relative Policy Optimization (GRPO) に内在する、応答長を不当に増加させるバイアスが認識されていなかった。特に、不正解の場合に長い応答が生成されやすいという問題が、モデルの真の推論能力の向上と区別されていなかった。
*   **自己反省行動（Aha moment）の誤解**: RLトレーニングによる自己反省行動の出現は、重要な成果とされていたが、ベースモデル自体が既に同様の行動を示すこと、また、自己反省行動が必ずしも高い精度に繋がらないことが見過ごされていた。
*   **テンプレートの影響**: 質問応答能力を引き出すために適切なテンプレートを使用する必要があるにもかかわらず、一部のモデル（Qwen2.5）ではテンプレートを使用しない方が良い性能が得られるという、テンプレートとモデルの相互作用が十分に理解されていなかった。
*   **オープンソース実装における長さバイアス**: PPOアルゴリズムのオープンソース実装において、応答長で損失を正規化する際に、意図しない長さバイアスが生じていることが認識されていなかった。

## 2. どのようなアプローチでそれを解決しようとしたか

上記の問題点を解決するために、本研究では以下のアプローチを採用した。

*   **ベースモデルの詳細な分析**: DeepSeek-V3-BaseやQwen2.5を含む様々なベースモデルの特性を調査し、事前学習データや学習方法がRL性能に及ぼす影響を分析した。特に、各モデルがテンプレートなしでどの程度質問に回答できるか、"Aha moment"をどの程度示すかを定量的に評価した。
*   **GRPOのバイアス除去**: GRPOに内在する最適化バイアスを特定し、その原因が応答長と標準偏差による正規化にあることを明らかにした。このバイアスを取り除くため、Dr. GRPOという新しい最適化手法を提案した。Dr. GRPOでは、応答長と標準偏差による正規化を削除することで、勾配が不当に操作されるのを防ぎ、より公平な学習を可能にする。
*   **Dr. GRPOの実証**: Dr. GRPOの有効性を実験的に検証するために、Qwen2.5-1.5Bベースモデルを用いてオンラインRLトレーニングを実施し、従来のGRPOと比較した。トレーニングのダイナミクス（報酬、応答長など）と、評価ベンチマークでの性能を比較し、Dr. GRPOがバイアスを軽減し、トークン効率を改善することを示した。
*   **テンプレートと質問セットの影響**: Qwen2.5-Mathモデルを用いて、異なるテンプレート（R1テンプレート、Qwen-Mathテンプレート、テンプレートなし）がRLトレーニングに与える影響を調査した。また、異なる難易度とカバー範囲を持つ質問セットを用いて、テンプレートと質問セットの相互作用を分析した。
*   **ドメイン固有の事前学習**: 数学推論能力が低いベースモデル（Llama-3.2-3B）に対して、ドメイン固有の事前学習（FineMathデータセットや連結された質問応答テキストによる事前学習）を実施し、RLの性能向上に対する効果を検証した。
*   **オープンソースPPO実装の調査**: PPOアルゴリズムのオープンソース実装を調査し、応答長による損失の正規化に起因する長さバイアスが存在することを確認した。

## 3. 結果、何が達成できたのか

本研究によって、以下の成果が達成された。

*   **R1-Zeroライクなトレーニングにおける重要な洞察**: ベースモデルの特性、GRPOのバイアス、テンプレートの影響など、R1-Zeroライクなトレーニングにおける重要な要素を明らかにし、それらがRLの性能に与える影響を詳細に分析した。
*   **Dr. GRPOの開発**: GRPOの最適化バイアスを効果的に除去するDr. GRPOを開発し、トークン効率を改善しつつ、推論性能を維持できることを実験的に示した。
*   **新たなState-of-the-Artの達成**: Dr. GRPOを用いてQwen2.5-Math-7BをRLファインチューニングすることで、AIME 2024において43.3%の精度を達成し、新たなState-of-the-Artを確立した。
*   **ベースモデルとテンプレートの相互作用の解明**: Qwen2.5-Mathモデルがテンプレートなしで高い性能を発揮する理由を明らかにし、ベースモデルの特性とテンプレートの適切な組み合わせが重要であることを示した。
*   **ドメイン固有の事前学習の有効性の確認**: 数学推論能力が低いベースモデルでも、ドメイン固有の事前学習によってRLの性能を大幅に向上できることを示した。
*   **長さバイアスの指摘と修正**: オープンソースPPO実装に存在する長さバイアスを指摘し、Dr. GRPOによってこのバイアスを修正できることを示した。
*   **R1-Zeroレシピの簡素化**: 7BベースモデルでState-of-the-Artを達成する、ミニマリストなR1-Zeroレシピを提示した。

## 4. Limitationや問題点は何か

本研究には、以下の Limitationや問題点が存在する。

*   **ベースモデルの選択**: 実験で使用したベースモデル（Qwen2.5、Llama-3.2など）に限定されており、他のモデルにも同様の傾向が当てはまるかどうかは不明である。
*   **質問セットの偏り**: MATHデータセットなどの特定の質問セットに焦点を当てており、他の種類の推論タスクやデータセットでの一般化可能性は検証されていない。
*   **報酬関数の単純さ**: 検証に基づく単純な報酬関数を使用しており、より複雑な報酬関数（例えば、プロセス報酬）を用いた場合の結果は異なる可能性がある。
*   **自己反省行動の評価**: 自己反省行動の検出には、キーワードベースの手法とLLMベースの手法を組み合わせているが、完璧な検出は難しく、誤検出のリスクが残る。また、自己反省行動が推論性能に与える影響についても、さらなる詳細な分析が必要である。
*   **計算リソース**: 大規模な実験には依然として多くの計算リソースが必要であり、全ての組み合わせを網羅的に検証することは難しい。
*   **言語モデルのバイアス**: 事前学習データに含まれるバイアスが、RLの結果に影響を与える可能性がある。特に、Qwen2.5-Mathモデルが連結された質問応答テキストで事前学習されている可能性があり、その影響は完全には解明されていない。

**その他考えられる問題点**

*   **タスクの限定性**: 数学の問題解決に特化しており、他の種類の推論や知識集約的なタスクへの応用可能性は不明確である。
*   **現実世界への適用**: 実験環境と現実世界とのギャップがあり、現実世界の複雑な問題に対する有効性は検証が必要である。
*   **解釈可能性の欠如**: モデルがなぜ特定の推論を行うのか、その内部メカニズムは依然としてブラックボックスであり、解釈可能性を高めるための研究が必要である。

## 5. 技術的な詳細について

本研究における技術的な詳細を以下に示す。

### 5.1 Dr. GRPOアルゴリズム

Dr. GRPOは、GRPO (Group Relative Policy Optimization) の最適化バイアスを修正したアルゴリズムである。GRPOの目的関数は以下の通り。

```
def grpo_loss(pi_theta, pi_theta_old, q, responses, rewards, epsilon):
    G = len(responses)
    loss = 0
    for i in range(G):
        response = responses[i]
        reward_i = rewards[i]
        advantage_i = (reward_i - mean(rewards)) / std(rewards) # GRPOのadvantage計算
        loss_i = 0
        for t in range(len(response)):
            o_t = response[t]
            ratio = pi_theta(o_t | q, response[:t]) / pi_theta_old(o_t | q, response[:t])
            clipped_ratio = clip(ratio, 1 - epsilon, 1 + epsilon)
            loss_i += min(ratio * advantage_i, clipped_ratio * advantage_i)
        loss += (1 / len(response)) * loss_i  # 応答長による正規化 (GRPOのバイアス)
    return (1 / G) * loss
```

GRPOにおける問題点は、`advantage_i`の計算における標準偏差による正規化と、`loss_i`の計算結果を応答長`len(response)`で正規化している点である。
これらの正規化項を取り除くことで、Dr. GRPOの目的関数は以下のようになる。

```
def dr_grpo_loss(pi_theta, pi_theta_old, q, responses, rewards, epsilon):
    G = len(responses)
    loss = 0
    for i in range(G):
        response = responses[i]
        reward_i = rewards[i]
        advantage_i = reward_i - mean(rewards)  # Dr. GRPOのadvantage計算 (標準偏差による正規化なし)
        loss_i = 0
        for t in range(len(response)):
            o_t = response[t]
            ratio = pi_theta(o_t | q, response[:t]) / pi_theta_old(o_t | q, response[:t])
            clipped_ratio = clip(ratio, 1 - epsilon, 1 + epsilon)
            loss_i += min(ratio * advantage_i, clipped_ratio * advantage_i)
        loss += loss_i # 応答長による正規化なし
    return (1 / G) * loss
```

また、PPOの実装における長さバイアスを修正するために、損失を応答長で正規化するのではなく、固定値（例えば、生成トークン数の最大値）で正規化する。

### 5.2 実験設定

*   **ベースモデル**: Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Llama-3.2-3Bなど
*   **テンプレート**: R1テンプレート, Qwen-Mathテンプレート, テンプレートなし
*   **質問セット**: MATH, AIME, GSM8Kなど
*   **報酬関数**: 検証に基づく二値報酬 (正解なら1, 不正解なら0)
*   **最適化アルゴリズム**: Dr. GRPO, GRPO
*   **実装フレームワーク**: Oat (LLM RLフレームワーク)
*   **評価指標**: AIME 2024の精度, Pass@8, 平均応答長

## 6. コストや物理的な詳細について

本研究で使用されたコストや物理的な詳細について、論文で言及されている範囲で以下に示す。

*   **GPU**: A100 GPUs
*   **データセット**:
    *   MATH (数学問題解決)
    *   NuminaMath-1.5 (競技数学問題)
    *   FineMath (数学テキスト)
    *   GSM8K (小学校レベルの数学問題)
*   **モデルサイズ**: 1.5B, 3B, 7BパラメータのLLM

具体的なGPUの数、トレーニング時間、ハイパーパラメータ等の詳細については、論文のセクションに記載されている。

## 7. 参考文献のうち、特に参照すべきもの

以下の参考文献は、本研究を理解する上で特に重要である。

*   **Guo et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.** DeepSeek-R1のオリジナル論文であり、本研究の出発点となっている。
*   **Liu et al. Oat: A research-friendly framework for llm online alignment.** 実験で使用したLLM RLフレームワークOatに関する論文。
*   **Shao et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models.** DeepSeekMathに関する論文であり、本研究のベースモデルの一つであるDeepSeek-V3-Baseを理解する上で参考になる。
*   **An et al. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement.** Qwen2.5-Mathに関するテクニカルレポートであり、本研究で使用したQwen2.5モデルの特性を理解する上で重要である。

また、GRPOに関する論文や、PPOアルゴリズムに関する論文も参照すると、より深く理解できる。

## 8. この論文を140字以内のツイートで要約すると？

R1-Zeroライクな学習を徹底分析！ベースモデルの特性、GRPOのバイアス、テンプレートの影響を解明。Dr.GRPOでSOTA達成🎉 #LLM #RL #DeepSeek #Qwen


---


# DASH: Detection and Assessment of Systematic Hallucinations of VLMs

[View Paper](http://arxiv.org/abs/2503.23573v1)

## 1. 既存研究では何ができなかったのか

既存研究は、Vision-Language Models (VLMs) の object hallucination（存在しないオブジェクトを誤って認識する現象）を、主に以下の点で十分に評価できていませんでした。

*   **オープンワールド設定への対応不足:** 既存のベンチマークは、MSCOCOなどの小規模でラベル付けされたデータセットに依存しており、実世界のオープンワールドな環境でVLMが広く使用される状況におけるhallucinationを十分に捉えられていませんでした。
*   **体系的なエラーの検出不足:** 既存研究では、VLMsがどのような種類の画像でhallucinationを起こしやすいのか体系的に評価されておらず、エラーが単なるランダムなものなのか、体系的な問題を示しているのか判断することが困難でした。
*   **モデル固有のhallucinationの特定不足:** 既存研究は、特定のVLMに固有のhallucinationを特定することに焦点を当てていませんでした。
*   **大規模な自動化パイプラインの欠如:** 大規模なデータセットを対象とした、自動化されたhallucination検出・評価パイプラインが存在しませんでした。人手によるラベル付けが必要な手法では、VLMsの評価・改善に必要な大規模なデータセットを作成できませんでした。

## 2. どのようなアプローチでそれを解決しようとしたか

論文では、上記の問題を解決するために、DASH (Detection and Assessment of Systematic Hallucinations) という自動化された大規模パイプラインを提案しました。DASHは、以下の主要なコンポーネントから構成されています。

*   **DASH-LLM (Language Model):** 大規模言語モデル(LLM)を使用して、VLMがhallucinationを起こしやすいテキストクエリを生成します。LLMには、「AIモデルが画像にオブジェクトが存在しなくても、スプリアスな特徴の存在によって誤ってオブジェクトを認識するように誘導できるプロンプトを作成する」ように指示します。
*   **DASH-OPT (Optimization):** 拡散モデルの潜在空間を最適化して、VLMがオブジェクトをhallucinationする画像を生成します。この最適化では、VLMがオブジェクトを認識する確率を最大化し、同時にオープンワールドオブジェクト検出器がオブジェクトを検出する確率を最小化するようにします。
*   **ReLAION-5Bを用いた検索:** DASH-LLMおよびDASH-OPTで生成されたテキストクエリおよび画像クエリを使用して、大規模な画像データセットReLAION-5Bから、VLMがhallucinationを起こす実世界の画像を検索します。CLIPの類似度を用いてkNN検索を行います。
*   **クラスタリング:** 検索された画像を、セマンティックに類似した画像のクラスターにまとめます。これにより、VLMの体系的なhallucinationを特定します。
*   **DASH-B ベンチマークの提案:** より信頼性の高い評価を可能にするために、新しいベンチマーク DASH-B を提案します。これは DASH で発見された画像で構成されています。

## 3. 結果、何が達成できたのか

DASHを用いて、PaliGemmaおよび2つのLLaVA-NeXTモデルに対して、380のオブジェクトクラスにわたる実験を行いました。その結果、合計で95万枚の画像を含む1万9千以上のクラスターを特定しました。

*   **体系的なhallucinationの特定:** DASHは、VLMsの体系的なhallucinationを大規模に特定できることを示しました。
*   **hallucinationの転移性の検証:** 特定されたhallucinationが、他のVLMsにも転移することを示しました。これは、hallucinationが特定のモデルに固有のものではなく、より一般的な問題であることを示唆しています。
*   **hallucinationの軽減:** DASHで特定されたhallucinationを用いてPaliGemmaをファインチューニングすることで、hallucinationを軽減できることを示しました。
*   **DASH-B ベンチマークの提案:** 既存の POPE ベンチマークが飽和していることを示し、DASH によって生成された画像に基づいて、より厳密な評価のための新しいベンチマーク DASH-B を提案しました。

## 4. Limitationや問題点は何か

論文で言及されている制限事項：

*   **完全なカバレッジの困難性:** DASHでも、すべての体系的なhallucinationを網羅的に検出することは不可能であると述べています。これは、完全に網羅的なアプローチが必要となるためです。
*   **画像データの偏り:** ReLAION-5Bは大規模なデータセットですが、一部の画像は十分に表現されていない可能性があります。その結果、VLMがhallucinationを起こす画像を特定できても、セマンティックに類似した画像が十分に存在しない場合、体系的なhallucinationと見なせないことがあります。
*   **オブジェクト検出器の限界:** 最も高度なVLMsに対しては、オブジェクト検出器の閾値設定が厳しすぎる可能性があると述べています。
*   **評価におけるラベルノイズ:** POPEベンチマークでの評価において、COCOアノテーションの誤りが多く、ラベルノイズが存在することが指摘されています。

その他考えられる制限事項：

*   **LLMのプロンプト依存性:** DASH-LLMにおけるテキストクエリの生成は、LLMに与えるプロンプトに依存します。プロンプトの設計が不適切な場合、効果的なクエリを生成できない可能性があります。
*   **計算コスト:** DASH-OPTは、拡散モデルの最適化を行うため、計算コストが高い可能性があります。特に、大規模なデータセットに対して適用する場合、計算資源がボトルネックになる可能性があります。
*   **ReLAION-5Bへの依存:** DASHは、ReLAION-5Bに依存しています。ReLAION-5Bの質や偏りが、DASHの性能に影響を与える可能性があります。
*   **ファインチューニングの一般化性能:** DASHで特定されたhallucinationを用いてファインチューニングを行いましたが、その効果は特定のタスクに限られる可能性があります。より一般的なタスクに対する効果を検証する必要があります。

## 5. 技術的な詳細について

DASHの技術的な詳細について、技術者向けに解説します。

*   **DASH-LLM:** Llama 3.1-70B を使用して、各オブジェクトに対して50個のテキストプロンプトを生成します。プロンプトは、オブジェクトそのものやその一部に言及せず、クエリが繰り返されないように指示されます。 chain-of-thought prompting によって、プロンプトを改善します。
*   **DASH-OPT:** SDXLの蒸留版である single-step SDXL U-Net と Latent Consistency Model (LCM) スケジューラを使用します。潜在空間の最適化にはAdam optimizer を使用し、最初の3ステップでlinear warmupを適用し、勾配をL2ノルム0.1にクリップします。最適化対象は、拡散プロセスの Gaussian random latent、SDXLのCLIPテキストエンコーダによるテキストエンコーディングです。 random latent には chi-square latent regularization を適用します。loss関数は、VLMのnext-token predictionに対するクロスエントロピー損失と、オブジェクト検出器の信頼度に対する損失の組み合わせです。

    ```python
    def loss_vlm(conditioning):
        # VLMに"Yes"と答えさせるための損失
        query = "Can you see a OBJ in this image?"
        p_yes = vlm.predict_proba("Yes", query, conditioning)
        return -torch.log(p_yes)

    def loss_det(conditioning):
        # オブジェクト検出器がOBJを検出しないようにするための損失
        p_obj = object_detector.predict_proba("OBJ", conditioning)
        return -torch.log(1 - p_obj)

    def total_loss(conditioning):
        return loss_vlm(conditioning) + loss_det(conditioning)

    # Adam Optimizerでconditioningを最適化
    optimizer = Adam(params, lr=learning_rate)

    for step in range(num_steps):
        loss = total_loss(conditioning)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(params, clip_value) # 勾配クリッピング
        optimizer.step()
        optimizer.zero_grad()
    ```

*   **kNN検索:** ReLAION-5B に対して、OpenCLIP ViT-H を使用した高速なkNNインデックスを使用します。
*   **クラスタリング:** 探索フェーズで同じ画像に対して検索されたすべての画像を、まずプレクラスタにグループ化します。その後、DreamSim距離に基づいて平均連結を用いた凝集型クラスタリングを使用して、これらのプレクラスタをマージして最終的なクラスタを形成します。DreamSim距離のマージ閾値は0.2です。

## 6. コストや物理的な詳細について

論文中に記載されているコストと物理的な詳細：

*   **計算リソース:** DASH-OPTの最適化は、NVIDIA A100 GPU（80GBメモリ）で、PaliGemmaに対して約50秒、LLaVA-NeXTモデルに対して約1分かかります。
*   **データセット:** 大規模画像データセットとしてReLAION-5Bを使用します。
*   **オブジェクトカテゴリ:** 実験には、COCOの80クラス、Objects365からランダムに選択された100クラス、Spurious ImageNetで使用されたImageNetの100クラス、OpenImagesのオブジェクトに基づいて作成された4つのサブセット（各25オブジェクト）を含む、合計380のオブジェクトカテゴリを使用します。

論文中に明示的な記載はありませんが、その他考慮すべき点：

*   **モデルサイズ:** 実験には、PaliGemma（3B）、LLaVA-NeXT（各種サイズ）、Qwen2-VL（各種サイズ）など、様々なサイズのVLMsが使用されています。
*   **ファインチューニング:** ファインチューニングには、LoRAが使用されています。

## 7. 参考文献のうち、特に参照すべきもの

特に参照すべき参考文献は以下の通りです。

*   **Releasing RE-LAION 5B:** 透明性のある反復によって LAION-5B の安全性を向上させています。大規模データセットの構築に関する重要な情報が含まれています。
*   **Latent consistency models: Synthesizing high-resolution images with few-step inference.** 拡散モデルの効率的な蒸留に関する重要な技術が説明されています。
*   **Scaling open-vocabulary object detection.** オープン語彙オブジェクト検出器に関する重要な情報が含まれています。
*   **LLaVA-NeXT:** 既存のLLaVAを大幅に改良したVision Language Modelで、DASHによる評価対象となっています。

## 8. この論文を140字以内のツイートで要約すると？

VLMsの体系的なhallucinationを大規模に検出するDASHを提案！既存研究では捉えきれないオープンワールドなhallucinationを自動で発見。DASH-OPTによる画像生成とReLAION-5Bの検索で、PaliGemma等の弱点を特定し改善。#VLM #hallucination #AI
