
# Deciphering language processing in the human brain through LLM representations

[View on Google Research Blog](https://research.google/blog/deciphering-language-processing-in-the-human-brain-through-llm-representations/)

Google Researchの研究者たちが、大規模言語モデル（LLM）と人間の脳が自然言語をどのように処理するかを比較した研究について解説する。LLMは、次に来る単語を予測し、文脈に応じた埋め込みを使用することで、人間とほぼ同じレベルで自然言語を処理できる。この研究では、日常会話を処理する際、人間の脳の神経活動が、LLMの内部的な文脈埋め込みと線形に一致することが示された。

研究チームは、過去5年間にわたり、特定の深層学習モデルの内部表現（埋め込み）と、自然な会話中の人間の脳の神経活動の類似性を調査し、LLMの埋め込みが人間の脳の言語処理を理解するためのフレームワークとして機能することを示してきた。具体的には、Transformerベースの音声テキスト変換モデルの内部表現と、実際の会話中の人間の脳の神経処理シーケンスとの間の整合性を調査した。

この研究では、自然な会話中に頭蓋内電極を使用して記録された神経活動を分析し、神経活動のパターンを、Whisper音声テキスト変換モデルによって生成された埋め込みと比較した。その結果、単語を聞いたり話したりする際に、音声テキスト変換モデルから抽出された音声埋め込みと単語ベースの言語埋め込みが、脳の音声理解および生成に関連する領域の神経活動パターンと驚くほど一致することがわかった。

さらに、LLMと人間の脳の類似点として、LLMがシーケンス内の次の単語を予測するように学習されるのに対し、人間の脳の言語領域も同様に、単語が発せられる前に次の単語を予測しようとすることが発見された。また、LLMの埋め込み空間における単語間の関係が、脳の言語領域における脳埋め込みによって誘導される表現の構造と一致することも発見された。

一方で、LLMと人間の脳の自然言語処理には違いもある。例えば、Transformerアーキテクチャが多数の単語を同時に処理するのに対し、脳の言語領域は、単語ごとに連続的、反復的、時間的に言語を分析するように見える。

これらの発見は、深層学習モデルが、統計学習、盲目的最適化、そして自然への直接的な適合という原理に基づいて、自然言語を処理するための脳の神経コードを理解するための新しい計算フレームワークを提供する可能性を示唆している。今後は、人間の経験により良く合致するニューラルアーキテクチャ、学習プロトコル、およびトレーニングデータを取り入れることで、情報処理能力と現実世界での機能が向上した、生物学的にインスパイアされた革新的な人工ニューラルネットワークを開発することを目指している。

