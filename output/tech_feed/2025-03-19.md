
# Generating synthetic data with differentially private LLM inference

[View on Google Research Blog](https://research.google/blog/generating-synthetic-data-with-differentially-private-llm-inference/)

この記事では、差分プライバシー（DP）を保証しつつ、大規模言語モデル（LLM）を用いて合成データを生成する新しい推論ベースの手法が紹介されています。この手法は、機密データを含む多数のプロンプトをLLMに並行して与え、その予測をプライバシー保護された方法で集約することで実現されます。

DP合成データは、モデル開発チームがDPの知識なしに共同作業できるインターフェースとして機能し、大規模MLにおけるスケーリング問題を解決します。従来、DP合成データの生成にはLLMのプライベートなファインチューニングが必要でしたが、コストが高く、データ要件も大きいという課題がありました。そこで、本研究では、モデル自体ではなく、モデルの出力のみにDPを適用するDP予測に着目しています。

この手法では、LLMに機密データの断片を含む複数のプロンプトを与え、各プロンプトからの予測を集約し、DPを適用して次のトークンを生成します。これにより、選択されたトークンが特定の機密データに過度に影響されないようにします。

この手法は、プライバシー予算と計算効率に関する課題を解決することで、高品質な合成データを大量に生成することを可能にします。具体的には、次のトークンのサンプリングにおけるランダム性を活用してプライバシーを確保し、言語モデルのソフトマックスサンプリングとDP技術である指数メカニズムとの関連性に基づいて、新しいDPトークンサンプリングアルゴリズムを設計します。また、各生成ステップで同じコンテキストを使用し、再計算を避けるための新しいプライバシー分析を提案し、KVキャッシングなどの標準的な推論効率化技術との互換性を実現しています。さらに、公開ドラフターを導入し、機密データではなく、すでに生成された合成テキストのみに基づいて次のトークンを予測することで、プライバシーコストを削減しています。

実験では、ベンチマークMLデータセットを機密データの代わりとして使用し、Gemmaモデルを用いて合成データセットを生成しました。そして、生成された合成データが、GPT-3を用いたインコンテキスト学習や、BERTモデルのファインチューニングといった、下流タスクにどれだけ有用であるかを評価しました。その結果、提案手法により、より多くの高品質な合成データを生成できるようになったため、インコンテキスト学習の精度が向上し、特にデータが限られた状況下では、DP推論がDPファインチューニングを上回るという結果が得られました。

今後の展望として、DP推論のさらなる応用と、生成される合成データの品質と量の改善が挙げられています。

