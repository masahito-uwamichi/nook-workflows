
# ECLeKTic: A novel benchmark for evaluating cross-lingual knowledge transfer in LLMs

[View on Google Research Blog](https://research.google/blog/eclektic-a-novel-benchmark-for-evaluating-cross-lingual-knowledge-transfer-in-llms/)

ECLeKTicは、大規模言語モデル(LLM)の多言語知識転移能力を評価するための新しいベンチマークです。LLMは特定の言語でのみ学習した知識を、他の言語でも利用できるようにする必要がありますが、現状のLLMは言語によって知識へのアクセスに差があります。ECLeKTicは、Wikipediaのある言語版にしか存在しない記事に基づいた質問応答タスクを通じて、この問題を評価します。12言語で質問が用意されており、モデルが元の言語で正しく答えられた質問に対して、別の言語でも正しく答えられるかを評価します。評価の結果、Gemini 2.5 Proが最高の性能を示しましたが、依然として改善の余地があることが示されました。また、知識転移の度合いは言語ペアによって異なり、文字体系が共有されている言語間での転移がよりスムーズであることがわかりました。ECLeKTicの公開により、LLM開発者がモデルの多言語知識転移能力を評価し、改善するためのツールを提供することを目指しています。

---

# Taking a responsible path to AGI

[View on DeepMind Blog](https://deepmind.google/discover/blog/taking-a-responsible-path-to-agi/)

Google DeepMindは、AGI（汎用人工知能）の開発において、準備、リスク評価、AIコミュニティとの連携を重視し、責任あるアプローチを取っている。AGIは、医療診断の迅速化、教育の個別化、イノベーションの促進など、社会に多大な利益をもたらす可能性がある。

AGIの潜在的なリスクを軽減するため、同社は「Levels of AGI」フレームワークを導入し、AGIの安全性とセキュリティに関する新たな論文を発表。この論文では、AGIの進捗を監視し、安全かつ責任ある開発を確保するための取り組みを詳細に説明している。

主なリスク領域として、以下の4つを挙げている。

1.  **Misuse (悪用):** AIシステムが意図的に有害な目的で使用されること。これには、有害コンテンツの生成や不正確な情報の拡散などが含まれる。対策として、危険な能力へのアクセス制限、セキュリティメカニズムの強化、脅威モデリング研究などが挙げられる。

2.  **Misalignment (アラインメントのずれ):** AIシステムが人間の意図とは異なる目標を追求すること。仕様の誤りや目標の誤った一般化などが原因となる。対策として、AIシステムが倫理的に問題のある近道を使用しないように、人間の価値観と整合させるための研究が行われている。

3.  **Accidents (事故):** 意図しない結果を引き起こす可能性のあるAIシステムの動作。

4.  **Structural Risks (構造的リスク):** AI技術の広範な展開によって引き起こされる可能性のある社会構造の変化。

アラインメントのずれを防ぐために、AIシステムが人間の指示に正確に従うように訓練し、目標達成のために倫理的に問題のあるショートカットを使用しないようにする。また、AIシステムの意思決定の透明性を高めるために、解釈可能性に関する研究も行われている。

AGIの安全な開発を促進するために、AGI Safety Council (ASC) を設立し、リスク分析とベストプラクティスの推奨を行っている。さらに、外部の専門家、業界、政府、非営利団体、市民社会組織との連携を強化し、国際的な合意形成に貢献していく考えである。AI研究者や専門家向けにAGI安全に関するコースも開始。

最後に、AGIの責任ある開発を推進し、この技術の恩恵をすべての人々が享受できるよう、AI研究コミュニティとの協力を呼びかけている。


---

# Evaluating potential cybersecurity threats of advanced AI

[View on DeepMind Blog](https://deepmind.google/discover/blog/evaluating-potential-cybersecurity-threats-of-advanced-ai/)

Google DeepMindは、高度なAIがもたらす可能性のあるサイバーセキュリティ上の脅威を評価するための新しいフレームワークを開発しました。このフレームワークは、AIが悪用されてサイバー攻撃が強化されるリスクを理解し、軽減することを目的としています。

このフレームワークは、サイバー攻撃チェーンのすべての段階を網羅し、広範な脅威タイプに対応し、実際のデータに基づいています。これにより、サイバーセキュリティの専門家は、悪意のある攻撃者がAIを利用して高度なサイバー攻撃を実行する前に、必要な防御策を特定し、優先順位を付けることができます。

DeepMindは、MITRE ATT&CKなどの実績のあるサイバーセキュリティ評価フレームワークを参考に、AIによる攻撃を考慮した評価を可能にしました。Googleの脅威インテリジェンスグループからのデータに基づき、20か国で発生した12,000件以上のAIを使用したサイバー攻撃の試みを分析し、一般的な攻撃パターンを特定しました。そして、フィッシング、マルウェア、DoS攻撃など、7つの典型的な攻撃カテゴリを特定し、AIが攻撃コストを大幅に削減する可能性のある重要なボトルネック段階を特定しました。

さらに、サイバーセキュリティの強みと弱みを包括的に評価するための攻撃的なサイバー能力ベンチマークを作成しました。このベンチマークは、情報収集、脆弱性攻撃、マルウェア開発など、攻撃チェーン全体を網羅する50の課題で構成されています。

初期評価では、現時点のAIモデルが単独で脅威アクターに画期的な機能をもたらす可能性は低いものの、AIが高度化するにつれて、可能なサイバー攻撃の種類も進化し、防御戦略の継続的な改善が必要になることが示唆されています。

このフレームワークは、AIが攻撃の隠蔽や、侵害されたシステムへの長期的なアクセス維持といった、既存の評価で見落とされがちな側面を強化する可能性も指摘しています。

DeepMindは、このフレームワークとベンチマークが、サイバーセキュリティチームが防御を強化し、急速に進化する脅威に先んじるのに役立つと考えています。
