
# Evaluating progress of LLMs on scientific problem-solving

[View on Google Research Blog](https://research.google/blog/evaluating-progress-of-llms-on-scientific-problem-solving/)

Google Researchは、科学分野における大規模言語モデル(LLM)の進歩を評価するための新しいベンチマーク「CURIE」を発表した。科学研究の発展には、科学文献に蓄積された知識を基盤とし、深い専門知識と推論能力が必要となる。LLMは知識の表面化から問題解決へと移行しており、科学研究への応用において大きな可能性を秘めている。

CURIEは、LLMが科学タスクの複雑さを処理する能力を厳密に評価するために設計された。長文でコンテキストが豊富な科学情報を理解し、推論する能力、図表などのマルチモーダルコンテンツを理解する能力、そして問題を解決するための適切なツールを選択する推論プロセスを理解する能力を測定する。

CURIEに加え、NeurIPS 2024では、科学論文の図表に基づく質問応答能力を評価する「SPIQA」データセットを、MATH-AIワークショップでは、有限要素解析ソフトウェアを用いて物理、数学、工学の問題をシミュレーションする能力を評価する「FEABench」を発表した。

CURIEは、材料科学、凝縮物性物理学、量子コンピューティング、地理空間分析、生物多様性、タンパク質の6つの分野における科学的問題解決を評価する。ドメイン専門知識、長文コンテキスト情報の理解、多段階推論を必要とする10のタスクが含まれている。タスクは、情報抽出、推論、概念追跡、集計、代数操作、マルチモーダル理解、分野横断的な専門知識など、さまざまな科学ワークフローを網羅している。

CURIEのタスクは多様であり、JSON、LaTeX数式、YAMLファイル、自由形式テキストなど、混合された異質な形式で正解アノテーションがされている。自由形式の生成を評価するために、ROUGE-Lなどのプログラムによる評価指標に加え、LMScoreとLLMSimの2つのモデルベースの評価指標を提案する。

CURIEで一般的な長文コンテキストモデルを評価した結果、特に複数の値の網羅的な検索と集計を必要とするタスクにおいて、すべてのモデルとタスクに改善の余地があることがわかった。専門家は、モデルの応答が有望であり、特に科学論文から詳細を抽出し、適切にグループ化し、希望する形式で応答を生成することに期待を示した。

SPIQAは、科学論文の複数の図表と関連テキストを同時に推論するLLMの能力を評価するために導入された。SPIQAを用いて、LLMのマルチモーダルおよび長文コンテキスト機能をテストする。

FEABenchは、自然言語の問題記述を推論し、有限要素解析ソフトウェアを操作して答えを計算することにより、LLMがエンジニアリングモデリング問題をエンドツーエンドで解決する能力を調査する。

CURIE、SPIQA、FEABenchのデータセットと評価コードはGitHubで公開されている。これらのベンチマークは、長文コンテキストの理解、マルチモーダル推論、計算ツールの統合に焦点を当てた、AIシステムの挑戦的で現実的な評価を作成するための重要な取り組みである。

